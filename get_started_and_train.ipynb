{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetsandeepan/gdp-hmm_aapmchallenge/blob/main/get_started_and_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4bb345-7c95-4c74-a059-c079e5cd0d5d",
      "metadata": {
        "tags": [],
        "id": "4a4bb345-7c95-4c74-a059-c079e5cd0d5d"
      },
      "source": [
        "# Get Started for GDP-HMM Challenge\n",
        "\n",
        "This tutorial offers a quick start for training a 3D dose prediction. The participants are encouraged to bring more advanced techniques to improvement the baseline.\n",
        "\n",
        "If you do not like this Jupyter Notebook style, you can directly run the [train.py](train.py) with command line as below (after you have installed necessary packages):\n",
        "\n",
        "```\n",
        "python train.py config_files/config.yaml\n",
        "```\n",
        "\n",
        "or\n",
        "\n",
        "```\n",
        "python train_lightning.py config_files/config.yaml\n",
        "```\n",
        "\n",
        "where [config.yaml](config_files/config.yaml) summarizes all the important hyperparameters. The lightning version can directly use multi-process and multi-gpu.\n",
        "\n",
        "After the training finished, run the below command after set the pre-trained model path in the `config_infer.yaml` file.\n",
        "\n",
        "```\n",
        "python inference.py config_files/config_infer.yaml\n",
        "```\n",
        "\n",
        "Want more details? please continue the following.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mOgAWJ8GRX1Z",
        "outputId": "43ac8dc1-bb29-4a3a-a25d-b7d59c2beaba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mOgAWJ8GRX1Z",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f9132d",
      "metadata": {
        "id": "f6f9132d"
      },
      "source": [
        "# 0. Before Start\n",
        "\n",
        "**Step 1. Register the challenge**.\n",
        "\n",
        "Go to the <a href=\"https://qtim-challenges.southcentralus.cloudapp.azure.com/competitions/38/\" _target='blank'>challenge platform</a>:\n",
        "\n",
        "1.1 create an account of the platform;\n",
        "\n",
        "1.2 go to \"My Submissions\" and read the terms carefully and register the challenge.\n",
        "\n",
        "**Step 2: Download data/model resources**.\n",
        "\n",
        "2.1 download the data (and pre-train models) in huggingface (you will need to submit registration to challenge platform first).\n",
        "\n",
        "[Data](https://huggingface.co/datasets/Jungle15/GDP-HMM_Challenge)\n",
        "\n",
        "[Model](https://huggingface.co/Jungle15/GDP-HMM_baseline)\n",
        "\n",
        "2.2 [optional] for data/prediction samples, you can download from [OneDrive](https://1drv.ms/f/c/347c1b40c8c6e5ec/Ej5OQVE_APpOnNuP-ZXpnZcBnr_-ix5W-twQcYIJ-dvW2A?e=YcBSPF), and put them into `data` and `results` folders, respectively. This is not the whole dataset for the challenge.\n",
        "\n",
        "2.3 change the `npz_path` in the `meta_files/meta_data.csv` depending on the data path on your local machine.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5c857e-340f-4c27-ba3a-6bd7bea5f56b",
      "metadata": {
        "id": "2c5c857e-340f-4c27-ba3a-6bd7bea5f56b"
      },
      "source": [
        "## 1. Python Environment\n",
        "\n",
        "The baseline has been tested with Python 3.10, PyTorch 2.1.2, and MONAI 1.4.0. Similar versions should work but have not been tested by organizers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c386e22-0f31-41d7-9674-1422aa754d31",
      "metadata": {
        "id": "1c386e22-0f31-41d7-9674-1422aa754d31"
      },
      "source": [
        "## 2. Install the MedNeXt as the network backbone\n",
        "\n",
        "In the baseline, we choose the [MedNeXt](https://github.com/MIC-DKFZ/MedNeXt) as backbone. One major reason is that MedNeXt has achieved the top performance in recently release **external** testing benckmarks including the [TouchStone (NeurIPS 2024)](https://github.com/MrGiovanni/Touchstone) and [nnUnet revisited (MICCAI 2024)](https://arxiv.org/abs/2404.09556). MedNeXt is still a CNN-based structure, while in the external testing benckmarks, it has consistently beated all the other Transformers and Mamaba structures, sometimes with a large margin.\n",
        "\n",
        "Please follow the [MedNeXt official instructions](https://github.com/MIC-DKFZ/MedNeXt) to install and use. It is quite detailed and easy to follow. For example, you can use below command lines to install:\n",
        "\n",
        "```\n",
        "git clone https://github.com/MIC-DKFZ/MedNeXt.git mednext\n",
        "cd mednext\n",
        "pip install -e .\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95fa33d-e04d-4004-b44f-77a1f0b6b5a4",
      "metadata": {
        "id": "d95fa33d-e04d-4004-b44f-77a1f0b6b5a4"
      },
      "source": [
        "## 3. Import neccessary packages and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MIC-DKFZ/MedNeXt.git mednext\n",
        "!cd mednext && pip install -e .\n",
        "!pip install monai && pip install nnunet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6r3jJIBpRgmR",
        "outputId": "65e0c9ee-0bd0-4ecc-901f-7893056e4597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "6r3jJIBpRgmR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mednext'...\n",
            "remote: Enumerating objects: 762, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 762 (delta 49), reused 51 (delta 29), pack-reused 677 (from 1)\u001b[K\n",
            "Receiving objects: 100% (762/762), 568.41 KiB | 4.90 MiB/s, done.\n",
            "Resolving deltas: 100% (422/422), done.\n",
            "Obtaining file:///content/mednext\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>1.10.0 in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (4.67.1)\n",
            "Collecting dicom2nifti (from mednextv1==1.7.0)\n",
            "  Downloading dicom2nifti-2.5.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (0.25.0)\n",
            "Collecting medpy (from mednextv1==1.7.0)\n",
            "  Downloading medpy-0.5.2.tar.gz (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (1.13.1)\n",
            "Collecting batchgenerators>=0.23 (from mednextv1==1.7.0)\n",
            "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (1.6.0)\n",
            "Collecting SimpleITK (from mednextv1==1.7.0)\n",
            "  Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2.32.3)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (5.3.2)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2024.12.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (11.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (1.0.0)\n",
            "Collecting unittest2 (from batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (3.5.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (2.36.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>1.10.0->mednextv1==1.7.0) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>1.10.0->mednextv1==1.7.0) (1.3.0)\n",
            "Collecting pydicom>=2.2.0 (from dicom2nifti->mednextv1==1.7.0)\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting python-gdcm (from dicom2nifti->mednextv1==1.7.0)\n",
            "  Downloading python_gdcm-3.0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->mednextv1==1.7.0) (6.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mednextv1==1.7.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mednextv1==1.7.0) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->mednextv1==1.7.0) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mednextv1==1.7.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>1.10.0->mednextv1==1.7.0) (3.0.2)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting traceback2 (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
            "Downloading dicom2nifti-2.5.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_gdcm-3.0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: batchgenerators, medpy\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=cac244731fad573273bbfef28a1111105bf545083c47ce54d74a39d3b1fb66e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/11/c7/fadca30e054c602093ffe36ba8a2f0a87dd2f86ac75191d3ed\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.5.2-py3-none-any.whl size=224709 sha256=24bc0f4022f1f366fc595a5567b88977405887c8fb838f1637b007e9f3bcf36c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/33/ed/aaac5a347fb8d41679ca515b8f5c49dfdf49be15bdbb9a905d\n",
            "Successfully built batchgenerators medpy\n",
            "Installing collected packages: SimpleITK, linecache2, argparse, traceback2, python-gdcm, pydicom, unittest2, medpy, dicom2nifti, batchgenerators, mednextv1\n",
            "  Running setup.py develop for mednextv1\n",
            "Successfully installed SimpleITK-2.4.1 argparse-1.4.0 batchgenerators-0.25.1 dicom2nifti-2.5.1 linecache2-1.0.0 mednextv1-1.7.0 medpy-0.5.2 pydicom-3.0.1 python-gdcm-3.0.24.1 traceback2-1.4.0 unittest2-1.1.0\n",
            "Collecting monai\n",
            "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->monai) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\n",
            "Downloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.4.0\n",
            "Collecting nnunet\n",
            "  Downloading nnunet-1.7.1.tar.gz (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>1.10.0 in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nnunet) (4.67.1)\n",
            "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.5.1)\n",
            "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.11/dist-packages (from nnunet) (0.25.0)\n",
            "Requirement already satisfied: medpy in /usr/local/lib/python3.11/dist-packages (from nnunet) (0.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nnunet) (1.13.1)\n",
            "Requirement already satisfied: batchgenerators>=0.23 in /usr/local/lib/python3.11/dist-packages (from nnunet) (0.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnunet) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nnunet) (1.6.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.32.3)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from nnunet) (5.3.2)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from nnunet) (2024.12.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nnunet) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (11.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (1.0.0)\n",
            "Requirement already satisfied: unittest2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (3.5.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (2.36.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>1.10.0->nnunet) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>1.10.0->nnunet) (1.3.0)\n",
            "Requirement already satisfied: pydicom>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from dicom2nifti->nnunet) (3.0.1)\n",
            "Requirement already satisfied: python-gdcm in /usr/local/lib/python3.11/dist-packages (from dicom2nifti->nnunet) (3.0.24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->nnunet) (6.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunet) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nnunet) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->nnunet) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>1.10.0->nnunet) (3.0.2)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.23->nnunet)\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: traceback2 in /usr/local/lib/python3.11/dist-packages (from unittest2->batchgenerators>=0.23->nnunet) (1.4.0)\n",
            "Requirement already satisfied: linecache2 in /usr/local/lib/python3.11/dist-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet) (1.0.0)\n",
            "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: nnunet\n",
            "  Building wheel for nnunet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nnunet: filename=nnunet-1.7.1-py3-none-any.whl size=531260 sha256=8454ad97c4af1f7f54d51e0e03f8a20df881fe6cc198cbe522c693d565da8bd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/24/6f/29ae4df3d7ad633127125dba95ba0e3ef10bfea39da3a0f44c\n",
            "Successfully built nnunet\n",
            "Installing collected packages: argparse, nnunet\n",
            "Successfully installed argparse-1.4.0 nnunet-1.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "40c6a0c6ffad478489b9c82afe2844ca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "63cb291c-5e87-4a61-94c4-ccdf8d6bb39d",
      "metadata": {
        "id": "63cb291c-5e87-4a61-94c4-ccdf8d6bb39d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "\n",
        "from nnunet_mednext import create_mednext_v1\n",
        "\n",
        "# Import data_loader from the current directory\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge')\n",
        "import data_loader\n",
        "\n",
        "cfig = yaml.load(open('/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/config_files/config.yaml'), Loader=yaml.FullLoader)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4531415-8ffb-46c9-81d9-b357785a76d1",
      "metadata": {
        "id": "e4531415-8ffb-46c9-81d9-b357785a76d1"
      },
      "source": [
        "The config includes two major parts: loader_params and model_params. We will introduce them more in the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bfdb9d4d-cc8d-435c-87f6-7a5418c6d0f2",
      "metadata": {
        "id": "bfdb9d4d-cc8d-435c-87f6-7a5418c6d0f2",
        "outputId": "a5d41879-f950-4dfe-e4f4-3aa5029af5df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_bs': 2,\n",
              " 'val_bs': 2,\n",
              " 'csv_root': '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/meta_files/meta_data.csv',\n",
              " 'scale_dose_dict': '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/meta_files/PTV_DICT.json',\n",
              " 'pat_obj_dict': '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/meta_files/Pat_Obj_DICT.json',\n",
              " 'num_workers': 4,\n",
              " 'down_HU': -1000,\n",
              " 'up_HU': 1000,\n",
              " 'denom_norm_HU': 500,\n",
              " 'in_size': [96, 128, 144],\n",
              " 'out_size': [96, 128, 144],\n",
              " 'norm_oar': True,\n",
              " 'CatStructures': False,\n",
              " 'dose_div_factor': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "cfig['loader_params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "055c9bff-c98d-44ab-8aed-217c1e4832d8",
      "metadata": {
        "id": "055c9bff-c98d-44ab-8aed-217c1e4832d8",
        "outputId": "e8ce5ea9-c242-4b87-f9a2-8868b30494d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_input_channels': 8,\n",
              " 'out_channels': 1,\n",
              " 'model_id': 'B',\n",
              " 'kernel_size': 3,\n",
              " 'deep_supervision': False}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "cfig['model_params']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec80cde4-46fd-4cdc-bd60-3d79b3290be6",
      "metadata": {
        "id": "ec80cde4-46fd-4cdc-bd60-3d79b3290be6"
      },
      "source": [
        "## 3. Data loader for this challenge\n",
        "\n",
        "For getting started, data loader might be most difficult part for the majority of participants. Do not worry, we will help you here!\n",
        "\n",
        "We include a complete data loader script in [data_loader.py](data_loader.py), with explanation of each input and parameter. You can simply test the data loader alone by running\n",
        "\n",
        "```\n",
        "python data_loader.py\n",
        "```\n",
        "\n",
        "If you want to visualize the 3D data and Dose-Volume Histograms (DVHs) with Python, we provide a jupyter notebook [here](data_visual_understand.ipynb).\n",
        "\n",
        "If you want to know more about the preprocess of data and adjust it if needed, we provide code [here](geometry_creation.ipynb).\n",
        "\n",
        "For loading the data in deep learning framework, you can use below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5fa66dea-fdad-490e-99b0-ee84a3953dc7",
      "metadata": {
        "id": "5fa66dea-fdad-490e-99b0-ee84a3953dc7"
      },
      "outputs": [],
      "source": [
        "loaders = data_loader.GetLoader(cfig = cfig['loader_params'])\n",
        "train_loader =loaders.train_dataloader()\n",
        "val_loader = loaders.val_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffd9cc78-89cd-4e27-8d2e-e32ed6d6375e",
      "metadata": {
        "id": "ffd9cc78-89cd-4e27-8d2e-e32ed6d6375e"
      },
      "source": [
        "## 4. Network structure\n",
        "\n",
        "As mentioned earlier, we use MedNeXt as the backbone. Please follow the MedNeXt official instructions to adjust the structure. The example we use is as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "de8de195-b584-4f51-afcb-67002a4f09df",
      "metadata": {
        "id": "de8de195-b584-4f51-afcb-67002a4f09df"
      },
      "outputs": [],
      "source": [
        "model = create_mednext_v1( num_input_channels = cfig['model_params']['num_input_channels'],\n",
        "  num_classes = cfig['model_params']['out_channels'],\n",
        "  model_id = cfig['model_params']['model_id'],          # S, B, M and L are valid model ids\n",
        "  kernel_size = cfig['model_params']['kernel_size'],   # 3x3x3 and 5x5x5 were tested in publication\n",
        "  deep_supervision = cfig['model_params']['deep_supervision']\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c19968b-6d82-4785-992f-8f13d1043892",
      "metadata": {
        "id": "3c19968b-6d82-4785-992f-8f13d1043892"
      },
      "source": [
        "## 5. Define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dbe7bca4-a3f1-44b3-b1d8-7af538ad865f",
      "metadata": {
        "id": "dbe7bca4-a3f1-44b3-b1d8-7af538ad865f"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=cfig['lr'])\n",
        "criterion = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d5c4753-af88-4384-84f9-58a3618a6532",
      "metadata": {
        "id": "6d5c4753-af88-4384-84f9-58a3618a6532"
      },
      "source": [
        "## 6. Training\n",
        "\n",
        "Then, you are ready to with training loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d5ef64-e046-43b6-ad6b-df3b767cbb3f",
      "metadata": {
        "id": "76d5ef64-e046-43b6-ad6b-df3b767cbb3f",
        "outputId": "77d832c7-ecbd-4030-a815-f569c595cece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-87fe06b139c7>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from epoch 1 with loss 0.5264\n",
            "Epoch [1/400], Step [1/716], Loss: 0.4549\n",
            "Epoch [1/400], Step [2/716], Loss: 0.3698\n",
            "Epoch [1/400], Step [3/716], Loss: 0.4775\n",
            "Epoch [1/400], Step [4/716], Loss: 0.3899\n",
            "Epoch [1/400], Step [5/716], Loss: 0.4181\n",
            "Epoch [1/400], Step [6/716], Loss: 0.4178\n",
            "Epoch [1/400], Step [7/716], Loss: 0.3317\n",
            "Epoch [1/400], Step [8/716], Loss: 0.4752\n",
            "Epoch [1/400], Step [9/716], Loss: 0.3219\n",
            "Epoch [1/400], Step [10/716], Loss: 0.6050\n",
            "Epoch [1/400], Step [11/716], Loss: 0.4046\n",
            "Epoch [1/400], Step [12/716], Loss: 0.3856\n",
            "Epoch [1/400], Step [13/716], Loss: 0.4489\n",
            "Epoch [1/400], Step [14/716], Loss: 0.3758\n",
            "Epoch [1/400], Step [15/716], Loss: 0.5123\n",
            "Epoch [1/400], Step [16/716], Loss: 0.4258\n",
            "Epoch [1/400], Step [17/716], Loss: 0.4853\n",
            "Epoch [1/400], Step [18/716], Loss: 0.3687\n",
            "Epoch [1/400], Step [19/716], Loss: 0.5592\n",
            "Epoch [1/400], Step [20/716], Loss: 0.4341\n",
            "Epoch [1/400], Step [21/716], Loss: 0.3651\n",
            "Epoch [1/400], Step [22/716], Loss: 0.3451\n",
            "Epoch [1/400], Step [23/716], Loss: 0.5051\n",
            "Epoch [1/400], Step [24/716], Loss: 0.3523\n",
            "Epoch [1/400], Step [25/716], Loss: 0.4048\n",
            "Epoch [1/400], Step [26/716], Loss: 0.3671\n",
            "Epoch [1/400], Step [27/716], Loss: 0.2819\n",
            "Epoch [1/400], Step [28/716], Loss: 0.3654\n",
            "Epoch [1/400], Step [29/716], Loss: 0.3068\n",
            "Epoch [1/400], Step [30/716], Loss: 0.3565\n",
            "Epoch [1/400], Step [31/716], Loss: 0.3447\n",
            "Epoch [1/400], Step [32/716], Loss: 0.3351\n",
            "Epoch [1/400], Step [33/716], Loss: 0.3836\n",
            "Epoch [1/400], Step [34/716], Loss: 0.4135\n",
            "Epoch [1/400], Step [35/716], Loss: 0.2771\n",
            "Epoch [1/400], Step [36/716], Loss: 0.3031\n",
            "Epoch [1/400], Step [37/716], Loss: 0.4934\n",
            "Epoch [1/400], Step [38/716], Loss: 0.4737\n",
            "Epoch [1/400], Step [39/716], Loss: 0.2938\n",
            "Epoch [1/400], Step [40/716], Loss: 0.3409\n",
            "Epoch [1/400], Step [41/716], Loss: 0.3334\n",
            "Epoch [1/400], Step [42/716], Loss: 0.3401\n",
            "Epoch [1/400], Step [43/716], Loss: 0.2733\n",
            "Epoch [1/400], Step [44/716], Loss: 0.3989\n",
            "Epoch [1/400], Step [45/716], Loss: 0.4029\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define a function to save a checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "\n",
        "    # Make sure the checkpoint directory exists\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Save the checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "# Define a function to load a checkpoint\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Resuming from epoch {epoch + 1} with loss {loss:.4f}\")\n",
        "    return model, optimizer, epoch, loss\n",
        "\n",
        "# Set your checkpoint directory\n",
        "checkpoint_dir = '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints'\n",
        "checkpoint_filename = 'checkpoint_epoch_1_step_100.pth'  # The latest checkpoint file\n",
        "\n",
        "# Check if a checkpoint exists to resume training\n",
        "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "start_epoch = 0  # Default starting epoch\n",
        "loss = 0.0\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    # Load checkpoint if it exists\n",
        "    model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "else:\n",
        "    print(\"No checkpoint found, starting from scratch.\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, cfig['num_epochs']):\n",
        "    model.train()\n",
        "    for i, data_dict in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(data_dict['data'].to(device))\n",
        "        loss = criterion(outputs, data_dict['label'].to(device))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the current loss\n",
        "        print(f\"Epoch [{epoch+1}/{cfig['num_epochs']}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Save checkpoint every 'N' steps (for example every 100 steps)\n",
        "        if (i + 1) % 100 == 0:\n",
        "            save_checkpoint(model, optimizer, epoch, loss.item(), checkpoint_dir, filename=f\"checkpoint_epoch_{epoch+1}_step_{i+1}.pth\")\n",
        "\n",
        "    # Optionally, save a checkpoint at the end of each epoch\n",
        "    save_checkpoint(model, optimizer, epoch, loss.item(), checkpoint_dir, filename=f\"checkpoint_epoch_{epoch+1}_last.pth\")\n",
        "\n",
        "# After training, save the final model weights\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/model_weights.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f83985",
      "metadata": {
        "tags": [],
        "id": "55f83985"
      },
      "source": [
        "## 7. Timing for inference of the deep learning module\n",
        "\n",
        "In this challenge, we impose a time regularization specifically for the deep learning module in dose prediction, reflecting its clinical application nature. Data preprocessing, however, is outside the scope of this challenge and can be optimized using C++/CUDA for significantly faster performance.  \n",
        "\n",
        "The MedNeXt baseline we provided comprises approximately 10 million parameters and achieves an inference time of just 0.13 seconds. While we allow a relatively lenient inference time constraint of **3 seconds**, caution is advised when employing diffusion models, particularly if acceleration techniques are not utilized. For example, the default DDPM requires 1000 steps to generate results, which can easily exceed the time constraint.\n",
        "\n",
        "Please check below code to get sense of how inference time is calculated. Also, the peak inference GPU memory cannot exceed **24 GB** (the baseline is ~5.7 GB).\n",
        "\n",
        "***The solution exceeds either time constraint or GPU memory constraint will be rejected!***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dea8dd9",
      "metadata": {
        "id": "0dea8dd9"
      },
      "outputs": [],
      "source": [
        "import time, os\n",
        "model.eval()\n",
        "data_dict = next(iter(train_loader)) # since this is a dummy test, it does not matter using train or test loaders.\n",
        "\n",
        "print (f\"the total parameters of the model is {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "    print ('----- skip first 20 times, to avoid delay because of running start ----')\n",
        "    for i in range(20):\n",
        "        outputs = model(data_dict['data'].to(device))\n",
        "    os.system('nvidia-smi')\n",
        "    start = time.time()\n",
        "    for i in range(20):\n",
        "        outputs = model(data_dict['data'].to(device))\n",
        "    end = time.time()\n",
        "    print(f\"Time taken for average forward pass: {(end-start) / 20:.4f} seconds\")\n",
        "    assert (end-start) / 20 < 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05b178d-d9da-4513-bf10-399fb74dd147",
      "metadata": {
        "id": "b05b178d-d9da-4513-bf10-399fb74dd147"
      },
      "source": [
        "## 8. Training Regularization\n",
        "\n",
        "To strengthen the challenge's objectives, participants are required to develop a generalizable model rather than separate models tailored to individual contexts. To ensure compliance, top-performing participants must submit their training and inference code for review by the organizers.\n",
        "\n",
        "**Prohibited approaches include (but are not limited to):**\n",
        "\n",
        "Training separate models for different treatment techniques, such as one for IMRT and another for VMAT.\n",
        "Training separate models for different treatment sites, such as one for head-and-neck cancers and another for lung cancers.\n",
        "\n",
        "**Rationale for this regularization:**\n",
        "\n",
        "In real-world applications, many other contexts exist, including diverse treatment sites (e.g., prostate, breast, cervical, esophageal, and bladder cancers) and varying treatment geometries (e.g., combinations of IMRT and VMAT, such as RapidArc Dynamic). The goal is to develop a generalizable model capable of adapting to new contexts as more training data become available, rather than creating multiple context-specific models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57aaaa7-cb72-449c-9330-7e34de410498",
      "metadata": {
        "id": "a57aaaa7-cb72-449c-9330-7e34de410498"
      },
      "source": [
        "## 9. Start your development"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec697f81-fd7a-4fb2-a13b-dae7471a1368",
      "metadata": {
        "id": "ec697f81-fd7a-4fb2-a13b-dae7471a1368"
      },
      "source": [
        "Congradulations! You have reached the end of the tutorial and should get the sense how the task is.\n",
        "\n",
        "Here we just provide a example to help you get started. Some of the parameters are not optimal, only few examples included in the csv file.\n",
        "\n",
        "Now, it is time for you to include more data from the challenge and use your AI expertise to get better results.\n",
        "\n",
        "Wish you a great experience with this challenge and research beyond!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42369d6c-efda-450e-a975-97e94ed8e20d",
      "metadata": {
        "id": "42369d6c-efda-450e-a975-97e94ed8e20d"
      },
      "outputs": [],
      "source": [
        "# Assuming 'model' is your trained model object\n",
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}