{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetsandeepan/gdp-hmm_aapmchallenge/blob/main/get_started_and_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4bb345-7c95-4c74-a059-c079e5cd0d5d",
      "metadata": {
        "id": "4a4bb345-7c95-4c74-a059-c079e5cd0d5d",
        "tags": []
      },
      "source": [
        "# Get Started for GDP-HMM Challenge\n",
        "\n",
        "This tutorial offers a quick start for training a 3D dose prediction. The participants are encouraged to bring more advanced techniques to improvement the baseline.\n",
        "\n",
        "If you do not like this Jupyter Notebook style, you can directly run the [train.py](train.py) with command line as below (after you have installed necessary packages):\n",
        "\n",
        "```\n",
        "python train.py config_files/config.yaml\n",
        "```\n",
        "\n",
        "or\n",
        "\n",
        "```\n",
        "python train_lightning.py config_files/config.yaml\n",
        "```\n",
        "\n",
        "where [config.yaml](config_files/config.yaml) summarizes all the important hyperparameters. The lightning version can directly use multi-process and multi-gpu.\n",
        "\n",
        "After the training finished, run the below command after set the pre-trained model path in the `config_infer.yaml` file.\n",
        "\n",
        "```\n",
        "python inference.py config_files/config_infer.yaml\n",
        "```\n",
        "\n",
        "Want more details? please continue the following.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mOgAWJ8GRX1Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOgAWJ8GRX1Z",
        "outputId": "43ac8dc1-bb29-4a3a-a25d-b7d59c2beaba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f9132d",
      "metadata": {
        "id": "f6f9132d"
      },
      "source": [
        "# 0. Before Start\n",
        "\n",
        "**Step 1. Register the challenge**.\n",
        "\n",
        "Go to the <a href=\"https://qtim-challenges.southcentralus.cloudapp.azure.com/competitions/38/\" _target='blank'>challenge platform</a>:\n",
        "\n",
        "1.1 create an account of the platform;\n",
        "\n",
        "1.2 go to \"My Submissions\" and read the terms carefully and register the challenge.\n",
        "\n",
        "**Step 2: Download data/model resources**.\n",
        "\n",
        "2.1 download the data (and pre-train models) in huggingface (you will need to submit registration to challenge platform first).\n",
        "\n",
        "[Data](https://huggingface.co/datasets/Jungle15/GDP-HMM_Challenge)\n",
        "\n",
        "[Model](https://huggingface.co/Jungle15/GDP-HMM_baseline)\n",
        "\n",
        "2.2 [optional] for data/prediction samples, you can download from [OneDrive](https://1drv.ms/f/c/347c1b40c8c6e5ec/Ej5OQVE_APpOnNuP-ZXpnZcBnr_-ix5W-twQcYIJ-dvW2A?e=YcBSPF), and put them into `data` and `results` folders, respectively. This is not the whole dataset for the challenge.\n",
        "\n",
        "2.3 change the `npz_path` in the `meta_files/meta_data.csv` depending on the data path on your local machine.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5c857e-340f-4c27-ba3a-6bd7bea5f56b",
      "metadata": {
        "id": "2c5c857e-340f-4c27-ba3a-6bd7bea5f56b"
      },
      "source": [
        "## 1. Python Environment\n",
        "\n",
        "The baseline has been tested with Python 3.10, PyTorch 2.1.2, and MONAI 1.4.0. Similar versions should work but have not been tested by organizers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c386e22-0f31-41d7-9674-1422aa754d31",
      "metadata": {
        "id": "1c386e22-0f31-41d7-9674-1422aa754d31"
      },
      "source": [
        "## 2. Install the MedNeXt as the network backbone\n",
        "\n",
        "In the baseline, we choose the [MedNeXt](https://github.com/MIC-DKFZ/MedNeXt) as backbone. One major reason is that MedNeXt has achieved the top performance in recently release **external** testing benckmarks including the [TouchStone (NeurIPS 2024)](https://github.com/MrGiovanni/Touchstone) and [nnUnet revisited (MICCAI 2024)](https://arxiv.org/abs/2404.09556). MedNeXt is still a CNN-based structure, while in the external testing benckmarks, it has consistently beated all the other Transformers and Mamaba structures, sometimes with a large margin.\n",
        "\n",
        "Please follow the [MedNeXt official instructions](https://github.com/MIC-DKFZ/MedNeXt) to install and use. It is quite detailed and easy to follow. For example, you can use below command lines to install:\n",
        "\n",
        "```\n",
        "git clone https://github.com/MIC-DKFZ/MedNeXt.git mednext\n",
        "cd mednext\n",
        "pip install -e .\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95fa33d-e04d-4004-b44f-77a1f0b6b5a4",
      "metadata": {
        "id": "d95fa33d-e04d-4004-b44f-77a1f0b6b5a4"
      },
      "source": [
        "## 3. Import neccessary packages and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6r3jJIBpRgmR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6r3jJIBpRgmR",
        "outputId": "65e0c9ee-0bd0-4ecc-901f-7893056e4597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mednext'...\n",
            "remote: Enumerating objects: 762, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 762 (delta 49), reused 51 (delta 29), pack-reused 677 (from 1)\u001b[K\n",
            "Receiving objects: 100% (762/762), 568.41 KiB | 4.90 MiB/s, done.\n",
            "Resolving deltas: 100% (422/422), done.\n",
            "Obtaining file:///content/mednext\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>1.10.0 in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (4.67.1)\n",
            "Collecting dicom2nifti (from mednextv1==1.7.0)\n",
            "  Downloading dicom2nifti-2.5.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (0.25.0)\n",
            "Collecting medpy (from mednextv1==1.7.0)\n",
            "  Downloading medpy-0.5.2.tar.gz (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (1.13.1)\n",
            "Collecting batchgenerators>=0.23 (from mednextv1==1.7.0)\n",
            "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (1.6.0)\n",
            "Collecting SimpleITK (from mednextv1==1.7.0)\n",
            "  Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2.32.3)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (5.3.2)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (2024.12.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mednextv1==1.7.0) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (11.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (1.0.0)\n",
            "Collecting unittest2 (from batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (3.5.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (2.36.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>1.10.0->mednextv1==1.7.0) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>1.10.0->mednextv1==1.7.0) (1.3.0)\n",
            "Collecting pydicom>=2.2.0 (from dicom2nifti->mednextv1==1.7.0)\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting python-gdcm (from dicom2nifti->mednextv1==1.7.0)\n",
            "  Downloading python_gdcm-3.0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mednextv1==1.7.0) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->mednextv1==1.7.0) (6.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mednextv1==1.7.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mednextv1==1.7.0) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->mednextv1==1.7.0) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->mednextv1==1.7.0) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mednextv1==1.7.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>1.10.0->mednextv1==1.7.0) (3.0.2)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting traceback2 (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
            "Downloading dicom2nifti-2.5.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_gdcm-3.0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: batchgenerators, medpy\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=cac244731fad573273bbfef28a1111105bf545083c47ce54d74a39d3b1fb66e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/11/c7/fadca30e054c602093ffe36ba8a2f0a87dd2f86ac75191d3ed\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.5.2-py3-none-any.whl size=224709 sha256=24bc0f4022f1f366fc595a5567b88977405887c8fb838f1637b007e9f3bcf36c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/33/ed/aaac5a347fb8d41679ca515b8f5c49dfdf49be15bdbb9a905d\n",
            "Successfully built batchgenerators medpy\n",
            "Installing collected packages: SimpleITK, linecache2, argparse, traceback2, python-gdcm, pydicom, unittest2, medpy, dicom2nifti, batchgenerators, mednextv1\n",
            "  Running setup.py develop for mednextv1\n",
            "Successfully installed SimpleITK-2.4.1 argparse-1.4.0 batchgenerators-0.25.1 dicom2nifti-2.5.1 linecache2-1.0.0 mednextv1-1.7.0 medpy-0.5.2 pydicom-3.0.1 python-gdcm-3.0.24.1 traceback2-1.4.0 unittest2-1.1.0\n",
            "Collecting monai\n",
            "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->monai) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\n",
            "Downloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.4.0\n",
            "Collecting nnunet\n",
            "  Downloading nnunet-1.7.1.tar.gz (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>1.10.0 in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nnunet) (4.67.1)\n",
            "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.5.1)\n",
            "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.11/dist-packages (from nnunet) (0.25.0)\n",
            "Requirement already satisfied: medpy in /usr/local/lib/python3.11/dist-packages (from nnunet) (0.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nnunet) (1.13.1)\n",
            "Requirement already satisfied: batchgenerators>=0.23 in /usr/local/lib/python3.11/dist-packages (from nnunet) (0.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnunet) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nnunet) (1.6.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from nnunet) (2.32.3)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from nnunet) (5.3.2)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from nnunet) (2024.12.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nnunet) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (11.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (1.0.0)\n",
            "Requirement already satisfied: unittest2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.23->nnunet) (3.5.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (2.36.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14->nnunet) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.10.0->nnunet) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>1.10.0->nnunet) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>1.10.0->nnunet) (1.3.0)\n",
            "Requirement already satisfied: pydicom>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from dicom2nifti->nnunet) (3.0.1)\n",
            "Requirement already satisfied: python-gdcm in /usr/local/lib/python3.11/dist-packages (from dicom2nifti->nnunet) (3.0.24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunet) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->nnunet) (6.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunet) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->nnunet) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nnunet) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->nnunet) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>1.10.0->nnunet) (3.0.2)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.23->nnunet)\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: traceback2 in /usr/local/lib/python3.11/dist-packages (from unittest2->batchgenerators>=0.23->nnunet) (1.4.0)\n",
            "Requirement already satisfied: linecache2 in /usr/local/lib/python3.11/dist-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet) (1.0.0)\n",
            "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: nnunet\n",
            "  Building wheel for nnunet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nnunet: filename=nnunet-1.7.1-py3-none-any.whl size=531260 sha256=8454ad97c4af1f7f54d51e0e03f8a20df881fe6cc198cbe522c693d565da8bd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/24/6f/29ae4df3d7ad633127125dba95ba0e3ef10bfea39da3a0f44c\n",
            "Successfully built nnunet\n",
            "Installing collected packages: argparse, nnunet\n",
            "Successfully installed argparse-1.4.0 nnunet-1.7.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "40c6a0c6ffad478489b9c82afe2844ca",
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!git clone https://github.com/MIC-DKFZ/MedNeXt.git mednext\n",
        "!cd mednext && pip install -e .\n",
        "!pip install monai && pip install nnunet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63cb291c-5e87-4a61-94c4-ccdf8d6bb39d",
      "metadata": {
        "id": "63cb291c-5e87-4a61-94c4-ccdf8d6bb39d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "\n",
        "from nnunet_mednext import create_mednext_v1\n",
        "\n",
        "# Import data_loader from the current directory\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge')\n",
        "import data_loader\n",
        "\n",
        "cfig = yaml.load(open('/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/config_files/config.yaml'), Loader=yaml.FullLoader)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4531415-8ffb-46c9-81d9-b357785a76d1",
      "metadata": {
        "id": "e4531415-8ffb-46c9-81d9-b357785a76d1"
      },
      "source": [
        "The config includes two major parts: loader_params and model_params. We will introduce them more in the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfdb9d4d-cc8d-435c-87f6-7a5418c6d0f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfdb9d4d-cc8d-435c-87f6-7a5418c6d0f2",
        "outputId": "a5d41879-f950-4dfe-e4f4-3aa5029af5df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train_bs': 2,\n",
              " 'val_bs': 2,\n",
              " 'csv_root': '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/meta_files/meta_data.csv',\n",
              " 'scale_dose_dict': '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/meta_files/PTV_DICT.json',\n",
              " 'pat_obj_dict': '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/meta_files/Pat_Obj_DICT.json',\n",
              " 'num_workers': 4,\n",
              " 'down_HU': -1000,\n",
              " 'up_HU': 1000,\n",
              " 'denom_norm_HU': 500,\n",
              " 'in_size': [96, 128, 144],\n",
              " 'out_size': [96, 128, 144],\n",
              " 'norm_oar': True,\n",
              " 'CatStructures': False,\n",
              " 'dose_div_factor': 10}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cfig['loader_params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055c9bff-c98d-44ab-8aed-217c1e4832d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "055c9bff-c98d-44ab-8aed-217c1e4832d8",
        "outputId": "e8ce5ea9-c242-4b87-f9a2-8868b30494d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_input_channels': 8,\n",
              " 'out_channels': 1,\n",
              " 'model_id': 'B',\n",
              " 'kernel_size': 3,\n",
              " 'deep_supervision': False}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cfig['model_params']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec80cde4-46fd-4cdc-bd60-3d79b3290be6",
      "metadata": {
        "id": "ec80cde4-46fd-4cdc-bd60-3d79b3290be6"
      },
      "source": [
        "## 3. Data loader for this challenge\n",
        "\n",
        "For getting started, data loader might be most difficult part for the majority of participants. Do not worry, we will help you here!\n",
        "\n",
        "We include a complete data loader script in [data_loader.py](data_loader.py), with explanation of each input and parameter. You can simply test the data loader alone by running\n",
        "\n",
        "```\n",
        "python data_loader.py\n",
        "```\n",
        "\n",
        "If you want to visualize the 3D data and Dose-Volume Histograms (DVHs) with Python, we provide a jupyter notebook [here](data_visual_understand.ipynb).\n",
        "\n",
        "If you want to know more about the preprocess of data and adjust it if needed, we provide code [here](geometry_creation.ipynb).\n",
        "\n",
        "For loading the data in deep learning framework, you can use below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa66dea-fdad-490e-99b0-ee84a3953dc7",
      "metadata": {
        "id": "5fa66dea-fdad-490e-99b0-ee84a3953dc7"
      },
      "outputs": [],
      "source": [
        "loaders = data_loader.GetLoader(cfig = cfig['loader_params'])\n",
        "train_loader =loaders.train_dataloader()\n",
        "val_loader = loaders.val_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffd9cc78-89cd-4e27-8d2e-e32ed6d6375e",
      "metadata": {
        "id": "ffd9cc78-89cd-4e27-8d2e-e32ed6d6375e"
      },
      "source": [
        "## 4. Network structure\n",
        "\n",
        "As mentioned earlier, we use MedNeXt as the backbone. Please follow the MedNeXt official instructions to adjust the structure. The example we use is as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8de195-b584-4f51-afcb-67002a4f09df",
      "metadata": {
        "id": "de8de195-b584-4f51-afcb-67002a4f09df"
      },
      "outputs": [],
      "source": [
        "model = create_mednext_v1( num_input_channels = cfig['model_params']['num_input_channels'],\n",
        "  num_classes = cfig['model_params']['out_channels'],\n",
        "  model_id = cfig['model_params']['model_id'],          # S, B, M and L are valid model ids\n",
        "  kernel_size = cfig['model_params']['kernel_size'],   # 3x3x3 and 5x5x5 were tested in publication\n",
        "  deep_supervision = cfig['model_params']['deep_supervision']\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c19968b-6d82-4785-992f-8f13d1043892",
      "metadata": {
        "id": "3c19968b-6d82-4785-992f-8f13d1043892"
      },
      "source": [
        "## 5. Define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe7bca4-a3f1-44b3-b1d8-7af538ad865f",
      "metadata": {
        "id": "dbe7bca4-a3f1-44b3-b1d8-7af538ad865f"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=cfig['lr'])\n",
        "criterion = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d5c4753-af88-4384-84f9-58a3618a6532",
      "metadata": {
        "id": "6d5c4753-af88-4384-84f9-58a3618a6532"
      },
      "source": [
        "## 6. Training\n",
        "\n",
        "Then, you are ready to with training loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "76d5ef64-e046-43b6-ad6b-df3b767cbb3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "76d5ef64-e046-43b6-ad6b-df3b767cbb3f",
        "outputId": "b269133e-b508-43cf-d38e-29605b7bfdaa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-27ed4653922c>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from epoch 1 with loss 0.3442\n",
            "Epoch [1/400], Step [1/716], Loss: 0.3060\n",
            "Epoch [1/400], Step [2/716], Loss: 0.2666\n",
            "Epoch [1/400], Step [3/716], Loss: 0.2876\n",
            "Epoch [1/400], Step [4/716], Loss: 0.3185\n",
            "Epoch [1/400], Step [5/716], Loss: 0.2662\n",
            "Epoch [1/400], Step [6/716], Loss: 0.2790\n",
            "Epoch [1/400], Step [7/716], Loss: 0.3090\n",
            "Epoch [1/400], Step [8/716], Loss: 0.3216\n",
            "Epoch [1/400], Step [9/716], Loss: 0.4031\n",
            "Epoch [1/400], Step [10/716], Loss: 0.2937\n",
            "Epoch [1/400], Step [11/716], Loss: 0.3045\n",
            "Epoch [1/400], Step [12/716], Loss: 0.3811\n",
            "Epoch [1/400], Step [13/716], Loss: 0.3427\n",
            "Epoch [1/400], Step [14/716], Loss: 0.2628\n",
            "Epoch [1/400], Step [15/716], Loss: 0.3072\n",
            "Epoch [1/400], Step [16/716], Loss: 0.2790\n",
            "Epoch [1/400], Step [17/716], Loss: 0.2392\n",
            "Epoch [1/400], Step [18/716], Loss: 0.2918\n",
            "Epoch [1/400], Step [19/716], Loss: 0.3022\n",
            "Epoch [1/400], Step [20/716], Loss: 0.3827\n",
            "Epoch [1/400], Step [21/716], Loss: 0.2722\n",
            "Epoch [1/400], Step [22/716], Loss: 0.2540\n",
            "Epoch [1/400], Step [23/716], Loss: 0.4543\n",
            "Epoch [1/400], Step [24/716], Loss: 0.4211\n",
            "Epoch [1/400], Step [25/716], Loss: 0.2813\n",
            "Epoch [1/400], Step [26/716], Loss: 0.3041\n",
            "Epoch [1/400], Step [27/716], Loss: 0.2887\n",
            "Epoch [1/400], Step [28/716], Loss: 0.4107\n",
            "Epoch [1/400], Step [29/716], Loss: 0.2642\n",
            "Epoch [1/400], Step [30/716], Loss: 0.3750\n",
            "Epoch [1/400], Step [31/716], Loss: 0.2841\n",
            "Epoch [1/400], Step [32/716], Loss: 0.3080\n",
            "Epoch [1/400], Step [33/716], Loss: 0.2695\n",
            "Epoch [1/400], Step [34/716], Loss: 0.3180\n",
            "Epoch [1/400], Step [35/716], Loss: 0.3641\n",
            "Epoch [1/400], Step [36/716], Loss: 0.3021\n",
            "Epoch [1/400], Step [37/716], Loss: 0.2650\n",
            "Epoch [1/400], Step [38/716], Loss: 0.4412\n",
            "Epoch [1/400], Step [39/716], Loss: 0.2925\n",
            "Epoch [1/400], Step [40/716], Loss: 0.2637\n",
            "Epoch [1/400], Step [41/716], Loss: 0.3269\n",
            "Epoch [1/400], Step [42/716], Loss: 0.3350\n",
            "Epoch [1/400], Step [43/716], Loss: 0.2442\n",
            "Epoch [1/400], Step [44/716], Loss: 0.3785\n",
            "Epoch [1/400], Step [45/716], Loss: 0.3913\n",
            "Epoch [1/400], Step [46/716], Loss: 0.3225\n",
            "Epoch [1/400], Step [47/716], Loss: 0.2682\n",
            "Epoch [1/400], Step [48/716], Loss: 0.2817\n",
            "Epoch [1/400], Step [49/716], Loss: 0.2948\n",
            "Epoch [1/400], Step [50/716], Loss: 0.3295\n",
            "Epoch [1/400], Step [51/716], Loss: 0.2908\n",
            "Epoch [1/400], Step [52/716], Loss: 0.2741\n",
            "Epoch [1/400], Step [53/716], Loss: 0.2684\n",
            "Epoch [1/400], Step [54/716], Loss: 0.2286\n",
            "Epoch [1/400], Step [55/716], Loss: 0.3378\n",
            "Epoch [1/400], Step [56/716], Loss: 0.2304\n",
            "Epoch [1/400], Step [57/716], Loss: 0.3002\n",
            "Epoch [1/400], Step [58/716], Loss: 0.3002\n",
            "Epoch [1/400], Step [59/716], Loss: 0.3655\n",
            "Epoch [1/400], Step [60/716], Loss: 0.4785\n",
            "Epoch [1/400], Step [61/716], Loss: 0.3162\n",
            "Epoch [1/400], Step [62/716], Loss: 0.4196\n",
            "Epoch [1/400], Step [63/716], Loss: 0.3088\n",
            "Epoch [1/400], Step [64/716], Loss: 0.2445\n",
            "Epoch [1/400], Step [65/716], Loss: 0.4336\n",
            "Epoch [1/400], Step [66/716], Loss: 0.3579\n",
            "Epoch [1/400], Step [67/716], Loss: 0.2912\n",
            "Epoch [1/400], Step [68/716], Loss: 0.2708\n",
            "Epoch [1/400], Step [69/716], Loss: 0.2802\n",
            "Epoch [1/400], Step [70/716], Loss: 0.3101\n",
            "Epoch [1/400], Step [71/716], Loss: 0.2580\n",
            "Epoch [1/400], Step [72/716], Loss: 0.3318\n",
            "Epoch [1/400], Step [73/716], Loss: 0.2657\n",
            "Epoch [1/400], Step [74/716], Loss: 0.2611\n",
            "Epoch [1/400], Step [75/716], Loss: 0.2975\n",
            "Epoch [1/400], Step [76/716], Loss: 0.2442\n",
            "Epoch [1/400], Step [77/716], Loss: 0.3604\n",
            "Epoch [1/400], Step [78/716], Loss: 0.2303\n",
            "Epoch [1/400], Step [79/716], Loss: 0.4009\n",
            "Epoch [1/400], Step [80/716], Loss: 0.2342\n",
            "Epoch [1/400], Step [81/716], Loss: 0.3604\n",
            "Epoch [1/400], Step [82/716], Loss: 0.2561\n",
            "Epoch [1/400], Step [83/716], Loss: 0.2897\n",
            "Epoch [1/400], Step [84/716], Loss: 0.2517\n",
            "Epoch [1/400], Step [85/716], Loss: 0.3178\n",
            "Epoch [1/400], Step [86/716], Loss: 0.4183\n",
            "Epoch [1/400], Step [87/716], Loss: 0.2286\n",
            "Epoch [1/400], Step [88/716], Loss: 0.2680\n",
            "Epoch [1/400], Step [89/716], Loss: 0.2866\n",
            "Epoch [1/400], Step [90/716], Loss: 0.2466\n",
            "Epoch [1/400], Step [91/716], Loss: 0.4067\n",
            "Epoch [1/400], Step [92/716], Loss: 0.2655\n",
            "Epoch [1/400], Step [93/716], Loss: 0.2124\n",
            "Epoch [1/400], Step [94/716], Loss: 0.2472\n",
            "Epoch [1/400], Step [95/716], Loss: 0.3570\n",
            "Epoch [1/400], Step [96/716], Loss: 0.2647\n",
            "Epoch [1/400], Step [97/716], Loss: 0.2600\n",
            "Epoch [1/400], Step [98/716], Loss: 0.2996\n",
            "Epoch [1/400], Step [99/716], Loss: 0.2425\n",
            "Epoch [1/400], Step [100/716], Loss: 0.2456\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_step_100.pth\n",
            "Epoch [1/400], Step [101/716], Loss: 0.3529\n",
            "Epoch [1/400], Step [102/716], Loss: 0.5208\n",
            "Epoch [1/400], Step [103/716], Loss: 0.3439\n",
            "Epoch [1/400], Step [104/716], Loss: 0.2335\n",
            "Epoch [1/400], Step [105/716], Loss: 0.2501\n",
            "Epoch [1/400], Step [106/716], Loss: 0.2261\n",
            "Epoch [1/400], Step [107/716], Loss: 0.2442\n",
            "Epoch [1/400], Step [108/716], Loss: 0.4763\n",
            "Epoch [1/400], Step [109/716], Loss: 0.3547\n",
            "Epoch [1/400], Step [110/716], Loss: 0.3041\n",
            "Epoch [1/400], Step [111/716], Loss: 0.3334\n",
            "Epoch [1/400], Step [112/716], Loss: 0.2716\n",
            "Epoch [1/400], Step [113/716], Loss: 0.3461\n",
            "Epoch [1/400], Step [114/716], Loss: 0.5115\n",
            "Epoch [1/400], Step [115/716], Loss: 0.2473\n",
            "Epoch [1/400], Step [116/716], Loss: 0.4300\n",
            "Epoch [1/400], Step [117/716], Loss: 0.3044\n",
            "Epoch [1/400], Step [118/716], Loss: 0.3002\n",
            "Epoch [1/400], Step [119/716], Loss: 0.3066\n",
            "Epoch [1/400], Step [120/716], Loss: 0.3738\n",
            "Epoch [1/400], Step [121/716], Loss: 0.2435\n",
            "Epoch [1/400], Step [122/716], Loss: 0.2421\n",
            "Epoch [1/400], Step [123/716], Loss: 0.3526\n",
            "Epoch [1/400], Step [124/716], Loss: 0.2499\n",
            "Epoch [1/400], Step [125/716], Loss: 0.2697\n",
            "Epoch [1/400], Step [126/716], Loss: 0.2494\n",
            "Epoch [1/400], Step [127/716], Loss: 0.2158\n",
            "Epoch [1/400], Step [128/716], Loss: 0.2107\n",
            "Epoch [1/400], Step [129/716], Loss: 0.2873\n",
            "Epoch [1/400], Step [130/716], Loss: 0.2541\n",
            "Epoch [1/400], Step [131/716], Loss: 0.3743\n",
            "Epoch [1/400], Step [132/716], Loss: 0.3001\n",
            "Epoch [1/400], Step [133/716], Loss: 0.2347\n",
            "Epoch [1/400], Step [134/716], Loss: 0.2775\n",
            "Epoch [1/400], Step [135/716], Loss: 0.2685\n",
            "Epoch [1/400], Step [136/716], Loss: 0.2420\n",
            "Epoch [1/400], Step [137/716], Loss: 0.3487\n",
            "Epoch [1/400], Step [138/716], Loss: 0.3385\n",
            "Epoch [1/400], Step [139/716], Loss: 0.2579\n",
            "Epoch [1/400], Step [140/716], Loss: 0.3479\n",
            "Epoch [1/400], Step [141/716], Loss: 0.3895\n",
            "Epoch [1/400], Step [142/716], Loss: 0.2607\n",
            "Epoch [1/400], Step [143/716], Loss: 0.3300\n",
            "Epoch [1/400], Step [144/716], Loss: 0.2566\n",
            "Epoch [1/400], Step [145/716], Loss: 0.4517\n",
            "Epoch [1/400], Step [146/716], Loss: 0.2760\n",
            "Epoch [1/400], Step [147/716], Loss: 0.2796\n",
            "Epoch [1/400], Step [148/716], Loss: 0.2421\n",
            "Epoch [1/400], Step [149/716], Loss: 0.3518\n",
            "Epoch [1/400], Step [150/716], Loss: 0.2497\n",
            "Epoch [1/400], Step [151/716], Loss: 0.2753\n",
            "Epoch [1/400], Step [152/716], Loss: 0.2360\n",
            "Epoch [1/400], Step [153/716], Loss: 0.2850\n",
            "Epoch [1/400], Step [154/716], Loss: 0.2282\n",
            "Epoch [1/400], Step [155/716], Loss: 0.2413\n",
            "Epoch [1/400], Step [156/716], Loss: 0.2410\n",
            "Epoch [1/400], Step [157/716], Loss: 0.2705\n",
            "Epoch [1/400], Step [158/716], Loss: 0.2427\n",
            "Epoch [1/400], Step [159/716], Loss: 0.2311\n",
            "Epoch [1/400], Step [160/716], Loss: 0.2117\n",
            "Epoch [1/400], Step [161/716], Loss: 0.4292\n",
            "Epoch [1/400], Step [162/716], Loss: 0.3814\n",
            "Epoch [1/400], Step [163/716], Loss: 0.2880\n",
            "Epoch [1/400], Step [164/716], Loss: 0.2495\n",
            "Epoch [1/400], Step [165/716], Loss: 0.2964\n",
            "Epoch [1/400], Step [166/716], Loss: 0.2369\n",
            "Epoch [1/400], Step [167/716], Loss: 0.2575\n",
            "Epoch [1/400], Step [168/716], Loss: 0.2206\n",
            "Epoch [1/400], Step [169/716], Loss: 0.2636\n",
            "Epoch [1/400], Step [170/716], Loss: 0.2992\n",
            "Epoch [1/400], Step [171/716], Loss: 0.1895\n",
            "Epoch [1/400], Step [172/716], Loss: 0.3040\n",
            "Epoch [1/400], Step [173/716], Loss: 0.2204\n",
            "Epoch [1/400], Step [174/716], Loss: 0.2169\n",
            "Epoch [1/400], Step [175/716], Loss: 0.2493\n",
            "Epoch [1/400], Step [176/716], Loss: 0.2699\n",
            "Epoch [1/400], Step [177/716], Loss: 0.2018\n",
            "Epoch [1/400], Step [178/716], Loss: 0.2201\n",
            "Epoch [1/400], Step [179/716], Loss: 0.2283\n",
            "Epoch [1/400], Step [180/716], Loss: 0.2213\n",
            "Epoch [1/400], Step [181/716], Loss: 0.3044\n",
            "Epoch [1/400], Step [182/716], Loss: 0.1797\n",
            "Epoch [1/400], Step [183/716], Loss: 0.2311\n",
            "Epoch [1/400], Step [184/716], Loss: 0.2789\n",
            "Epoch [1/400], Step [185/716], Loss: 0.3417\n",
            "Epoch [1/400], Step [186/716], Loss: 0.2751\n",
            "Epoch [1/400], Step [187/716], Loss: 0.2739\n",
            "Epoch [1/400], Step [188/716], Loss: 0.1912\n",
            "Epoch [1/400], Step [189/716], Loss: 0.2332\n",
            "Epoch [1/400], Step [190/716], Loss: 0.2681\n",
            "Epoch [1/400], Step [191/716], Loss: 0.2427\n",
            "Epoch [1/400], Step [192/716], Loss: 0.2714\n",
            "Epoch [1/400], Step [193/716], Loss: 0.2521\n",
            "Epoch [1/400], Step [194/716], Loss: 0.2617\n",
            "Epoch [1/400], Step [195/716], Loss: 0.2438\n",
            "Epoch [1/400], Step [196/716], Loss: 0.2627\n",
            "Epoch [1/400], Step [197/716], Loss: 0.3182\n",
            "Epoch [1/400], Step [198/716], Loss: 0.2164\n",
            "Epoch [1/400], Step [199/716], Loss: 0.2799\n",
            "Epoch [1/400], Step [200/716], Loss: 0.2550\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_step_200.pth\n",
            "Epoch [1/400], Step [201/716], Loss: 0.3202\n",
            "Epoch [1/400], Step [202/716], Loss: 0.2632\n",
            "Epoch [1/400], Step [203/716], Loss: 0.2333\n",
            "Epoch [1/400], Step [204/716], Loss: 0.2164\n",
            "Epoch [1/400], Step [205/716], Loss: 0.2261\n",
            "Epoch [1/400], Step [206/716], Loss: 0.3027\n",
            "Epoch [1/400], Step [207/716], Loss: 0.2850\n",
            "Epoch [1/400], Step [208/716], Loss: 0.2324\n",
            "Epoch [1/400], Step [209/716], Loss: 0.2716\n",
            "Epoch [1/400], Step [210/716], Loss: 0.3280\n",
            "Epoch [1/400], Step [211/716], Loss: 0.2586\n",
            "Epoch [1/400], Step [212/716], Loss: 0.2289\n",
            "Epoch [1/400], Step [213/716], Loss: 0.2685\n",
            "Epoch [1/400], Step [214/716], Loss: 0.3356\n",
            "Epoch [1/400], Step [215/716], Loss: 0.2001\n",
            "Epoch [1/400], Step [216/716], Loss: 0.2639\n",
            "Epoch [1/400], Step [217/716], Loss: 0.2580\n",
            "Epoch [1/400], Step [218/716], Loss: 0.2841\n",
            "Epoch [1/400], Step [219/716], Loss: 0.2311\n",
            "Epoch [1/400], Step [220/716], Loss: 0.2588\n",
            "Epoch [1/400], Step [221/716], Loss: 0.2778\n",
            "Epoch [1/400], Step [222/716], Loss: 0.2004\n",
            "Epoch [1/400], Step [223/716], Loss: 0.2151\n",
            "Epoch [1/400], Step [224/716], Loss: 0.2570\n",
            "Epoch [1/400], Step [225/716], Loss: 0.2844\n",
            "Epoch [1/400], Step [226/716], Loss: 0.2387\n",
            "Epoch [1/400], Step [227/716], Loss: 0.3401\n",
            "Epoch [1/400], Step [228/716], Loss: 0.4721\n",
            "Epoch [1/400], Step [229/716], Loss: 0.2733\n",
            "Epoch [1/400], Step [230/716], Loss: 0.2535\n",
            "Epoch [1/400], Step [231/716], Loss: 0.2001\n",
            "Epoch [1/400], Step [232/716], Loss: 0.2229\n",
            "Epoch [1/400], Step [233/716], Loss: 0.2364\n",
            "Epoch [1/400], Step [234/716], Loss: 0.1661\n",
            "Epoch [1/400], Step [235/716], Loss: 0.3732\n",
            "Epoch [1/400], Step [236/716], Loss: 0.2734\n",
            "Epoch [1/400], Step [237/716], Loss: 0.2626\n",
            "Epoch [1/400], Step [238/716], Loss: 0.2300\n",
            "Epoch [1/400], Step [239/716], Loss: 0.2233\n",
            "Epoch [1/400], Step [240/716], Loss: 0.2672\n",
            "Epoch [1/400], Step [241/716], Loss: 0.4172\n",
            "Epoch [1/400], Step [242/716], Loss: 0.3215\n",
            "Epoch [1/400], Step [243/716], Loss: 0.3209\n",
            "Epoch [1/400], Step [244/716], Loss: 0.2241\n",
            "Epoch [1/400], Step [245/716], Loss: 0.2078\n",
            "Epoch [1/400], Step [246/716], Loss: 0.3114\n",
            "Epoch [1/400], Step [247/716], Loss: 0.2085\n",
            "Epoch [1/400], Step [248/716], Loss: 0.2638\n",
            "Epoch [1/400], Step [249/716], Loss: 0.2185\n",
            "Epoch [1/400], Step [250/716], Loss: 0.2931\n",
            "Epoch [1/400], Step [251/716], Loss: 0.2763\n",
            "Epoch [1/400], Step [252/716], Loss: 0.2949\n",
            "Epoch [1/400], Step [253/716], Loss: 0.1998\n",
            "Epoch [1/400], Step [254/716], Loss: 0.3849\n",
            "Epoch [1/400], Step [255/716], Loss: 0.2336\n",
            "Epoch [1/400], Step [256/716], Loss: 0.1861\n",
            "Epoch [1/400], Step [257/716], Loss: 0.2475\n",
            "Epoch [1/400], Step [258/716], Loss: 0.1784\n",
            "Epoch [1/400], Step [259/716], Loss: 0.2189\n",
            "Epoch [1/400], Step [260/716], Loss: 0.2555\n",
            "Epoch [1/400], Step [261/716], Loss: 0.3062\n",
            "Epoch [1/400], Step [262/716], Loss: 0.3243\n",
            "Epoch [1/400], Step [263/716], Loss: 0.2545\n",
            "Epoch [1/400], Step [264/716], Loss: 0.2869\n",
            "Epoch [1/400], Step [265/716], Loss: 0.1911\n",
            "Epoch [1/400], Step [266/716], Loss: 0.2579\n",
            "Epoch [1/400], Step [267/716], Loss: 0.1717\n",
            "Epoch [1/400], Step [268/716], Loss: 0.2106\n",
            "Epoch [1/400], Step [269/716], Loss: 0.2392\n",
            "Epoch [1/400], Step [270/716], Loss: 0.2898\n",
            "Epoch [1/400], Step [271/716], Loss: 0.2080\n",
            "Epoch [1/400], Step [272/716], Loss: 0.2015\n",
            "Epoch [1/400], Step [273/716], Loss: 0.1892\n",
            "Epoch [1/400], Step [274/716], Loss: 0.2817\n",
            "Epoch [1/400], Step [275/716], Loss: 0.1767\n",
            "Epoch [1/400], Step [276/716], Loss: 0.3520\n",
            "Epoch [1/400], Step [277/716], Loss: 0.1974\n",
            "Epoch [1/400], Step [278/716], Loss: 0.2101\n",
            "Epoch [1/400], Step [279/716], Loss: 0.2567\n",
            "Epoch [1/400], Step [280/716], Loss: 0.2641\n",
            "Epoch [1/400], Step [281/716], Loss: 0.2018\n",
            "Epoch [1/400], Step [282/716], Loss: 0.2192\n",
            "Epoch [1/400], Step [283/716], Loss: 0.2048\n",
            "Epoch [1/400], Step [284/716], Loss: 0.2450\n",
            "Epoch [1/400], Step [285/716], Loss: 0.2326\n",
            "Epoch [1/400], Step [286/716], Loss: 0.3050\n",
            "Epoch [1/400], Step [287/716], Loss: 0.1840\n",
            "Epoch [1/400], Step [288/716], Loss: 0.1792\n",
            "Epoch [1/400], Step [289/716], Loss: 0.2684\n",
            "Epoch [1/400], Step [290/716], Loss: 0.2051\n",
            "Epoch [1/400], Step [291/716], Loss: 0.2381\n",
            "Epoch [1/400], Step [292/716], Loss: 0.2593\n",
            "Epoch [1/400], Step [293/716], Loss: 0.1966\n",
            "Epoch [1/400], Step [294/716], Loss: 0.2148\n",
            "Epoch [1/400], Step [295/716], Loss: 0.2565\n",
            "Epoch [1/400], Step [296/716], Loss: 0.2135\n",
            "Epoch [1/400], Step [297/716], Loss: 0.1593\n",
            "Epoch [1/400], Step [298/716], Loss: 0.2232\n",
            "Epoch [1/400], Step [299/716], Loss: 0.2533\n",
            "Epoch [1/400], Step [300/716], Loss: 0.2207\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_step_300.pth\n",
            "Epoch [1/400], Step [301/716], Loss: 0.2124\n",
            "Epoch [1/400], Step [302/716], Loss: 0.2865\n",
            "Epoch [1/400], Step [303/716], Loss: 0.2508\n",
            "Epoch [1/400], Step [304/716], Loss: 0.2980\n",
            "Epoch [1/400], Step [305/716], Loss: 0.1862\n",
            "Epoch [1/400], Step [306/716], Loss: 0.3137\n",
            "Epoch [1/400], Step [307/716], Loss: 0.2738\n",
            "Epoch [1/400], Step [308/716], Loss: 0.2189\n",
            "Epoch [1/400], Step [309/716], Loss: 0.2087\n",
            "Epoch [1/400], Step [310/716], Loss: 0.1887\n",
            "Epoch [1/400], Step [311/716], Loss: 0.3025\n",
            "Epoch [1/400], Step [312/716], Loss: 0.1780\n",
            "Epoch [1/400], Step [313/716], Loss: 0.3607\n",
            "Epoch [1/400], Step [314/716], Loss: 0.1785\n",
            "Epoch [1/400], Step [315/716], Loss: 0.4345\n",
            "Epoch [1/400], Step [316/716], Loss: 0.2797\n",
            "Epoch [1/400], Step [317/716], Loss: 0.2195\n",
            "Epoch [1/400], Step [318/716], Loss: 0.2927\n",
            "Epoch [1/400], Step [319/716], Loss: 0.2262\n",
            "Epoch [1/400], Step [320/716], Loss: 0.2025\n",
            "Epoch [1/400], Step [321/716], Loss: 0.2331\n",
            "Epoch [1/400], Step [322/716], Loss: 0.2426\n",
            "Epoch [1/400], Step [323/716], Loss: 0.1899\n",
            "Epoch [1/400], Step [324/716], Loss: 0.2356\n",
            "Epoch [1/400], Step [325/716], Loss: 0.2994\n",
            "Epoch [1/400], Step [326/716], Loss: 0.2288\n",
            "Epoch [1/400], Step [327/716], Loss: 0.2750\n",
            "Epoch [1/400], Step [328/716], Loss: 0.2446\n",
            "Epoch [1/400], Step [329/716], Loss: 0.2478\n",
            "Epoch [1/400], Step [330/716], Loss: 0.2709\n",
            "Epoch [1/400], Step [331/716], Loss: 0.2356\n",
            "Epoch [1/400], Step [332/716], Loss: 0.2516\n",
            "Epoch [1/400], Step [333/716], Loss: 0.2686\n",
            "Epoch [1/400], Step [334/716], Loss: 0.2393\n",
            "Epoch [1/400], Step [335/716], Loss: 0.2060\n",
            "Epoch [1/400], Step [336/716], Loss: 0.3260\n",
            "Epoch [1/400], Step [337/716], Loss: 0.2636\n",
            "Epoch [1/400], Step [338/716], Loss: 0.2171\n",
            "Epoch [1/400], Step [339/716], Loss: 0.2501\n",
            "Epoch [1/400], Step [340/716], Loss: 0.2867\n",
            "Epoch [1/400], Step [341/716], Loss: 0.3232\n",
            "Epoch [1/400], Step [342/716], Loss: 0.2128\n",
            "Epoch [1/400], Step [343/716], Loss: 0.2600\n",
            "Epoch [1/400], Step [344/716], Loss: 0.2315\n",
            "Epoch [1/400], Step [345/716], Loss: 0.1892\n",
            "Epoch [1/400], Step [346/716], Loss: 0.2350\n",
            "Epoch [1/400], Step [347/716], Loss: 0.3230\n",
            "Epoch [1/400], Step [348/716], Loss: 0.2049\n",
            "Epoch [1/400], Step [349/716], Loss: 0.2339\n",
            "Epoch [1/400], Step [350/716], Loss: 0.2611\n",
            "Epoch [1/400], Step [351/716], Loss: 0.2986\n",
            "Epoch [1/400], Step [352/716], Loss: 0.1995\n",
            "Epoch [1/400], Step [353/716], Loss: 0.2611\n",
            "Epoch [1/400], Step [354/716], Loss: 0.2691\n",
            "Epoch [1/400], Step [355/716], Loss: 0.2217\n",
            "Epoch [1/400], Step [356/716], Loss: 0.2213\n",
            "Epoch [1/400], Step [357/716], Loss: 0.2289\n",
            "Epoch [1/400], Step [358/716], Loss: 0.2295\n",
            "Epoch [1/400], Step [359/716], Loss: 0.2107\n",
            "Epoch [1/400], Step [360/716], Loss: 0.2170\n",
            "Epoch [1/400], Step [361/716], Loss: 0.2083\n",
            "Epoch [1/400], Step [362/716], Loss: 0.1710\n",
            "Epoch [1/400], Step [363/716], Loss: 0.3281\n",
            "Epoch [1/400], Step [364/716], Loss: 0.2097\n",
            "Epoch [1/400], Step [365/716], Loss: 0.1924\n",
            "Epoch [1/400], Step [366/716], Loss: 0.2937\n",
            "Epoch [1/400], Step [367/716], Loss: 0.2618\n",
            "Epoch [1/400], Step [368/716], Loss: 0.2212\n",
            "Epoch [1/400], Step [369/716], Loss: 0.2171\n",
            "Epoch [1/400], Step [370/716], Loss: 0.2207\n",
            "Epoch [1/400], Step [371/716], Loss: 0.4587\n",
            "Epoch [1/400], Step [372/716], Loss: 0.3246\n",
            "Epoch [1/400], Step [373/716], Loss: 0.2453\n",
            "Epoch [1/400], Step [374/716], Loss: 0.3245\n",
            "Epoch [1/400], Step [375/716], Loss: 0.2369\n",
            "Epoch [1/400], Step [376/716], Loss: 0.2656\n",
            "Epoch [1/400], Step [377/716], Loss: 0.2162\n",
            "Epoch [1/400], Step [378/716], Loss: 0.2700\n",
            "Epoch [1/400], Step [379/716], Loss: 0.2096\n",
            "Epoch [1/400], Step [380/716], Loss: 0.2132\n",
            "Epoch [1/400], Step [381/716], Loss: 0.2049\n",
            "Epoch [1/400], Step [382/716], Loss: 0.1785\n",
            "Epoch [1/400], Step [383/716], Loss: 0.4508\n",
            "Epoch [1/400], Step [384/716], Loss: 0.2475\n",
            "Epoch [1/400], Step [385/716], Loss: 0.3076\n",
            "Epoch [1/400], Step [386/716], Loss: 0.2428\n",
            "Epoch [1/400], Step [387/716], Loss: 0.2623\n",
            "Epoch [1/400], Step [388/716], Loss: 0.1632\n",
            "Epoch [1/400], Step [389/716], Loss: 0.2065\n",
            "Epoch [1/400], Step [390/716], Loss: 0.2455\n",
            "Epoch [1/400], Step [391/716], Loss: 0.2477\n",
            "Epoch [1/400], Step [392/716], Loss: 0.1836\n",
            "Epoch [1/400], Step [393/716], Loss: 0.2023\n",
            "Epoch [1/400], Step [394/716], Loss: 0.1844\n",
            "Epoch [1/400], Step [395/716], Loss: 0.2383\n",
            "Epoch [1/400], Step [396/716], Loss: 0.1793\n",
            "Epoch [1/400], Step [397/716], Loss: 0.3078\n",
            "Epoch [1/400], Step [398/716], Loss: 0.2725\n",
            "Epoch [1/400], Step [399/716], Loss: 0.2228\n",
            "Epoch [1/400], Step [400/716], Loss: 0.3050\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_step_400.pth\n",
            "Epoch [1/400], Step [401/716], Loss: 0.1610\n",
            "Epoch [1/400], Step [402/716], Loss: 0.1687\n",
            "Epoch [1/400], Step [403/716], Loss: 0.3414\n",
            "Epoch [1/400], Step [404/716], Loss: 0.2189\n",
            "Epoch [1/400], Step [405/716], Loss: 0.2052\n",
            "Epoch [1/400], Step [406/716], Loss: 0.1795\n",
            "Epoch [1/400], Step [407/716], Loss: 0.1754\n",
            "Epoch [1/400], Step [408/716], Loss: 0.1800\n",
            "Epoch [1/400], Step [409/716], Loss: 0.1928\n",
            "Epoch [1/400], Step [410/716], Loss: 0.2143\n",
            "Epoch [1/400], Step [411/716], Loss: 0.2146\n",
            "Epoch [1/400], Step [412/716], Loss: 0.1991\n",
            "Epoch [1/400], Step [413/716], Loss: 0.1640\n",
            "Epoch [1/400], Step [414/716], Loss: 0.4141\n",
            "Epoch [1/400], Step [415/716], Loss: 0.2006\n",
            "Epoch [1/400], Step [416/716], Loss: 0.2357\n",
            "Epoch [1/400], Step [417/716], Loss: 0.2906\n",
            "Epoch [1/400], Step [418/716], Loss: 0.1860\n",
            "Epoch [1/400], Step [419/716], Loss: 0.2028\n",
            "Epoch [1/400], Step [420/716], Loss: 0.2374\n",
            "Epoch [1/400], Step [421/716], Loss: 0.2059\n",
            "Epoch [1/400], Step [422/716], Loss: 0.2336\n",
            "Epoch [1/400], Step [423/716], Loss: 0.4869\n",
            "Epoch [1/400], Step [424/716], Loss: 0.2562\n",
            "Epoch [1/400], Step [425/716], Loss: 0.2388\n",
            "Epoch [1/400], Step [426/716], Loss: 0.2912\n",
            "Epoch [1/400], Step [427/716], Loss: 0.3274\n",
            "Epoch [1/400], Step [428/716], Loss: 0.2795\n",
            "Epoch [1/400], Step [429/716], Loss: 0.1703\n",
            "Epoch [1/400], Step [430/716], Loss: 0.1960\n",
            "Epoch [1/400], Step [431/716], Loss: 0.1938\n",
            "Epoch [1/400], Step [432/716], Loss: 0.1936\n",
            "Epoch [1/400], Step [433/716], Loss: 0.2743\n",
            "Epoch [1/400], Step [434/716], Loss: 0.3070\n",
            "Epoch [1/400], Step [435/716], Loss: 0.2016\n",
            "Epoch [1/400], Step [436/716], Loss: 0.1922\n",
            "Epoch [1/400], Step [437/716], Loss: 0.2577\n",
            "Epoch [1/400], Step [438/716], Loss: 0.3027\n",
            "Epoch [1/400], Step [439/716], Loss: 0.2270\n",
            "Epoch [1/400], Step [440/716], Loss: 0.2094\n",
            "Epoch [1/400], Step [441/716], Loss: 0.1960\n",
            "Epoch [1/400], Step [442/716], Loss: 0.3642\n",
            "Epoch [1/400], Step [443/716], Loss: 0.2256\n",
            "Epoch [1/400], Step [444/716], Loss: 0.2130\n",
            "Epoch [1/400], Step [445/716], Loss: 0.3616\n",
            "Epoch [1/400], Step [446/716], Loss: 0.2575\n",
            "Epoch [1/400], Step [447/716], Loss: 0.2420\n",
            "Epoch [1/400], Step [448/716], Loss: 0.3878\n",
            "Epoch [1/400], Step [449/716], Loss: 0.2687\n",
            "Epoch [1/400], Step [450/716], Loss: 0.2000\n",
            "Epoch [1/400], Step [451/716], Loss: 0.2252\n",
            "Epoch [1/400], Step [452/716], Loss: 0.3017\n",
            "Epoch [1/400], Step [453/716], Loss: 0.2507\n",
            "Epoch [1/400], Step [454/716], Loss: 0.1885\n",
            "Epoch [1/400], Step [455/716], Loss: 0.2360\n",
            "Epoch [1/400], Step [456/716], Loss: 0.2336\n",
            "Epoch [1/400], Step [457/716], Loss: 0.1973\n",
            "Epoch [1/400], Step [458/716], Loss: 0.1955\n",
            "Epoch [1/400], Step [459/716], Loss: 0.2967\n",
            "Epoch [1/400], Step [460/716], Loss: 0.2751\n",
            "Epoch [1/400], Step [461/716], Loss: 0.1816\n",
            "Epoch [1/400], Step [462/716], Loss: 0.2325\n",
            "Epoch [1/400], Step [463/716], Loss: 0.2086\n",
            "Epoch [1/400], Step [464/716], Loss: 0.2552\n",
            "Epoch [1/400], Step [465/716], Loss: 0.2203\n",
            "Epoch [1/400], Step [466/716], Loss: 0.1897\n",
            "Epoch [1/400], Step [467/716], Loss: 0.2505\n",
            "Epoch [1/400], Step [468/716], Loss: 0.2209\n",
            "Epoch [1/400], Step [469/716], Loss: 0.1530\n",
            "Epoch [1/400], Step [470/716], Loss: 0.1630\n",
            "Epoch [1/400], Step [471/716], Loss: 0.1808\n",
            "Epoch [1/400], Step [472/716], Loss: 0.2154\n",
            "Epoch [1/400], Step [473/716], Loss: 0.2044\n",
            "Epoch [1/400], Step [474/716], Loss: 0.2377\n",
            "Epoch [1/400], Step [475/716], Loss: 0.3047\n",
            "Epoch [1/400], Step [476/716], Loss: 0.2120\n",
            "Epoch [1/400], Step [477/716], Loss: 0.1769\n",
            "Epoch [1/400], Step [478/716], Loss: 0.1885\n",
            "Epoch [1/400], Step [479/716], Loss: 0.1732\n",
            "Epoch [1/400], Step [480/716], Loss: 0.1689\n",
            "Epoch [1/400], Step [481/716], Loss: 0.1505\n",
            "Epoch [1/400], Step [482/716], Loss: 0.1914\n",
            "Epoch [1/400], Step [483/716], Loss: 0.2142\n",
            "Epoch [1/400], Step [484/716], Loss: 0.1647\n",
            "Epoch [1/400], Step [485/716], Loss: 0.2055\n",
            "Epoch [1/400], Step [486/716], Loss: 0.2334\n",
            "Epoch [1/400], Step [487/716], Loss: 0.1640\n",
            "Epoch [1/400], Step [488/716], Loss: 0.2565\n",
            "Epoch [1/400], Step [489/716], Loss: 0.2025\n",
            "Epoch [1/400], Step [490/716], Loss: 0.2084\n",
            "Epoch [1/400], Step [491/716], Loss: 0.2788\n",
            "Epoch [1/400], Step [492/716], Loss: 0.2049\n",
            "Epoch [1/400], Step [493/716], Loss: 0.1857\n",
            "Epoch [1/400], Step [494/716], Loss: 0.1776\n",
            "Epoch [1/400], Step [495/716], Loss: 0.1738\n",
            "Epoch [1/400], Step [496/716], Loss: 0.2136\n",
            "Epoch [1/400], Step [497/716], Loss: 0.3092\n",
            "Epoch [1/400], Step [498/716], Loss: 0.1614\n",
            "Epoch [1/400], Step [499/716], Loss: 0.1926\n",
            "Epoch [1/400], Step [500/716], Loss: 0.3063\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_step_500.pth\n",
            "Epoch [1/400], Step [501/716], Loss: 0.3056\n",
            "Epoch [1/400], Step [502/716], Loss: 0.2594\n",
            "Epoch [1/400], Step [503/716], Loss: 0.2255\n",
            "Epoch [1/400], Step [504/716], Loss: 0.2451\n",
            "Epoch [1/400], Step [505/716], Loss: 0.2185\n",
            "Epoch [1/400], Step [506/716], Loss: 0.1822\n",
            "Epoch [1/400], Step [507/716], Loss: 0.2376\n",
            "Epoch [1/400], Step [508/716], Loss: 0.2005\n",
            "Epoch [1/400], Step [509/716], Loss: 0.2046\n",
            "Epoch [1/400], Step [510/716], Loss: 0.2172\n",
            "Epoch [1/400], Step [511/716], Loss: 0.1905\n",
            "Epoch [1/400], Step [512/716], Loss: 0.2848\n",
            "Epoch [1/400], Step [513/716], Loss: 0.2084\n",
            "Epoch [1/400], Step [514/716], Loss: 0.1253\n",
            "Epoch [1/400], Step [515/716], Loss: 0.2023\n",
            "Epoch [1/400], Step [516/716], Loss: 0.4017\n",
            "Epoch [1/400], Step [517/716], Loss: 0.1926\n",
            "Epoch [1/400], Step [518/716], Loss: 0.3488\n",
            "Epoch [1/400], Step [519/716], Loss: 0.2325\n",
            "Epoch [1/400], Step [520/716], Loss: 0.1602\n",
            "Epoch [1/400], Step [521/716], Loss: 0.2034\n",
            "Epoch [1/400], Step [522/716], Loss: 0.2389\n",
            "Epoch [1/400], Step [523/716], Loss: 0.2062\n",
            "Epoch [1/400], Step [524/716], Loss: 0.2221\n",
            "Epoch [1/400], Step [525/716], Loss: 0.1909\n",
            "Epoch [1/400], Step [526/716], Loss: 0.1646\n",
            "Epoch [1/400], Step [527/716], Loss: 0.2565\n",
            "Epoch [1/400], Step [528/716], Loss: 0.1936\n",
            "Epoch [1/400], Step [529/716], Loss: 0.1805\n",
            "Epoch [1/400], Step [530/716], Loss: 0.1564\n",
            "Epoch [1/400], Step [531/716], Loss: 0.2204\n",
            "Epoch [1/400], Step [532/716], Loss: 0.2180\n",
            "Epoch [1/400], Step [533/716], Loss: 0.1647\n",
            "Epoch [1/400], Step [534/716], Loss: 0.1961\n",
            "Epoch [1/400], Step [535/716], Loss: 0.1669\n",
            "Epoch [1/400], Step [536/716], Loss: 0.1479\n",
            "Epoch [1/400], Step [537/716], Loss: 0.1816\n",
            "Epoch [1/400], Step [538/716], Loss: 0.2028\n",
            "Epoch [1/400], Step [539/716], Loss: 0.1979\n",
            "Epoch [1/400], Step [540/716], Loss: 0.1847\n",
            "Epoch [1/400], Step [541/716], Loss: 0.1886\n",
            "Epoch [1/400], Step [542/716], Loss: 0.2619\n",
            "Epoch [1/400], Step [543/716], Loss: 0.1811\n",
            "Epoch [1/400], Step [544/716], Loss: 0.1928\n",
            "Epoch [1/400], Step [545/716], Loss: 0.1878\n",
            "Epoch [1/400], Step [546/716], Loss: 0.2609\n",
            "Epoch [1/400], Step [547/716], Loss: 0.1860\n",
            "Epoch [1/400], Step [548/716], Loss: 0.1670\n",
            "Epoch [1/400], Step [549/716], Loss: 0.1711\n",
            "Epoch [1/400], Step [550/716], Loss: 0.2031\n",
            "Epoch [1/400], Step [551/716], Loss: 0.1472\n",
            "Epoch [1/400], Step [552/716], Loss: 0.1937\n",
            "Epoch [1/400], Step [553/716], Loss: 0.2169\n",
            "Epoch [1/400], Step [554/716], Loss: 0.2109\n",
            "Epoch [1/400], Step [555/716], Loss: 0.2056\n",
            "Epoch [1/400], Step [556/716], Loss: 0.2105\n",
            "Epoch [1/400], Step [557/716], Loss: 0.2427\n",
            "Epoch [1/400], Step [558/716], Loss: 0.2270\n",
            "Epoch [1/400], Step [559/716], Loss: 0.1660\n",
            "Epoch [1/400], Step [560/716], Loss: 0.2014\n",
            "Epoch [1/400], Step [561/716], Loss: 0.1873\n",
            "Epoch [1/400], Step [562/716], Loss: 0.1883\n",
            "Epoch [1/400], Step [563/716], Loss: 0.2176\n",
            "Epoch [1/400], Step [564/716], Loss: 0.1959\n",
            "Epoch [1/400], Step [565/716], Loss: 0.2659\n",
            "Epoch [1/400], Step [566/716], Loss: 0.1738\n",
            "Epoch [1/400], Step [567/716], Loss: 0.1693\n",
            "Epoch [1/400], Step [568/716], Loss: 0.1561\n",
            "Epoch [1/400], Step [569/716], Loss: 0.1516\n",
            "Epoch [1/400], Step [570/716], Loss: 0.2566\n",
            "Epoch [1/400], Step [571/716], Loss: 0.1779\n",
            "Epoch [1/400], Step [572/716], Loss: 0.2296\n",
            "Epoch [1/400], Step [573/716], Loss: 0.2263\n",
            "Epoch [1/400], Step [574/716], Loss: 0.1558\n",
            "Epoch [1/400], Step [575/716], Loss: 0.2186\n",
            "Epoch [1/400], Step [576/716], Loss: 0.1787\n",
            "Epoch [1/400], Step [577/716], Loss: 0.1583\n",
            "Epoch [1/400], Step [578/716], Loss: 0.2361\n",
            "Epoch [1/400], Step [579/716], Loss: 0.2328\n",
            "Epoch [1/400], Step [580/716], Loss: 0.1969\n",
            "Epoch [1/400], Step [581/716], Loss: 0.1841\n",
            "Epoch [1/400], Step [582/716], Loss: 0.2073\n",
            "Epoch [1/400], Step [583/716], Loss: 0.2170\n",
            "Epoch [1/400], Step [584/716], Loss: 0.1915\n",
            "Epoch [1/400], Step [585/716], Loss: 0.2051\n",
            "Epoch [1/400], Step [586/716], Loss: 0.1773\n",
            "Epoch [1/400], Step [587/716], Loss: 0.3243\n",
            "Epoch [1/400], Step [588/716], Loss: 0.1912\n",
            "Epoch [1/400], Step [589/716], Loss: 0.1756\n",
            "Epoch [1/400], Step [590/716], Loss: 0.2123\n",
            "Epoch [1/400], Step [591/716], Loss: 0.2722\n",
            "Epoch [1/400], Step [592/716], Loss: 0.2256\n",
            "Epoch [1/400], Step [593/716], Loss: 0.1564\n",
            "Epoch [1/400], Step [594/716], Loss: 0.1939\n",
            "Epoch [1/400], Step [595/716], Loss: 0.2183\n",
            "Epoch [1/400], Step [596/716], Loss: 0.2193\n",
            "Epoch [1/400], Step [597/716], Loss: 0.2051\n",
            "Epoch [1/400], Step [598/716], Loss: 0.2213\n",
            "Epoch [1/400], Step [599/716], Loss: 0.1877\n",
            "Epoch [1/400], Step [600/716], Loss: 0.1868\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_step_600.pth\n",
            "Epoch [1/400], Step [601/716], Loss: 0.2475\n",
            "Epoch [1/400], Step [602/716], Loss: 0.2205\n",
            "Epoch [1/400], Step [603/716], Loss: 0.1735\n",
            "Epoch [1/400], Step [604/716], Loss: 0.2034\n",
            "Epoch [1/400], Step [605/716], Loss: 0.1450\n",
            "Epoch [1/400], Step [606/716], Loss: 0.2101\n",
            "Epoch [1/400], Step [607/716], Loss: 0.1853\n",
            "Epoch [1/400], Step [608/716], Loss: 0.2155\n",
            "Epoch [1/400], Step [609/716], Loss: 0.1878\n",
            "Epoch [1/400], Step [610/716], Loss: 0.1683\n",
            "Epoch [1/400], Step [611/716], Loss: 0.2302\n",
            "Epoch [1/400], Step [612/716], Loss: 0.1514\n",
            "Epoch [1/400], Step [613/716], Loss: 0.1588\n",
            "Epoch [1/400], Step [614/716], Loss: 0.2191\n",
            "Epoch [1/400], Step [615/716], Loss: 0.2142\n",
            "Epoch [1/400], Step [616/716], Loss: 0.1636\n",
            "Epoch [1/400], Step [617/716], Loss: 0.2321\n",
            "Epoch [1/400], Step [618/716], Loss: 0.2038\n",
            "Epoch [1/400], Step [619/716], Loss: 0.1715\n",
            "Epoch [1/400], Step [620/716], Loss: 0.2740\n",
            "Epoch [1/400], Step [621/716], Loss: 0.2220\n",
            "Epoch [1/400], Step [622/716], Loss: 0.1851\n",
            "Epoch [1/400], Step [623/716], Loss: 0.1915\n",
            "Epoch [1/400], Step [624/716], Loss: 0.1793\n",
            "Epoch [1/400], Step [625/716], Loss: 0.2802\n",
            "Epoch [1/400], Step [626/716], Loss: 0.2964\n",
            "Epoch [1/400], Step [627/716], Loss: 0.2489\n",
            "Epoch [1/400], Step [628/716], Loss: 0.2258\n",
            "Epoch [1/400], Step [629/716], Loss: 0.2831\n",
            "Epoch [1/400], Step [630/716], Loss: 0.1619\n",
            "Epoch [1/400], Step [631/716], Loss: 0.2068\n",
            "Epoch [1/400], Step [632/716], Loss: 0.2508\n",
            "Epoch [1/400], Step [633/716], Loss: 0.1918\n",
            "Epoch [1/400], Step [634/716], Loss: 0.2823\n",
            "Epoch [1/400], Step [635/716], Loss: 0.1476\n",
            "Epoch [1/400], Step [636/716], Loss: 0.2100\n",
            "Epoch [1/400], Step [637/716], Loss: 0.1928\n",
            "Epoch [1/400], Step [638/716], Loss: 0.2034\n",
            "Epoch [1/400], Step [639/716], Loss: 0.2383\n",
            "Epoch [1/400], Step [640/716], Loss: 0.1462\n",
            "Epoch [1/400], Step [641/716], Loss: 0.1790\n",
            "Epoch [1/400], Step [642/716], Loss: 0.2170\n",
            "Epoch [1/400], Step [643/716], Loss: 0.2588\n",
            "Epoch [1/400], Step [644/716], Loss: 0.2890\n",
            "Epoch [1/400], Step [645/716], Loss: 0.1874\n",
            "Epoch [1/400], Step [646/716], Loss: 0.1882\n",
            "Epoch [1/400], Step [647/716], Loss: 0.1630\n",
            "Epoch [1/400], Step [648/716], Loss: 0.1972\n",
            "Epoch [1/400], Step [649/716], Loss: 0.1765\n",
            "Epoch [1/400], Step [650/716], Loss: 0.2967\n",
            "Epoch [1/400], Step [651/716], Loss: 0.1912\n",
            "Epoch [1/400], Step [652/716], Loss: 0.3355\n",
            "Epoch [1/400], Step [653/716], Loss: 0.2889\n",
            "Epoch [1/400], Step [654/716], Loss: 0.1727\n",
            "Epoch [1/400], Step [655/716], Loss: 0.1947\n",
            "Epoch [1/400], Step [656/716], Loss: 0.1533\n",
            "Epoch [1/400], Step [657/716], Loss: 0.1975\n",
            "Epoch [1/400], Step [658/716], Loss: 0.2089\n",
            "Epoch [1/400], Step [659/716], Loss: 0.2433\n",
            "Epoch [1/400], Step [660/716], Loss: 0.1849\n",
            "Epoch [1/400], Step [661/716], Loss: 0.3094\n",
            "Epoch [1/400], Step [662/716], Loss: 0.2078\n",
            "Epoch [1/400], Step [663/716], Loss: 0.1994\n",
            "Epoch [1/400], Step [664/716], Loss: 0.2919\n",
            "Epoch [1/400], Step [665/716], Loss: 0.1867\n",
            "Epoch [1/400], Step [666/716], Loss: 0.2034\n",
            "Epoch [1/400], Step [667/716], Loss: 0.2353\n",
            "Epoch [1/400], Step [668/716], Loss: 0.1949\n",
            "Epoch [1/400], Step [669/716], Loss: 0.1649\n",
            "Epoch [1/400], Step [670/716], Loss: 0.2557\n",
            "Epoch [1/400], Step [671/716], Loss: 0.2283\n",
            "Epoch [1/400], Step [672/716], Loss: 0.1681\n",
            "Epoch [1/400], Step [673/716], Loss: 0.1928\n",
            "Epoch [1/400], Step [674/716], Loss: 0.2795\n",
            "Epoch [1/400], Step [675/716], Loss: 0.1708\n",
            "Epoch [1/400], Step [676/716], Loss: 0.2645\n",
            "Epoch [1/400], Step [677/716], Loss: 0.1191\n",
            "Epoch [1/400], Step [678/716], Loss: 0.1459\n",
            "Epoch [1/400], Step [679/716], Loss: 0.2733\n",
            "Epoch [1/400], Step [680/716], Loss: 0.1791\n",
            "Epoch [1/400], Step [681/716], Loss: 0.1793\n",
            "Epoch [1/400], Step [682/716], Loss: 0.1487\n",
            "Epoch [1/400], Step [683/716], Loss: 0.1758\n",
            "Epoch [1/400], Step [684/716], Loss: 0.1706\n",
            "Epoch [1/400], Step [685/716], Loss: 0.1918\n",
            "Epoch [1/400], Step [686/716], Loss: 0.1487\n",
            "Epoch [1/400], Step [687/716], Loss: 0.1742\n",
            "Epoch [1/400], Step [688/716], Loss: 0.1867\n",
            "Epoch [1/400], Step [689/716], Loss: 0.1694\n",
            "Epoch [1/400], Step [690/716], Loss: 0.2322\n",
            "Epoch [1/400], Step [691/716], Loss: 0.1770\n",
            "Epoch [1/400], Step [692/716], Loss: 0.3132\n",
            "Epoch [1/400], Step [693/716], Loss: 0.1883\n",
            "Epoch [1/400], Step [694/716], Loss: 0.2158\n",
            "Epoch [1/400], Step [695/716], Loss: 0.1812\n",
            "Epoch [1/400], Step [696/716], Loss: 0.1425\n",
            "Epoch [1/400], Step [697/716], Loss: 0.2035\n",
            "Epoch [1/400], Step [698/716], Loss: 0.1665\n",
            "Epoch [1/400], Step [699/716], Loss: 0.1966\n",
            "Epoch [1/400], Step [700/716], Loss: 0.1681\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_step_700.pth\n",
            "Epoch [1/400], Step [701/716], Loss: 0.2100\n",
            "Epoch [1/400], Step [702/716], Loss: 0.1628\n",
            "Epoch [1/400], Step [703/716], Loss: 0.2679\n",
            "Epoch [1/400], Step [704/716], Loss: 0.1521\n",
            "Epoch [1/400], Step [705/716], Loss: 0.2911\n",
            "Epoch [1/400], Step [706/716], Loss: 0.2145\n",
            "Epoch [1/400], Step [707/716], Loss: 0.1414\n",
            "Epoch [1/400], Step [708/716], Loss: 0.1925\n",
            "Epoch [1/400], Step [709/716], Loss: 0.1451\n",
            "Epoch [1/400], Step [710/716], Loss: 0.1419\n",
            "Epoch [1/400], Step [711/716], Loss: 0.1813\n",
            "Epoch [1/400], Step [712/716], Loss: 0.1865\n",
            "Epoch [1/400], Step [713/716], Loss: 0.1403\n",
            "Epoch [1/400], Step [714/716], Loss: 0.1935\n",
            "Epoch [1/400], Step [715/716], Loss: 0.2082\n",
            "Epoch [1/400], Step [716/716], Loss: 0.1525\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_1_last.pth\n",
            "Epoch [2/400], Step [1/716], Loss: 0.1332\n",
            "Epoch [2/400], Step [2/716], Loss: 0.3055\n",
            "Epoch [2/400], Step [3/716], Loss: 0.1868\n",
            "Epoch [2/400], Step [4/716], Loss: 0.2585\n",
            "Epoch [2/400], Step [5/716], Loss: 0.2586\n",
            "Epoch [2/400], Step [6/716], Loss: 0.2431\n",
            "Epoch [2/400], Step [7/716], Loss: 0.1883\n",
            "Epoch [2/400], Step [8/716], Loss: 0.1917\n",
            "Epoch [2/400], Step [9/716], Loss: 0.1577\n",
            "Epoch [2/400], Step [10/716], Loss: 0.1921\n",
            "Epoch [2/400], Step [11/716], Loss: 0.1525\n",
            "Epoch [2/400], Step [12/716], Loss: 0.1586\n",
            "Epoch [2/400], Step [13/716], Loss: 0.1736\n",
            "Epoch [2/400], Step [14/716], Loss: 0.1851\n",
            "Epoch [2/400], Step [15/716], Loss: 0.1744\n",
            "Epoch [2/400], Step [16/716], Loss: 0.1584\n",
            "Epoch [2/400], Step [17/716], Loss: 0.1657\n",
            "Epoch [2/400], Step [18/716], Loss: 0.2636\n",
            "Epoch [2/400], Step [19/716], Loss: 0.2136\n",
            "Epoch [2/400], Step [20/716], Loss: 0.1935\n",
            "Epoch [2/400], Step [21/716], Loss: 0.1823\n",
            "Epoch [2/400], Step [22/716], Loss: 0.1502\n",
            "Epoch [2/400], Step [23/716], Loss: 0.2291\n",
            "Epoch [2/400], Step [24/716], Loss: 0.1819\n",
            "Epoch [2/400], Step [25/716], Loss: 0.2011\n",
            "Epoch [2/400], Step [26/716], Loss: 0.2064\n",
            "Epoch [2/400], Step [27/716], Loss: 0.2006\n",
            "Epoch [2/400], Step [28/716], Loss: 0.1582\n",
            "Epoch [2/400], Step [29/716], Loss: 0.1792\n",
            "Epoch [2/400], Step [30/716], Loss: 0.2730\n",
            "Epoch [2/400], Step [31/716], Loss: 0.2360\n",
            "Epoch [2/400], Step [32/716], Loss: 0.1853\n",
            "Epoch [2/400], Step [33/716], Loss: 0.2059\n",
            "Epoch [2/400], Step [34/716], Loss: 0.2078\n",
            "Epoch [2/400], Step [35/716], Loss: 0.1298\n",
            "Epoch [2/400], Step [36/716], Loss: 0.2806\n",
            "Epoch [2/400], Step [37/716], Loss: 0.2071\n",
            "Epoch [2/400], Step [38/716], Loss: 0.1960\n",
            "Epoch [2/400], Step [39/716], Loss: 0.2581\n",
            "Epoch [2/400], Step [40/716], Loss: 0.1824\n",
            "Epoch [2/400], Step [41/716], Loss: 0.2083\n",
            "Epoch [2/400], Step [42/716], Loss: 0.1832\n",
            "Epoch [2/400], Step [43/716], Loss: 0.1857\n",
            "Epoch [2/400], Step [44/716], Loss: 0.1422\n",
            "Epoch [2/400], Step [45/716], Loss: 0.1798\n",
            "Epoch [2/400], Step [46/716], Loss: 0.1754\n",
            "Epoch [2/400], Step [47/716], Loss: 0.2412\n",
            "Epoch [2/400], Step [48/716], Loss: 0.1561\n",
            "Epoch [2/400], Step [49/716], Loss: 0.1779\n",
            "Epoch [2/400], Step [50/716], Loss: 0.2082\n",
            "Epoch [2/400], Step [51/716], Loss: 0.1570\n",
            "Epoch [2/400], Step [52/716], Loss: 0.1467\n",
            "Epoch [2/400], Step [53/716], Loss: 0.2368\n",
            "Epoch [2/400], Step [54/716], Loss: 0.1352\n",
            "Epoch [2/400], Step [55/716], Loss: 0.1482\n",
            "Epoch [2/400], Step [56/716], Loss: 0.2895\n",
            "Epoch [2/400], Step [57/716], Loss: 0.1541\n",
            "Epoch [2/400], Step [58/716], Loss: 0.2377\n",
            "Epoch [2/400], Step [59/716], Loss: 0.3282\n",
            "Epoch [2/400], Step [60/716], Loss: 0.2701\n",
            "Epoch [2/400], Step [61/716], Loss: 0.2291\n",
            "Epoch [2/400], Step [62/716], Loss: 0.1773\n",
            "Epoch [2/400], Step [63/716], Loss: 0.1865\n",
            "Epoch [2/400], Step [64/716], Loss: 0.1756\n",
            "Epoch [2/400], Step [65/716], Loss: 0.1760\n",
            "Epoch [2/400], Step [66/716], Loss: 0.1977\n",
            "Epoch [2/400], Step [67/716], Loss: 0.1807\n",
            "Epoch [2/400], Step [68/716], Loss: 0.1891\n",
            "Epoch [2/400], Step [69/716], Loss: 0.3089\n",
            "Epoch [2/400], Step [70/716], Loss: 0.1507\n",
            "Epoch [2/400], Step [71/716], Loss: 0.2447\n",
            "Epoch [2/400], Step [72/716], Loss: 0.1756\n",
            "Epoch [2/400], Step [73/716], Loss: 0.1884\n",
            "Epoch [2/400], Step [74/716], Loss: 0.2363\n",
            "Epoch [2/400], Step [75/716], Loss: 0.1869\n",
            "Epoch [2/400], Step [76/716], Loss: 0.1651\n",
            "Epoch [2/400], Step [77/716], Loss: 0.2305\n",
            "Epoch [2/400], Step [78/716], Loss: 0.1826\n",
            "Epoch [2/400], Step [79/716], Loss: 0.1902\n",
            "Epoch [2/400], Step [80/716], Loss: 0.2235\n",
            "Epoch [2/400], Step [81/716], Loss: 0.2431\n",
            "Epoch [2/400], Step [82/716], Loss: 0.2868\n",
            "Epoch [2/400], Step [83/716], Loss: 0.1583\n",
            "Epoch [2/400], Step [84/716], Loss: 0.1859\n",
            "Epoch [2/400], Step [85/716], Loss: 0.1500\n",
            "Epoch [2/400], Step [86/716], Loss: 0.2105\n",
            "Epoch [2/400], Step [87/716], Loss: 0.1995\n",
            "Epoch [2/400], Step [88/716], Loss: 0.1365\n",
            "Epoch [2/400], Step [89/716], Loss: 0.2155\n",
            "Epoch [2/400], Step [90/716], Loss: 0.1555\n",
            "Epoch [2/400], Step [91/716], Loss: 0.1707\n",
            "Epoch [2/400], Step [92/716], Loss: 0.1471\n",
            "Epoch [2/400], Step [93/716], Loss: 0.2322\n",
            "Epoch [2/400], Step [94/716], Loss: 0.1742\n",
            "Epoch [2/400], Step [95/716], Loss: 0.1803\n",
            "Epoch [2/400], Step [96/716], Loss: 0.1966\n",
            "Epoch [2/400], Step [97/716], Loss: 0.1645\n",
            "Epoch [2/400], Step [98/716], Loss: 0.3081\n",
            "Epoch [2/400], Step [99/716], Loss: 0.1898\n",
            "Epoch [2/400], Step [100/716], Loss: 0.1527\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_step_100.pth\n",
            "Epoch [2/400], Step [101/716], Loss: 0.2414\n",
            "Epoch [2/400], Step [102/716], Loss: 0.1482\n",
            "Epoch [2/400], Step [103/716], Loss: 0.1541\n",
            "Epoch [2/400], Step [104/716], Loss: 0.2849\n",
            "Epoch [2/400], Step [105/716], Loss: 0.2144\n",
            "Epoch [2/400], Step [106/716], Loss: 0.2010\n",
            "Epoch [2/400], Step [107/716], Loss: 0.1440\n",
            "Epoch [2/400], Step [108/716], Loss: 0.2495\n",
            "Epoch [2/400], Step [109/716], Loss: 0.1844\n",
            "Epoch [2/400], Step [110/716], Loss: 0.1948\n",
            "Epoch [2/400], Step [111/716], Loss: 0.1380\n",
            "Epoch [2/400], Step [112/716], Loss: 0.1954\n",
            "Epoch [2/400], Step [113/716], Loss: 0.1555\n",
            "Epoch [2/400], Step [114/716], Loss: 0.1967\n",
            "Epoch [2/400], Step [115/716], Loss: 0.1709\n",
            "Epoch [2/400], Step [116/716], Loss: 0.2414\n",
            "Epoch [2/400], Step [117/716], Loss: 0.1496\n",
            "Epoch [2/400], Step [118/716], Loss: 0.1938\n",
            "Epoch [2/400], Step [119/716], Loss: 0.1794\n",
            "Epoch [2/400], Step [120/716], Loss: 0.1596\n",
            "Epoch [2/400], Step [121/716], Loss: 0.2055\n",
            "Epoch [2/400], Step [122/716], Loss: 0.1871\n",
            "Epoch [2/400], Step [123/716], Loss: 0.2679\n",
            "Epoch [2/400], Step [124/716], Loss: 0.1577\n",
            "Epoch [2/400], Step [125/716], Loss: 0.1743\n",
            "Epoch [2/400], Step [126/716], Loss: 0.2047\n",
            "Epoch [2/400], Step [127/716], Loss: 0.3352\n",
            "Epoch [2/400], Step [128/716], Loss: 0.1639\n",
            "Epoch [2/400], Step [129/716], Loss: 0.1952\n",
            "Epoch [2/400], Step [130/716], Loss: 0.1608\n",
            "Epoch [2/400], Step [131/716], Loss: 0.1742\n",
            "Epoch [2/400], Step [132/716], Loss: 0.2196\n",
            "Epoch [2/400], Step [133/716], Loss: 0.2258\n",
            "Epoch [2/400], Step [134/716], Loss: 0.1618\n",
            "Epoch [2/400], Step [135/716], Loss: 0.1590\n",
            "Epoch [2/400], Step [136/716], Loss: 0.1499\n",
            "Epoch [2/400], Step [137/716], Loss: 0.1543\n",
            "Epoch [2/400], Step [138/716], Loss: 0.1852\n",
            "Epoch [2/400], Step [139/716], Loss: 0.1598\n",
            "Epoch [2/400], Step [140/716], Loss: 0.1529\n",
            "Epoch [2/400], Step [141/716], Loss: 0.1374\n",
            "Epoch [2/400], Step [142/716], Loss: 0.2601\n",
            "Epoch [2/400], Step [143/716], Loss: 0.1516\n",
            "Epoch [2/400], Step [144/716], Loss: 0.1736\n",
            "Epoch [2/400], Step [145/716], Loss: 0.1734\n",
            "Epoch [2/400], Step [146/716], Loss: 0.2002\n",
            "Epoch [2/400], Step [147/716], Loss: 0.2626\n",
            "Epoch [2/400], Step [148/716], Loss: 0.1865\n",
            "Epoch [2/400], Step [149/716], Loss: 0.1659\n",
            "Epoch [2/400], Step [150/716], Loss: 0.1682\n",
            "Epoch [2/400], Step [151/716], Loss: 0.1805\n",
            "Epoch [2/400], Step [152/716], Loss: 0.1411\n",
            "Epoch [2/400], Step [153/716], Loss: 0.1836\n",
            "Epoch [2/400], Step [154/716], Loss: 0.2361\n",
            "Epoch [2/400], Step [155/716], Loss: 0.1403\n",
            "Epoch [2/400], Step [156/716], Loss: 0.2132\n",
            "Epoch [2/400], Step [157/716], Loss: 0.2371\n",
            "Epoch [2/400], Step [158/716], Loss: 0.2137\n",
            "Epoch [2/400], Step [159/716], Loss: 0.2305\n",
            "Epoch [2/400], Step [160/716], Loss: 0.2040\n",
            "Epoch [2/400], Step [161/716], Loss: 0.1465\n",
            "Epoch [2/400], Step [162/716], Loss: 0.1897\n",
            "Epoch [2/400], Step [163/716], Loss: 0.1603\n",
            "Epoch [2/400], Step [164/716], Loss: 0.3086\n",
            "Epoch [2/400], Step [165/716], Loss: 0.2070\n",
            "Epoch [2/400], Step [166/716], Loss: 0.2392\n",
            "Epoch [2/400], Step [167/716], Loss: 0.1709\n",
            "Epoch [2/400], Step [168/716], Loss: 0.2158\n",
            "Epoch [2/400], Step [169/716], Loss: 0.2407\n",
            "Epoch [2/400], Step [170/716], Loss: 0.2127\n",
            "Epoch [2/400], Step [171/716], Loss: 0.1719\n",
            "Epoch [2/400], Step [172/716], Loss: 0.1462\n",
            "Epoch [2/400], Step [173/716], Loss: 0.2174\n",
            "Epoch [2/400], Step [174/716], Loss: 0.1741\n",
            "Epoch [2/400], Step [175/716], Loss: 0.2477\n",
            "Epoch [2/400], Step [176/716], Loss: 0.1849\n",
            "Epoch [2/400], Step [177/716], Loss: 0.1584\n",
            "Epoch [2/400], Step [178/716], Loss: 0.1464\n",
            "Epoch [2/400], Step [179/716], Loss: 0.1688\n",
            "Epoch [2/400], Step [180/716], Loss: 0.1890\n",
            "Epoch [2/400], Step [181/716], Loss: 0.1334\n",
            "Epoch [2/400], Step [182/716], Loss: 0.2258\n",
            "Epoch [2/400], Step [183/716], Loss: 0.2050\n",
            "Epoch [2/400], Step [184/716], Loss: 0.1757\n",
            "Epoch [2/400], Step [185/716], Loss: 0.1952\n",
            "Epoch [2/400], Step [186/716], Loss: 0.1939\n",
            "Epoch [2/400], Step [187/716], Loss: 0.2055\n",
            "Epoch [2/400], Step [188/716], Loss: 0.1749\n",
            "Epoch [2/400], Step [189/716], Loss: 0.2365\n",
            "Epoch [2/400], Step [190/716], Loss: 0.1513\n",
            "Epoch [2/400], Step [191/716], Loss: 0.1424\n",
            "Epoch [2/400], Step [192/716], Loss: 0.1797\n",
            "Epoch [2/400], Step [193/716], Loss: 0.1361\n",
            "Epoch [2/400], Step [194/716], Loss: 0.1766\n",
            "Epoch [2/400], Step [195/716], Loss: 0.2002\n",
            "Epoch [2/400], Step [196/716], Loss: 0.1294\n",
            "Epoch [2/400], Step [197/716], Loss: 0.2030\n",
            "Epoch [2/400], Step [198/716], Loss: 0.1403\n",
            "Epoch [2/400], Step [199/716], Loss: 0.2059\n",
            "Epoch [2/400], Step [200/716], Loss: 0.1662\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_step_200.pth\n",
            "Epoch [2/400], Step [201/716], Loss: 0.2290\n",
            "Epoch [2/400], Step [202/716], Loss: 0.1899\n",
            "Epoch [2/400], Step [203/716], Loss: 0.3054\n",
            "Epoch [2/400], Step [204/716], Loss: 0.2126\n",
            "Epoch [2/400], Step [205/716], Loss: 0.1951\n",
            "Epoch [2/400], Step [206/716], Loss: 0.1765\n",
            "Epoch [2/400], Step [207/716], Loss: 0.2358\n",
            "Epoch [2/400], Step [208/716], Loss: 0.1712\n",
            "Epoch [2/400], Step [209/716], Loss: 0.1808\n",
            "Epoch [2/400], Step [210/716], Loss: 0.1715\n",
            "Epoch [2/400], Step [211/716], Loss: 0.1649\n",
            "Epoch [2/400], Step [212/716], Loss: 0.2743\n",
            "Epoch [2/400], Step [213/716], Loss: 0.1565\n",
            "Epoch [2/400], Step [214/716], Loss: 0.1495\n",
            "Epoch [2/400], Step [215/716], Loss: 0.1931\n",
            "Epoch [2/400], Step [216/716], Loss: 0.1939\n",
            "Epoch [2/400], Step [217/716], Loss: 0.1939\n",
            "Epoch [2/400], Step [218/716], Loss: 0.1385\n",
            "Epoch [2/400], Step [219/716], Loss: 0.2044\n",
            "Epoch [2/400], Step [220/716], Loss: 0.1948\n",
            "Epoch [2/400], Step [221/716], Loss: 0.1969\n",
            "Epoch [2/400], Step [222/716], Loss: 0.1880\n",
            "Epoch [2/400], Step [223/716], Loss: 0.1674\n",
            "Epoch [2/400], Step [224/716], Loss: 0.1983\n",
            "Epoch [2/400], Step [225/716], Loss: 0.1740\n",
            "Epoch [2/400], Step [226/716], Loss: 0.1700\n",
            "Epoch [2/400], Step [227/716], Loss: 0.1517\n",
            "Epoch [2/400], Step [228/716], Loss: 0.1476\n",
            "Epoch [2/400], Step [229/716], Loss: 0.1460\n",
            "Epoch [2/400], Step [230/716], Loss: 0.1808\n",
            "Epoch [2/400], Step [231/716], Loss: 0.3494\n",
            "Epoch [2/400], Step [232/716], Loss: 0.1725\n",
            "Epoch [2/400], Step [233/716], Loss: 0.1672\n",
            "Epoch [2/400], Step [234/716], Loss: 0.2379\n",
            "Epoch [2/400], Step [235/716], Loss: 0.1534\n",
            "Epoch [2/400], Step [236/716], Loss: 0.1578\n",
            "Epoch [2/400], Step [237/716], Loss: 0.2261\n",
            "Epoch [2/400], Step [238/716], Loss: 0.2384\n",
            "Epoch [2/400], Step [239/716], Loss: 0.2178\n",
            "Epoch [2/400], Step [240/716], Loss: 0.1679\n",
            "Epoch [2/400], Step [241/716], Loss: 0.2491\n",
            "Epoch [2/400], Step [242/716], Loss: 0.1738\n",
            "Epoch [2/400], Step [243/716], Loss: 0.1527\n",
            "Epoch [2/400], Step [244/716], Loss: 0.1859\n",
            "Epoch [2/400], Step [245/716], Loss: 0.1692\n",
            "Epoch [2/400], Step [246/716], Loss: 0.2231\n",
            "Epoch [2/400], Step [247/716], Loss: 0.1819\n",
            "Epoch [2/400], Step [248/716], Loss: 0.2105\n",
            "Epoch [2/400], Step [249/716], Loss: 0.1737\n",
            "Epoch [2/400], Step [250/716], Loss: 0.2460\n",
            "Epoch [2/400], Step [251/716], Loss: 0.1910\n",
            "Epoch [2/400], Step [252/716], Loss: 0.2165\n",
            "Epoch [2/400], Step [253/716], Loss: 0.1749\n",
            "Epoch [2/400], Step [254/716], Loss: 0.1494\n",
            "Epoch [2/400], Step [255/716], Loss: 0.2615\n",
            "Epoch [2/400], Step [256/716], Loss: 0.1564\n",
            "Epoch [2/400], Step [257/716], Loss: 0.1292\n",
            "Epoch [2/400], Step [258/716], Loss: 0.1955\n",
            "Epoch [2/400], Step [259/716], Loss: 0.1714\n",
            "Epoch [2/400], Step [260/716], Loss: 0.2495\n",
            "Epoch [2/400], Step [261/716], Loss: 0.1667\n",
            "Epoch [2/400], Step [262/716], Loss: 0.1677\n",
            "Epoch [2/400], Step [263/716], Loss: 0.2021\n",
            "Epoch [2/400], Step [264/716], Loss: 0.1960\n",
            "Epoch [2/400], Step [265/716], Loss: 0.1505\n",
            "Epoch [2/400], Step [266/716], Loss: 0.1848\n",
            "Epoch [2/400], Step [267/716], Loss: 0.1738\n",
            "Epoch [2/400], Step [268/716], Loss: 0.1740\n",
            "Epoch [2/400], Step [269/716], Loss: 0.1386\n",
            "Epoch [2/400], Step [270/716], Loss: 0.1615\n",
            "Epoch [2/400], Step [271/716], Loss: 0.1746\n",
            "Epoch [2/400], Step [272/716], Loss: 0.1845\n",
            "Epoch [2/400], Step [273/716], Loss: 0.1949\n",
            "Epoch [2/400], Step [274/716], Loss: 0.1918\n",
            "Epoch [2/400], Step [275/716], Loss: 0.1728\n",
            "Epoch [2/400], Step [276/716], Loss: 0.1831\n",
            "Epoch [2/400], Step [277/716], Loss: 0.1533\n",
            "Epoch [2/400], Step [278/716], Loss: 0.1665\n",
            "Epoch [2/400], Step [279/716], Loss: 0.1922\n",
            "Epoch [2/400], Step [280/716], Loss: 0.3160\n",
            "Epoch [2/400], Step [281/716], Loss: 0.2186\n",
            "Epoch [2/400], Step [282/716], Loss: 0.1253\n",
            "Epoch [2/400], Step [283/716], Loss: 0.1448\n",
            "Epoch [2/400], Step [284/716], Loss: 0.1616\n",
            "Epoch [2/400], Step [285/716], Loss: 0.1434\n",
            "Epoch [2/400], Step [286/716], Loss: 0.1512\n",
            "Epoch [2/400], Step [287/716], Loss: 0.1837\n",
            "Epoch [2/400], Step [288/716], Loss: 0.1521\n",
            "Epoch [2/400], Step [289/716], Loss: 0.2103\n",
            "Epoch [2/400], Step [290/716], Loss: 0.1951\n",
            "Epoch [2/400], Step [291/716], Loss: 0.2181\n",
            "Epoch [2/400], Step [292/716], Loss: 0.1670\n",
            "Epoch [2/400], Step [293/716], Loss: 0.1331\n",
            "Epoch [2/400], Step [294/716], Loss: 0.1457\n",
            "Epoch [2/400], Step [295/716], Loss: 0.2328\n",
            "Epoch [2/400], Step [296/716], Loss: 0.1879\n",
            "Epoch [2/400], Step [297/716], Loss: 0.1974\n",
            "Epoch [2/400], Step [298/716], Loss: 0.1528\n",
            "Epoch [2/400], Step [299/716], Loss: 0.1426\n",
            "Epoch [2/400], Step [300/716], Loss: 0.1875\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_step_300.pth\n",
            "Epoch [2/400], Step [301/716], Loss: 0.1716\n",
            "Epoch [2/400], Step [302/716], Loss: 0.2338\n",
            "Epoch [2/400], Step [303/716], Loss: 0.2543\n",
            "Epoch [2/400], Step [304/716], Loss: 0.2387\n",
            "Epoch [2/400], Step [305/716], Loss: 0.1752\n",
            "Epoch [2/400], Step [306/716], Loss: 0.1766\n",
            "Epoch [2/400], Step [307/716], Loss: 0.1435\n",
            "Epoch [2/400], Step [308/716], Loss: 0.1473\n",
            "Epoch [2/400], Step [309/716], Loss: 0.1375\n",
            "Epoch [2/400], Step [310/716], Loss: 0.1594\n",
            "Epoch [2/400], Step [311/716], Loss: 0.1429\n",
            "Epoch [2/400], Step [312/716], Loss: 0.1771\n",
            "Epoch [2/400], Step [313/716], Loss: 0.1449\n",
            "Epoch [2/400], Step [314/716], Loss: 0.3071\n",
            "Epoch [2/400], Step [315/716], Loss: 0.1655\n",
            "Epoch [2/400], Step [316/716], Loss: 0.1445\n",
            "Epoch [2/400], Step [317/716], Loss: 0.1777\n",
            "Epoch [2/400], Step [318/716], Loss: 0.1740\n",
            "Epoch [2/400], Step [319/716], Loss: 0.1954\n",
            "Epoch [2/400], Step [320/716], Loss: 0.1321\n",
            "Epoch [2/400], Step [321/716], Loss: 0.1944\n",
            "Epoch [2/400], Step [322/716], Loss: 0.1412\n",
            "Epoch [2/400], Step [323/716], Loss: 0.2116\n",
            "Epoch [2/400], Step [324/716], Loss: 0.2226\n",
            "Epoch [2/400], Step [325/716], Loss: 0.1407\n",
            "Epoch [2/400], Step [326/716], Loss: 0.1825\n",
            "Epoch [2/400], Step [327/716], Loss: 0.1616\n",
            "Epoch [2/400], Step [328/716], Loss: 0.1929\n",
            "Epoch [2/400], Step [329/716], Loss: 0.3042\n",
            "Epoch [2/400], Step [330/716], Loss: 0.2084\n",
            "Epoch [2/400], Step [331/716], Loss: 0.2280\n",
            "Epoch [2/400], Step [332/716], Loss: 0.2508\n",
            "Epoch [2/400], Step [333/716], Loss: 0.1754\n",
            "Epoch [2/400], Step [334/716], Loss: 0.1671\n",
            "Epoch [2/400], Step [335/716], Loss: 0.1787\n",
            "Epoch [2/400], Step [336/716], Loss: 0.1397\n",
            "Epoch [2/400], Step [337/716], Loss: 0.2796\n",
            "Epoch [2/400], Step [338/716], Loss: 0.2020\n",
            "Epoch [2/400], Step [339/716], Loss: 0.1887\n",
            "Epoch [2/400], Step [340/716], Loss: 0.1993\n",
            "Epoch [2/400], Step [341/716], Loss: 0.1665\n",
            "Epoch [2/400], Step [342/716], Loss: 0.2486\n",
            "Epoch [2/400], Step [343/716], Loss: 0.2084\n",
            "Epoch [2/400], Step [344/716], Loss: 0.2470\n",
            "Epoch [2/400], Step [345/716], Loss: 0.1867\n",
            "Epoch [2/400], Step [346/716], Loss: 0.1829\n",
            "Epoch [2/400], Step [347/716], Loss: 0.1681\n",
            "Epoch [2/400], Step [348/716], Loss: 0.1655\n",
            "Epoch [2/400], Step [349/716], Loss: 0.1792\n",
            "Epoch [2/400], Step [350/716], Loss: 0.1705\n",
            "Epoch [2/400], Step [351/716], Loss: 0.2532\n",
            "Epoch [2/400], Step [352/716], Loss: 0.1396\n",
            "Epoch [2/400], Step [353/716], Loss: 0.1589\n",
            "Epoch [2/400], Step [354/716], Loss: 0.1443\n",
            "Epoch [2/400], Step [355/716], Loss: 0.1594\n",
            "Epoch [2/400], Step [356/716], Loss: 0.1344\n",
            "Epoch [2/400], Step [357/716], Loss: 0.2693\n",
            "Epoch [2/400], Step [358/716], Loss: 0.2126\n",
            "Epoch [2/400], Step [359/716], Loss: 0.1423\n",
            "Epoch [2/400], Step [360/716], Loss: 0.1598\n",
            "Epoch [2/400], Step [361/716], Loss: 0.1348\n",
            "Epoch [2/400], Step [362/716], Loss: 0.1511\n",
            "Epoch [2/400], Step [363/716], Loss: 0.1489\n",
            "Epoch [2/400], Step [364/716], Loss: 0.1757\n",
            "Epoch [2/400], Step [365/716], Loss: 0.3246\n",
            "Epoch [2/400], Step [366/716], Loss: 0.1560\n",
            "Epoch [2/400], Step [367/716], Loss: 0.1480\n",
            "Epoch [2/400], Step [368/716], Loss: 0.1442\n",
            "Epoch [2/400], Step [369/716], Loss: 0.1641\n",
            "Epoch [2/400], Step [370/716], Loss: 0.1703\n",
            "Epoch [2/400], Step [371/716], Loss: 0.1639\n",
            "Epoch [2/400], Step [372/716], Loss: 0.2298\n",
            "Epoch [2/400], Step [373/716], Loss: 0.1255\n",
            "Epoch [2/400], Step [374/716], Loss: 0.1680\n",
            "Epoch [2/400], Step [375/716], Loss: 0.2006\n",
            "Epoch [2/400], Step [376/716], Loss: 0.2076\n",
            "Epoch [2/400], Step [377/716], Loss: 0.1884\n",
            "Epoch [2/400], Step [378/716], Loss: 0.1840\n",
            "Epoch [2/400], Step [379/716], Loss: 0.2182\n",
            "Epoch [2/400], Step [380/716], Loss: 0.2068\n",
            "Epoch [2/400], Step [381/716], Loss: 0.1723\n",
            "Epoch [2/400], Step [382/716], Loss: 0.2105\n",
            "Epoch [2/400], Step [383/716], Loss: 0.2083\n",
            "Epoch [2/400], Step [384/716], Loss: 0.2310\n",
            "Epoch [2/400], Step [385/716], Loss: 0.1818\n",
            "Epoch [2/400], Step [386/716], Loss: 0.2196\n",
            "Epoch [2/400], Step [387/716], Loss: 0.1916\n",
            "Epoch [2/400], Step [388/716], Loss: 0.1885\n",
            "Epoch [2/400], Step [389/716], Loss: 0.1865\n",
            "Epoch [2/400], Step [390/716], Loss: 0.2039\n",
            "Epoch [2/400], Step [391/716], Loss: 0.2885\n",
            "Epoch [2/400], Step [392/716], Loss: 0.1854\n",
            "Epoch [2/400], Step [393/716], Loss: 0.1402\n",
            "Epoch [2/400], Step [394/716], Loss: 0.1397\n",
            "Epoch [2/400], Step [395/716], Loss: 0.1571\n",
            "Epoch [2/400], Step [396/716], Loss: 0.1985\n",
            "Epoch [2/400], Step [397/716], Loss: 0.2079\n",
            "Epoch [2/400], Step [398/716], Loss: 0.1328\n",
            "Epoch [2/400], Step [399/716], Loss: 0.2003\n",
            "Epoch [2/400], Step [400/716], Loss: 0.1752\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_step_400.pth\n",
            "Epoch [2/400], Step [401/716], Loss: 0.1374\n",
            "Epoch [2/400], Step [402/716], Loss: 0.2565\n",
            "Epoch [2/400], Step [403/716], Loss: 0.1605\n",
            "Epoch [2/400], Step [404/716], Loss: 0.1158\n",
            "Epoch [2/400], Step [405/716], Loss: 0.1307\n",
            "Epoch [2/400], Step [406/716], Loss: 0.1883\n",
            "Epoch [2/400], Step [407/716], Loss: 0.1889\n",
            "Epoch [2/400], Step [408/716], Loss: 0.1761\n",
            "Epoch [2/400], Step [409/716], Loss: 0.1844\n",
            "Epoch [2/400], Step [410/716], Loss: 0.1446\n",
            "Epoch [2/400], Step [411/716], Loss: 0.1805\n",
            "Epoch [2/400], Step [412/716], Loss: 0.1427\n",
            "Epoch [2/400], Step [413/716], Loss: 0.2120\n",
            "Epoch [2/400], Step [414/716], Loss: 0.1859\n",
            "Epoch [2/400], Step [415/716], Loss: 0.1596\n",
            "Epoch [2/400], Step [416/716], Loss: 0.1766\n",
            "Epoch [2/400], Step [417/716], Loss: 0.1507\n",
            "Epoch [2/400], Step [418/716], Loss: 0.1576\n",
            "Epoch [2/400], Step [419/716], Loss: 0.1229\n",
            "Epoch [2/400], Step [420/716], Loss: 0.1674\n",
            "Epoch [2/400], Step [421/716], Loss: 0.1428\n",
            "Epoch [2/400], Step [422/716], Loss: 0.1768\n",
            "Epoch [2/400], Step [423/716], Loss: 0.2082\n",
            "Epoch [2/400], Step [424/716], Loss: 0.2152\n",
            "Epoch [2/400], Step [425/716], Loss: 0.2388\n",
            "Epoch [2/400], Step [426/716], Loss: 0.2271\n",
            "Epoch [2/400], Step [427/716], Loss: 0.2115\n",
            "Epoch [2/400], Step [428/716], Loss: 0.2111\n",
            "Epoch [2/400], Step [429/716], Loss: 0.2696\n",
            "Epoch [2/400], Step [430/716], Loss: 0.1672\n",
            "Epoch [2/400], Step [431/716], Loss: 0.1587\n",
            "Epoch [2/400], Step [432/716], Loss: 0.1341\n",
            "Epoch [2/400], Step [433/716], Loss: 0.1223\n",
            "Epoch [2/400], Step [434/716], Loss: 0.1227\n",
            "Epoch [2/400], Step [435/716], Loss: 0.2462\n",
            "Epoch [2/400], Step [436/716], Loss: 0.1690\n",
            "Epoch [2/400], Step [437/716], Loss: 0.1988\n",
            "Epoch [2/400], Step [438/716], Loss: 0.1670\n",
            "Epoch [2/400], Step [439/716], Loss: 0.1334\n",
            "Epoch [2/400], Step [440/716], Loss: 0.1289\n",
            "Epoch [2/400], Step [441/716], Loss: 0.3260\n",
            "Epoch [2/400], Step [442/716], Loss: 0.1247\n",
            "Epoch [2/400], Step [443/716], Loss: 0.1694\n",
            "Epoch [2/400], Step [444/716], Loss: 0.1419\n",
            "Epoch [2/400], Step [445/716], Loss: 0.1258\n",
            "Epoch [2/400], Step [446/716], Loss: 0.2514\n",
            "Epoch [2/400], Step [447/716], Loss: 0.1556\n",
            "Epoch [2/400], Step [448/716], Loss: 0.1358\n",
            "Epoch [2/400], Step [449/716], Loss: 0.1300\n",
            "Epoch [2/400], Step [450/716], Loss: 0.1585\n",
            "Epoch [2/400], Step [451/716], Loss: 0.1225\n",
            "Epoch [2/400], Step [452/716], Loss: 0.2075\n",
            "Epoch [2/400], Step [453/716], Loss: 0.1584\n",
            "Epoch [2/400], Step [454/716], Loss: 0.1693\n",
            "Epoch [2/400], Step [455/716], Loss: 0.2271\n",
            "Epoch [2/400], Step [456/716], Loss: 0.1758\n",
            "Epoch [2/400], Step [457/716], Loss: 0.2002\n",
            "Epoch [2/400], Step [458/716], Loss: 0.1553\n",
            "Epoch [2/400], Step [459/716], Loss: 0.1502\n",
            "Epoch [2/400], Step [460/716], Loss: 0.1228\n",
            "Epoch [2/400], Step [461/716], Loss: 0.2120\n",
            "Epoch [2/400], Step [462/716], Loss: 0.1320\n",
            "Epoch [2/400], Step [463/716], Loss: 0.1533\n",
            "Epoch [2/400], Step [464/716], Loss: 0.1396\n",
            "Epoch [2/400], Step [465/716], Loss: 0.1214\n",
            "Epoch [2/400], Step [466/716], Loss: 0.1764\n",
            "Epoch [2/400], Step [467/716], Loss: 0.2350\n",
            "Epoch [2/400], Step [468/716], Loss: 0.2125\n",
            "Epoch [2/400], Step [469/716], Loss: 0.1701\n",
            "Epoch [2/400], Step [470/716], Loss: 0.1031\n",
            "Epoch [2/400], Step [471/716], Loss: 0.2157\n",
            "Epoch [2/400], Step [472/716], Loss: 0.1376\n",
            "Epoch [2/400], Step [473/716], Loss: 0.1665\n",
            "Epoch [2/400], Step [474/716], Loss: 0.1404\n",
            "Epoch [2/400], Step [475/716], Loss: 0.1897\n",
            "Epoch [2/400], Step [476/716], Loss: 0.1501\n",
            "Epoch [2/400], Step [477/716], Loss: 0.1428\n",
            "Epoch [2/400], Step [478/716], Loss: 0.1362\n",
            "Epoch [2/400], Step [479/716], Loss: 0.1882\n",
            "Epoch [2/400], Step [480/716], Loss: 0.1421\n",
            "Epoch [2/400], Step [481/716], Loss: 0.1146\n",
            "Epoch [2/400], Step [482/716], Loss: 0.1508\n",
            "Epoch [2/400], Step [483/716], Loss: 0.1339\n",
            "Epoch [2/400], Step [484/716], Loss: 0.1273\n",
            "Epoch [2/400], Step [485/716], Loss: 0.1249\n",
            "Epoch [2/400], Step [486/716], Loss: 0.1680\n",
            "Epoch [2/400], Step [487/716], Loss: 0.1447\n",
            "Epoch [2/400], Step [488/716], Loss: 0.1181\n",
            "Epoch [2/400], Step [489/716], Loss: 0.1736\n",
            "Epoch [2/400], Step [490/716], Loss: 0.1480\n",
            "Epoch [2/400], Step [491/716], Loss: 0.1370\n",
            "Epoch [2/400], Step [492/716], Loss: 0.1499\n",
            "Epoch [2/400], Step [493/716], Loss: 0.1829\n",
            "Epoch [2/400], Step [494/716], Loss: 0.2359\n",
            "Epoch [2/400], Step [495/716], Loss: 0.1366\n",
            "Epoch [2/400], Step [496/716], Loss: 0.1401\n",
            "Epoch [2/400], Step [497/716], Loss: 0.1461\n",
            "Epoch [2/400], Step [498/716], Loss: 0.1499\n",
            "Epoch [2/400], Step [499/716], Loss: 0.1331\n",
            "Epoch [2/400], Step [500/716], Loss: 0.1516\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_step_500.pth\n",
            "Epoch [2/400], Step [501/716], Loss: 0.1974\n",
            "Epoch [2/400], Step [502/716], Loss: 0.2291\n",
            "Epoch [2/400], Step [503/716], Loss: 0.1580\n",
            "Epoch [2/400], Step [504/716], Loss: 0.1400\n",
            "Epoch [2/400], Step [505/716], Loss: 0.1675\n",
            "Epoch [2/400], Step [506/716], Loss: 0.1488\n",
            "Epoch [2/400], Step [507/716], Loss: 0.1496\n",
            "Epoch [2/400], Step [508/716], Loss: 0.1594\n",
            "Epoch [2/400], Step [509/716], Loss: 0.1467\n",
            "Epoch [2/400], Step [510/716], Loss: 0.1174\n",
            "Epoch [2/400], Step [511/716], Loss: 0.1607\n",
            "Epoch [2/400], Step [512/716], Loss: 0.2377\n",
            "Epoch [2/400], Step [513/716], Loss: 0.1598\n",
            "Epoch [2/400], Step [514/716], Loss: 0.1987\n",
            "Epoch [2/400], Step [515/716], Loss: 0.1610\n",
            "Epoch [2/400], Step [516/716], Loss: 0.2271\n",
            "Epoch [2/400], Step [517/716], Loss: 0.1432\n",
            "Epoch [2/400], Step [518/716], Loss: 0.1436\n",
            "Epoch [2/400], Step [519/716], Loss: 0.1520\n",
            "Epoch [2/400], Step [520/716], Loss: 0.1227\n",
            "Epoch [2/400], Step [521/716], Loss: 0.1445\n",
            "Epoch [2/400], Step [522/716], Loss: 0.1653\n",
            "Epoch [2/400], Step [523/716], Loss: 0.1833\n",
            "Epoch [2/400], Step [524/716], Loss: 0.1644\n",
            "Epoch [2/400], Step [525/716], Loss: 0.1595\n",
            "Epoch [2/400], Step [526/716], Loss: 0.1544\n",
            "Epoch [2/400], Step [527/716], Loss: 0.1306\n",
            "Epoch [2/400], Step [528/716], Loss: 0.1006\n",
            "Epoch [2/400], Step [529/716], Loss: 0.1503\n",
            "Epoch [2/400], Step [530/716], Loss: 0.1453\n",
            "Epoch [2/400], Step [531/716], Loss: 0.1789\n",
            "Epoch [2/400], Step [532/716], Loss: 0.1678\n",
            "Epoch [2/400], Step [533/716], Loss: 0.1170\n",
            "Epoch [2/400], Step [534/716], Loss: 0.1506\n",
            "Epoch [2/400], Step [535/716], Loss: 0.1878\n",
            "Epoch [2/400], Step [536/716], Loss: 0.1572\n",
            "Epoch [2/400], Step [537/716], Loss: 0.1623\n",
            "Epoch [2/400], Step [538/716], Loss: 0.1398\n",
            "Epoch [2/400], Step [539/716], Loss: 0.1632\n",
            "Epoch [2/400], Step [540/716], Loss: 0.2136\n",
            "Epoch [2/400], Step [541/716], Loss: 0.1370\n",
            "Epoch [2/400], Step [542/716], Loss: 0.1869\n",
            "Epoch [2/400], Step [543/716], Loss: 0.2050\n",
            "Epoch [2/400], Step [544/716], Loss: 0.1710\n",
            "Epoch [2/400], Step [545/716], Loss: 0.2487\n",
            "Epoch [2/400], Step [546/716], Loss: 0.1981\n",
            "Epoch [2/400], Step [547/716], Loss: 0.1305\n",
            "Epoch [2/400], Step [548/716], Loss: 0.1439\n",
            "Epoch [2/400], Step [549/716], Loss: 0.2200\n",
            "Epoch [2/400], Step [550/716], Loss: 0.3000\n",
            "Epoch [2/400], Step [551/716], Loss: 0.2093\n",
            "Epoch [2/400], Step [552/716], Loss: 0.2001\n",
            "Epoch [2/400], Step [553/716], Loss: 0.2503\n",
            "Epoch [2/400], Step [554/716], Loss: 0.1616\n",
            "Epoch [2/400], Step [555/716], Loss: 0.1509\n",
            "Epoch [2/400], Step [556/716], Loss: 0.1793\n",
            "Epoch [2/400], Step [557/716], Loss: 0.1307\n",
            "Epoch [2/400], Step [558/716], Loss: 0.1840\n",
            "Epoch [2/400], Step [559/716], Loss: 0.1581\n",
            "Epoch [2/400], Step [560/716], Loss: 0.2074\n",
            "Epoch [2/400], Step [561/716], Loss: 0.1450\n",
            "Epoch [2/400], Step [562/716], Loss: 0.1400\n",
            "Epoch [2/400], Step [563/716], Loss: 0.1449\n",
            "Epoch [2/400], Step [564/716], Loss: 0.1555\n",
            "Epoch [2/400], Step [565/716], Loss: 0.1185\n",
            "Epoch [2/400], Step [566/716], Loss: 0.1144\n",
            "Epoch [2/400], Step [567/716], Loss: 0.2099\n",
            "Epoch [2/400], Step [568/716], Loss: 0.1863\n",
            "Epoch [2/400], Step [569/716], Loss: 0.1003\n",
            "Epoch [2/400], Step [570/716], Loss: 0.1670\n",
            "Epoch [2/400], Step [571/716], Loss: 0.1966\n",
            "Epoch [2/400], Step [572/716], Loss: 0.1549\n",
            "Epoch [2/400], Step [573/716], Loss: 0.1474\n",
            "Epoch [2/400], Step [574/716], Loss: 0.1544\n",
            "Epoch [2/400], Step [575/716], Loss: 0.1144\n",
            "Epoch [2/400], Step [576/716], Loss: 0.1358\n",
            "Epoch [2/400], Step [577/716], Loss: 0.2164\n",
            "Epoch [2/400], Step [578/716], Loss: 0.1345\n",
            "Epoch [2/400], Step [579/716], Loss: 0.1781\n",
            "Epoch [2/400], Step [580/716], Loss: 0.1797\n",
            "Epoch [2/400], Step [581/716], Loss: 0.1332\n",
            "Epoch [2/400], Step [582/716], Loss: 0.1180\n",
            "Epoch [2/400], Step [583/716], Loss: 0.1501\n",
            "Epoch [2/400], Step [584/716], Loss: 0.1521\n",
            "Epoch [2/400], Step [585/716], Loss: 0.1689\n",
            "Epoch [2/400], Step [586/716], Loss: 0.1921\n",
            "Epoch [2/400], Step [587/716], Loss: 0.2205\n",
            "Epoch [2/400], Step [588/716], Loss: 0.2513\n",
            "Epoch [2/400], Step [589/716], Loss: 0.1161\n",
            "Epoch [2/400], Step [590/716], Loss: 0.2351\n",
            "Epoch [2/400], Step [591/716], Loss: 0.1584\n",
            "Epoch [2/400], Step [592/716], Loss: 0.2247\n",
            "Epoch [2/400], Step [593/716], Loss: 0.1545\n",
            "Epoch [2/400], Step [594/716], Loss: 0.1717\n",
            "Epoch [2/400], Step [595/716], Loss: 0.1662\n",
            "Epoch [2/400], Step [596/716], Loss: 0.1547\n",
            "Epoch [2/400], Step [597/716], Loss: 0.1721\n",
            "Epoch [2/400], Step [598/716], Loss: 0.1785\n",
            "Epoch [2/400], Step [599/716], Loss: 0.1933\n",
            "Epoch [2/400], Step [600/716], Loss: 0.1613\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_step_600.pth\n",
            "Epoch [2/400], Step [601/716], Loss: 0.1624\n",
            "Epoch [2/400], Step [602/716], Loss: 0.1884\n",
            "Epoch [2/400], Step [603/716], Loss: 0.1415\n",
            "Epoch [2/400], Step [604/716], Loss: 0.1524\n",
            "Epoch [2/400], Step [605/716], Loss: 0.1778\n",
            "Epoch [2/400], Step [606/716], Loss: 0.1418\n",
            "Epoch [2/400], Step [607/716], Loss: 0.2001\n",
            "Epoch [2/400], Step [608/716], Loss: 0.1608\n",
            "Epoch [2/400], Step [609/716], Loss: 0.1635\n",
            "Epoch [2/400], Step [610/716], Loss: 0.2202\n",
            "Epoch [2/400], Step [611/716], Loss: 0.1928\n",
            "Epoch [2/400], Step [612/716], Loss: 0.1707\n",
            "Epoch [2/400], Step [613/716], Loss: 0.1040\n",
            "Epoch [2/400], Step [614/716], Loss: 0.1998\n",
            "Epoch [2/400], Step [615/716], Loss: 0.2143\n",
            "Epoch [2/400], Step [616/716], Loss: 0.1646\n",
            "Epoch [2/400], Step [617/716], Loss: 0.2062\n",
            "Epoch [2/400], Step [618/716], Loss: 0.1551\n",
            "Epoch [2/400], Step [619/716], Loss: 0.2632\n",
            "Epoch [2/400], Step [620/716], Loss: 0.1881\n",
            "Epoch [2/400], Step [621/716], Loss: 0.1451\n",
            "Epoch [2/400], Step [622/716], Loss: 0.3206\n",
            "Epoch [2/400], Step [623/716], Loss: 0.1134\n",
            "Epoch [2/400], Step [624/716], Loss: 0.1572\n",
            "Epoch [2/400], Step [625/716], Loss: 0.1582\n",
            "Epoch [2/400], Step [626/716], Loss: 0.1677\n",
            "Epoch [2/400], Step [627/716], Loss: 0.2094\n",
            "Epoch [2/400], Step [628/716], Loss: 0.1730\n",
            "Epoch [2/400], Step [629/716], Loss: 0.1906\n",
            "Epoch [2/400], Step [630/716], Loss: 0.1613\n",
            "Epoch [2/400], Step [631/716], Loss: 0.1292\n",
            "Epoch [2/400], Step [632/716], Loss: 0.1256\n",
            "Epoch [2/400], Step [633/716], Loss: 0.2942\n",
            "Epoch [2/400], Step [634/716], Loss: 0.2077\n",
            "Epoch [2/400], Step [635/716], Loss: 0.1829\n",
            "Epoch [2/400], Step [636/716], Loss: 0.1687\n",
            "Epoch [2/400], Step [637/716], Loss: 0.1751\n",
            "Epoch [2/400], Step [638/716], Loss: 0.1407\n",
            "Epoch [2/400], Step [639/716], Loss: 0.1491\n",
            "Epoch [2/400], Step [640/716], Loss: 0.1282\n",
            "Epoch [2/400], Step [641/716], Loss: 0.2187\n",
            "Epoch [2/400], Step [642/716], Loss: 0.1646\n",
            "Epoch [2/400], Step [643/716], Loss: 0.1373\n",
            "Epoch [2/400], Step [644/716], Loss: 0.1764\n",
            "Epoch [2/400], Step [645/716], Loss: 0.1543\n",
            "Epoch [2/400], Step [646/716], Loss: 0.1251\n",
            "Epoch [2/400], Step [647/716], Loss: 0.1730\n",
            "Epoch [2/400], Step [648/716], Loss: 0.1623\n",
            "Epoch [2/400], Step [649/716], Loss: 0.0979\n",
            "Epoch [2/400], Step [650/716], Loss: 0.1817\n",
            "Epoch [2/400], Step [651/716], Loss: 0.1471\n",
            "Epoch [2/400], Step [652/716], Loss: 0.1414\n",
            "Epoch [2/400], Step [653/716], Loss: 0.1786\n",
            "Epoch [2/400], Step [654/716], Loss: 0.1434\n",
            "Epoch [2/400], Step [655/716], Loss: 0.1478\n",
            "Epoch [2/400], Step [656/716], Loss: 0.1306\n",
            "Epoch [2/400], Step [657/716], Loss: 0.2010\n",
            "Epoch [2/400], Step [658/716], Loss: 0.1486\n",
            "Epoch [2/400], Step [659/716], Loss: 0.1592\n",
            "Epoch [2/400], Step [660/716], Loss: 0.1943\n",
            "Epoch [2/400], Step [661/716], Loss: 0.1127\n",
            "Epoch [2/400], Step [662/716], Loss: 0.1361\n",
            "Epoch [2/400], Step [663/716], Loss: 0.1358\n",
            "Epoch [2/400], Step [664/716], Loss: 0.1371\n",
            "Epoch [2/400], Step [665/716], Loss: 0.1373\n",
            "Epoch [2/400], Step [666/716], Loss: 0.1811\n",
            "Epoch [2/400], Step [667/716], Loss: 0.1570\n",
            "Epoch [2/400], Step [668/716], Loss: 0.1223\n",
            "Epoch [2/400], Step [669/716], Loss: 0.2030\n",
            "Epoch [2/400], Step [670/716], Loss: 0.1382\n",
            "Epoch [2/400], Step [671/716], Loss: 0.1699\n",
            "Epoch [2/400], Step [672/716], Loss: 0.1421\n",
            "Epoch [2/400], Step [673/716], Loss: 0.1314\n",
            "Epoch [2/400], Step [674/716], Loss: 0.1496\n",
            "Epoch [2/400], Step [675/716], Loss: 0.1252\n",
            "Epoch [2/400], Step [676/716], Loss: 0.1118\n",
            "Epoch [2/400], Step [677/716], Loss: 0.0980\n",
            "Epoch [2/400], Step [678/716], Loss: 0.1575\n",
            "Epoch [2/400], Step [679/716], Loss: 0.1583\n",
            "Epoch [2/400], Step [680/716], Loss: 0.1242\n",
            "Epoch [2/400], Step [681/716], Loss: 0.1626\n",
            "Epoch [2/400], Step [682/716], Loss: 0.1600\n",
            "Epoch [2/400], Step [683/716], Loss: 0.1562\n",
            "Epoch [2/400], Step [684/716], Loss: 0.1421\n",
            "Epoch [2/400], Step [685/716], Loss: 0.1155\n",
            "Epoch [2/400], Step [686/716], Loss: 0.1055\n",
            "Epoch [2/400], Step [687/716], Loss: 0.1640\n",
            "Epoch [2/400], Step [688/716], Loss: 0.1488\n",
            "Epoch [2/400], Step [689/716], Loss: 0.1947\n",
            "Epoch [2/400], Step [690/716], Loss: 0.1999\n",
            "Epoch [2/400], Step [691/716], Loss: 0.1219\n",
            "Epoch [2/400], Step [692/716], Loss: 0.1199\n",
            "Epoch [2/400], Step [693/716], Loss: 0.1569\n",
            "Epoch [2/400], Step [694/716], Loss: 0.1662\n",
            "Epoch [2/400], Step [695/716], Loss: 0.1084\n",
            "Epoch [2/400], Step [696/716], Loss: 0.1513\n",
            "Epoch [2/400], Step [697/716], Loss: 0.1627\n",
            "Epoch [2/400], Step [698/716], Loss: 0.1289\n",
            "Epoch [2/400], Step [699/716], Loss: 0.2437\n",
            "Epoch [2/400], Step [700/716], Loss: 0.1338\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_step_700.pth\n",
            "Epoch [2/400], Step [701/716], Loss: 0.1306\n",
            "Epoch [2/400], Step [702/716], Loss: 0.1737\n",
            "Epoch [2/400], Step [703/716], Loss: 0.1579\n",
            "Epoch [2/400], Step [704/716], Loss: 0.1931\n",
            "Epoch [2/400], Step [705/716], Loss: 0.1444\n",
            "Epoch [2/400], Step [706/716], Loss: 0.1805\n",
            "Epoch [2/400], Step [707/716], Loss: 0.1251\n",
            "Epoch [2/400], Step [708/716], Loss: 0.2096\n",
            "Epoch [2/400], Step [709/716], Loss: 0.1604\n",
            "Epoch [2/400], Step [710/716], Loss: 0.1529\n",
            "Epoch [2/400], Step [711/716], Loss: 0.1818\n",
            "Epoch [2/400], Step [712/716], Loss: 0.2477\n",
            "Epoch [2/400], Step [713/716], Loss: 0.1713\n",
            "Epoch [2/400], Step [714/716], Loss: 0.1869\n",
            "Epoch [2/400], Step [715/716], Loss: 0.2154\n",
            "Epoch [2/400], Step [716/716], Loss: 0.1852\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_2_last.pth\n",
            "Epoch [3/400], Step [1/716], Loss: 0.1396\n",
            "Epoch [3/400], Step [2/716], Loss: 0.1665\n",
            "Epoch [3/400], Step [3/716], Loss: 0.2106\n",
            "Epoch [3/400], Step [4/716], Loss: 0.1853\n",
            "Epoch [3/400], Step [5/716], Loss: 0.1564\n",
            "Epoch [3/400], Step [6/716], Loss: 0.1143\n",
            "Epoch [3/400], Step [7/716], Loss: 0.1656\n",
            "Epoch [3/400], Step [8/716], Loss: 0.2376\n",
            "Epoch [3/400], Step [9/716], Loss: 0.1544\n",
            "Epoch [3/400], Step [10/716], Loss: 0.1453\n",
            "Epoch [3/400], Step [11/716], Loss: 0.2694\n",
            "Epoch [3/400], Step [12/716], Loss: 0.1656\n",
            "Epoch [3/400], Step [13/716], Loss: 0.1383\n",
            "Epoch [3/400], Step [14/716], Loss: 0.1269\n",
            "Epoch [3/400], Step [15/716], Loss: 0.1860\n",
            "Epoch [3/400], Step [16/716], Loss: 0.1196\n",
            "Epoch [3/400], Step [17/716], Loss: 0.1304\n",
            "Epoch [3/400], Step [18/716], Loss: 0.1644\n",
            "Epoch [3/400], Step [19/716], Loss: 0.1448\n",
            "Epoch [3/400], Step [20/716], Loss: 0.1455\n",
            "Epoch [3/400], Step [21/716], Loss: 0.1406\n",
            "Epoch [3/400], Step [22/716], Loss: 0.1333\n",
            "Epoch [3/400], Step [23/716], Loss: 0.1295\n",
            "Epoch [3/400], Step [24/716], Loss: 0.1154\n",
            "Epoch [3/400], Step [25/716], Loss: 0.1040\n",
            "Epoch [3/400], Step [26/716], Loss: 0.1255\n",
            "Epoch [3/400], Step [27/716], Loss: 0.1193\n",
            "Epoch [3/400], Step [28/716], Loss: 0.1516\n",
            "Epoch [3/400], Step [29/716], Loss: 0.2249\n",
            "Epoch [3/400], Step [30/716], Loss: 0.1477\n",
            "Epoch [3/400], Step [31/716], Loss: 0.1844\n",
            "Epoch [3/400], Step [32/716], Loss: 0.1911\n",
            "Epoch [3/400], Step [33/716], Loss: 0.1906\n",
            "Epoch [3/400], Step [34/716], Loss: 0.1313\n",
            "Epoch [3/400], Step [35/716], Loss: 0.1221\n",
            "Epoch [3/400], Step [36/716], Loss: 0.1436\n",
            "Epoch [3/400], Step [37/716], Loss: 0.1179\n",
            "Epoch [3/400], Step [38/716], Loss: 0.1422\n",
            "Epoch [3/400], Step [39/716], Loss: 0.1132\n",
            "Epoch [3/400], Step [40/716], Loss: 0.1583\n",
            "Epoch [3/400], Step [41/716], Loss: 0.1262\n",
            "Epoch [3/400], Step [42/716], Loss: 0.1297\n",
            "Epoch [3/400], Step [43/716], Loss: 0.1364\n",
            "Epoch [3/400], Step [44/716], Loss: 0.1384\n",
            "Epoch [3/400], Step [45/716], Loss: 0.1068\n",
            "Epoch [3/400], Step [46/716], Loss: 0.1495\n",
            "Epoch [3/400], Step [47/716], Loss: 0.1332\n",
            "Epoch [3/400], Step [48/716], Loss: 0.1478\n",
            "Epoch [3/400], Step [49/716], Loss: 0.1595\n",
            "Epoch [3/400], Step [50/716], Loss: 0.1120\n",
            "Epoch [3/400], Step [51/716], Loss: 0.1809\n",
            "Epoch [3/400], Step [52/716], Loss: 0.1045\n",
            "Epoch [3/400], Step [53/716], Loss: 0.1499\n",
            "Epoch [3/400], Step [54/716], Loss: 0.1806\n",
            "Epoch [3/400], Step [55/716], Loss: 0.1619\n",
            "Epoch [3/400], Step [56/716], Loss: 0.2319\n",
            "Epoch [3/400], Step [57/716], Loss: 0.1360\n",
            "Epoch [3/400], Step [58/716], Loss: 0.1313\n",
            "Epoch [3/400], Step [59/716], Loss: 0.1546\n",
            "Epoch [3/400], Step [60/716], Loss: 0.1368\n",
            "Epoch [3/400], Step [61/716], Loss: 0.1241\n",
            "Epoch [3/400], Step [62/716], Loss: 0.1475\n",
            "Epoch [3/400], Step [63/716], Loss: 0.0998\n",
            "Epoch [3/400], Step [64/716], Loss: 0.1767\n",
            "Epoch [3/400], Step [65/716], Loss: 0.1801\n",
            "Epoch [3/400], Step [66/716], Loss: 0.1936\n",
            "Epoch [3/400], Step [67/716], Loss: 0.1336\n",
            "Epoch [3/400], Step [68/716], Loss: 0.1510\n",
            "Epoch [3/400], Step [69/716], Loss: 0.1503\n",
            "Epoch [3/400], Step [70/716], Loss: 0.1377\n",
            "Epoch [3/400], Step [71/716], Loss: 0.1611\n",
            "Epoch [3/400], Step [72/716], Loss: 0.1561\n",
            "Epoch [3/400], Step [73/716], Loss: 0.1680\n",
            "Epoch [3/400], Step [74/716], Loss: 0.1735\n",
            "Epoch [3/400], Step [75/716], Loss: 0.1565\n",
            "Epoch [3/400], Step [76/716], Loss: 0.1487\n",
            "Epoch [3/400], Step [77/716], Loss: 0.1175\n",
            "Epoch [3/400], Step [78/716], Loss: 0.1472\n",
            "Epoch [3/400], Step [79/716], Loss: 0.1422\n",
            "Epoch [3/400], Step [80/716], Loss: 0.1184\n",
            "Epoch [3/400], Step [81/716], Loss: 0.1523\n",
            "Epoch [3/400], Step [82/716], Loss: 0.1456\n",
            "Epoch [3/400], Step [83/716], Loss: 0.1511\n",
            "Epoch [3/400], Step [84/716], Loss: 0.2871\n",
            "Epoch [3/400], Step [85/716], Loss: 0.1353\n",
            "Epoch [3/400], Step [86/716], Loss: 0.2039\n",
            "Epoch [3/400], Step [87/716], Loss: 0.1070\n",
            "Epoch [3/400], Step [88/716], Loss: 0.1974\n",
            "Epoch [3/400], Step [89/716], Loss: 0.1821\n",
            "Epoch [3/400], Step [90/716], Loss: 0.1178\n",
            "Epoch [3/400], Step [91/716], Loss: 0.1788\n",
            "Epoch [3/400], Step [92/716], Loss: 0.1976\n",
            "Epoch [3/400], Step [93/716], Loss: 0.1866\n",
            "Epoch [3/400], Step [94/716], Loss: 0.1552\n",
            "Epoch [3/400], Step [95/716], Loss: 0.1500\n",
            "Epoch [3/400], Step [96/716], Loss: 0.1611\n",
            "Epoch [3/400], Step [97/716], Loss: 0.1228\n",
            "Epoch [3/400], Step [98/716], Loss: 0.2132\n",
            "Epoch [3/400], Step [99/716], Loss: 0.1500\n",
            "Epoch [3/400], Step [100/716], Loss: 0.1442\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_step_100.pth\n",
            "Epoch [3/400], Step [101/716], Loss: 0.1133\n",
            "Epoch [3/400], Step [102/716], Loss: 0.1443\n",
            "Epoch [3/400], Step [103/716], Loss: 0.2178\n",
            "Epoch [3/400], Step [104/716], Loss: 0.1486\n",
            "Epoch [3/400], Step [105/716], Loss: 0.1361\n",
            "Epoch [3/400], Step [106/716], Loss: 0.1237\n",
            "Epoch [3/400], Step [107/716], Loss: 0.1818\n",
            "Epoch [3/400], Step [108/716], Loss: 0.1894\n",
            "Epoch [3/400], Step [109/716], Loss: 0.1501\n",
            "Epoch [3/400], Step [110/716], Loss: 0.1996\n",
            "Epoch [3/400], Step [111/716], Loss: 0.1633\n",
            "Epoch [3/400], Step [112/716], Loss: 0.1301\n",
            "Epoch [3/400], Step [113/716], Loss: 0.1045\n",
            "Epoch [3/400], Step [114/716], Loss: 0.1435\n",
            "Epoch [3/400], Step [115/716], Loss: 0.1370\n",
            "Epoch [3/400], Step [116/716], Loss: 0.1513\n",
            "Epoch [3/400], Step [117/716], Loss: 0.1330\n",
            "Epoch [3/400], Step [118/716], Loss: 0.1220\n",
            "Epoch [3/400], Step [119/716], Loss: 0.1219\n",
            "Epoch [3/400], Step [120/716], Loss: 0.1603\n",
            "Epoch [3/400], Step [121/716], Loss: 0.1866\n",
            "Epoch [3/400], Step [122/716], Loss: 0.1568\n",
            "Epoch [3/400], Step [123/716], Loss: 0.1353\n",
            "Epoch [3/400], Step [124/716], Loss: 0.1390\n",
            "Epoch [3/400], Step [125/716], Loss: 0.1742\n",
            "Epoch [3/400], Step [126/716], Loss: 0.1264\n",
            "Epoch [3/400], Step [127/716], Loss: 0.1589\n",
            "Epoch [3/400], Step [128/716], Loss: 0.2389\n",
            "Epoch [3/400], Step [129/716], Loss: 0.1758\n",
            "Epoch [3/400], Step [130/716], Loss: 0.1739\n",
            "Epoch [3/400], Step [131/716], Loss: 0.1268\n",
            "Epoch [3/400], Step [132/716], Loss: 0.1112\n",
            "Epoch [3/400], Step [133/716], Loss: 0.1141\n",
            "Epoch [3/400], Step [134/716], Loss: 0.1872\n",
            "Epoch [3/400], Step [135/716], Loss: 0.1989\n",
            "Epoch [3/400], Step [136/716], Loss: 0.1560\n",
            "Epoch [3/400], Step [137/716], Loss: 0.1417\n",
            "Epoch [3/400], Step [138/716], Loss: 0.1519\n",
            "Epoch [3/400], Step [139/716], Loss: 0.1475\n",
            "Epoch [3/400], Step [140/716], Loss: 0.1855\n",
            "Epoch [3/400], Step [141/716], Loss: 0.1428\n",
            "Epoch [3/400], Step [142/716], Loss: 0.1626\n",
            "Epoch [3/400], Step [143/716], Loss: 0.1667\n",
            "Epoch [3/400], Step [144/716], Loss: 0.1666\n",
            "Epoch [3/400], Step [145/716], Loss: 0.1540\n",
            "Epoch [3/400], Step [146/716], Loss: 0.1748\n",
            "Epoch [3/400], Step [147/716], Loss: 0.2176\n",
            "Epoch [3/400], Step [148/716], Loss: 0.1506\n",
            "Epoch [3/400], Step [149/716], Loss: 0.1488\n",
            "Epoch [3/400], Step [150/716], Loss: 0.1808\n",
            "Epoch [3/400], Step [151/716], Loss: 0.1354\n",
            "Epoch [3/400], Step [152/716], Loss: 0.1848\n",
            "Epoch [3/400], Step [153/716], Loss: 0.1628\n",
            "Epoch [3/400], Step [154/716], Loss: 0.1437\n",
            "Epoch [3/400], Step [155/716], Loss: 0.1792\n",
            "Epoch [3/400], Step [156/716], Loss: 0.1651\n",
            "Epoch [3/400], Step [157/716], Loss: 0.2689\n",
            "Epoch [3/400], Step [158/716], Loss: 0.1578\n",
            "Epoch [3/400], Step [159/716], Loss: 0.2079\n",
            "Epoch [3/400], Step [160/716], Loss: 0.1457\n",
            "Epoch [3/400], Step [161/716], Loss: 0.1412\n",
            "Epoch [3/400], Step [162/716], Loss: 0.1621\n",
            "Epoch [3/400], Step [163/716], Loss: 0.1970\n",
            "Epoch [3/400], Step [164/716], Loss: 0.1301\n",
            "Epoch [3/400], Step [165/716], Loss: 0.1931\n",
            "Epoch [3/400], Step [166/716], Loss: 0.1372\n",
            "Epoch [3/400], Step [167/716], Loss: 0.1757\n",
            "Epoch [3/400], Step [168/716], Loss: 0.1401\n",
            "Epoch [3/400], Step [169/716], Loss: 0.1978\n",
            "Epoch [3/400], Step [170/716], Loss: 0.1448\n",
            "Epoch [3/400], Step [171/716], Loss: 0.1885\n",
            "Epoch [3/400], Step [172/716], Loss: 0.1150\n",
            "Epoch [3/400], Step [173/716], Loss: 0.1272\n",
            "Epoch [3/400], Step [174/716], Loss: 0.1559\n",
            "Epoch [3/400], Step [175/716], Loss: 0.1671\n",
            "Epoch [3/400], Step [176/716], Loss: 0.1765\n",
            "Epoch [3/400], Step [177/716], Loss: 0.1650\n",
            "Epoch [3/400], Step [178/716], Loss: 0.1342\n",
            "Epoch [3/400], Step [179/716], Loss: 0.1396\n",
            "Epoch [3/400], Step [180/716], Loss: 0.1435\n",
            "Epoch [3/400], Step [181/716], Loss: 0.1346\n",
            "Epoch [3/400], Step [182/716], Loss: 0.1364\n",
            "Epoch [3/400], Step [183/716], Loss: 0.1321\n",
            "Epoch [3/400], Step [184/716], Loss: 0.1200\n",
            "Epoch [3/400], Step [185/716], Loss: 0.1192\n",
            "Epoch [3/400], Step [186/716], Loss: 0.1434\n",
            "Epoch [3/400], Step [187/716], Loss: 0.1746\n",
            "Epoch [3/400], Step [188/716], Loss: 0.1316\n",
            "Epoch [3/400], Step [189/716], Loss: 0.1362\n",
            "Epoch [3/400], Step [190/716], Loss: 0.1288\n",
            "Epoch [3/400], Step [191/716], Loss: 0.1690\n",
            "Epoch [3/400], Step [192/716], Loss: 0.1584\n",
            "Epoch [3/400], Step [193/716], Loss: 0.1322\n",
            "Epoch [3/400], Step [194/716], Loss: 0.1662\n",
            "Epoch [3/400], Step [195/716], Loss: 0.1832\n",
            "Epoch [3/400], Step [196/716], Loss: 0.1837\n",
            "Epoch [3/400], Step [197/716], Loss: 0.1362\n",
            "Epoch [3/400], Step [198/716], Loss: 0.1383\n",
            "Epoch [3/400], Step [199/716], Loss: 0.1193\n",
            "Epoch [3/400], Step [200/716], Loss: 0.1366\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_step_200.pth\n",
            "Epoch [3/400], Step [201/716], Loss: 0.1403\n",
            "Epoch [3/400], Step [202/716], Loss: 0.1262\n",
            "Epoch [3/400], Step [203/716], Loss: 0.1852\n",
            "Epoch [3/400], Step [204/716], Loss: 0.1104\n",
            "Epoch [3/400], Step [205/716], Loss: 0.1349\n",
            "Epoch [3/400], Step [206/716], Loss: 0.1285\n",
            "Epoch [3/400], Step [207/716], Loss: 0.1549\n",
            "Epoch [3/400], Step [208/716], Loss: 0.0876\n",
            "Epoch [3/400], Step [209/716], Loss: 0.1525\n",
            "Epoch [3/400], Step [210/716], Loss: 0.1134\n",
            "Epoch [3/400], Step [211/716], Loss: 0.1573\n",
            "Epoch [3/400], Step [212/716], Loss: 0.1224\n",
            "Epoch [3/400], Step [213/716], Loss: 0.1657\n",
            "Epoch [3/400], Step [214/716], Loss: 0.1383\n",
            "Epoch [3/400], Step [215/716], Loss: 0.1499\n",
            "Epoch [3/400], Step [216/716], Loss: 0.1500\n",
            "Epoch [3/400], Step [217/716], Loss: 0.1426\n",
            "Epoch [3/400], Step [218/716], Loss: 0.1469\n",
            "Epoch [3/400], Step [219/716], Loss: 0.1623\n",
            "Epoch [3/400], Step [220/716], Loss: 0.1278\n",
            "Epoch [3/400], Step [221/716], Loss: 0.1399\n",
            "Epoch [3/400], Step [222/716], Loss: 0.1551\n",
            "Epoch [3/400], Step [223/716], Loss: 0.1239\n",
            "Epoch [3/400], Step [224/716], Loss: 0.1049\n",
            "Epoch [3/400], Step [225/716], Loss: 0.2460\n",
            "Epoch [3/400], Step [226/716], Loss: 0.1297\n",
            "Epoch [3/400], Step [227/716], Loss: 0.1907\n",
            "Epoch [3/400], Step [228/716], Loss: 0.1313\n",
            "Epoch [3/400], Step [229/716], Loss: 0.1333\n",
            "Epoch [3/400], Step [230/716], Loss: 0.2066\n",
            "Epoch [3/400], Step [231/716], Loss: 0.1773\n",
            "Epoch [3/400], Step [232/716], Loss: 0.1500\n",
            "Epoch [3/400], Step [233/716], Loss: 0.1547\n",
            "Epoch [3/400], Step [234/716], Loss: 0.1549\n",
            "Epoch [3/400], Step [235/716], Loss: 0.1628\n",
            "Epoch [3/400], Step [236/716], Loss: 0.1392\n",
            "Epoch [3/400], Step [237/716], Loss: 0.2186\n",
            "Epoch [3/400], Step [238/716], Loss: 0.1932\n",
            "Epoch [3/400], Step [239/716], Loss: 0.1512\n",
            "Epoch [3/400], Step [240/716], Loss: 0.1451\n",
            "Epoch [3/400], Step [241/716], Loss: 0.1453\n",
            "Epoch [3/400], Step [242/716], Loss: 0.1859\n",
            "Epoch [3/400], Step [243/716], Loss: 0.1453\n",
            "Epoch [3/400], Step [244/716], Loss: 0.2045\n",
            "Epoch [3/400], Step [245/716], Loss: 0.1097\n",
            "Epoch [3/400], Step [246/716], Loss: 0.1583\n",
            "Epoch [3/400], Step [247/716], Loss: 0.1602\n",
            "Epoch [3/400], Step [248/716], Loss: 0.2332\n",
            "Epoch [3/400], Step [249/716], Loss: 0.1372\n",
            "Epoch [3/400], Step [250/716], Loss: 0.1530\n",
            "Epoch [3/400], Step [251/716], Loss: 0.1945\n",
            "Epoch [3/400], Step [252/716], Loss: 0.1878\n",
            "Epoch [3/400], Step [253/716], Loss: 0.1330\n",
            "Epoch [3/400], Step [254/716], Loss: 0.2018\n",
            "Epoch [3/400], Step [255/716], Loss: 0.1144\n",
            "Epoch [3/400], Step [256/716], Loss: 0.2098\n",
            "Epoch [3/400], Step [257/716], Loss: 0.1118\n",
            "Epoch [3/400], Step [258/716], Loss: 0.2508\n",
            "Epoch [3/400], Step [259/716], Loss: 0.1579\n",
            "Epoch [3/400], Step [260/716], Loss: 0.1221\n",
            "Epoch [3/400], Step [261/716], Loss: 0.1638\n",
            "Epoch [3/400], Step [262/716], Loss: 0.2400\n",
            "Epoch [3/400], Step [263/716], Loss: 0.1517\n",
            "Epoch [3/400], Step [264/716], Loss: 0.1545\n",
            "Epoch [3/400], Step [265/716], Loss: 0.1296\n",
            "Epoch [3/400], Step [266/716], Loss: 0.1297\n",
            "Epoch [3/400], Step [267/716], Loss: 0.0969\n",
            "Epoch [3/400], Step [268/716], Loss: 0.1063\n",
            "Epoch [3/400], Step [269/716], Loss: 0.1216\n",
            "Epoch [3/400], Step [270/716], Loss: 0.2043\n",
            "Epoch [3/400], Step [271/716], Loss: 0.1601\n",
            "Epoch [3/400], Step [272/716], Loss: 0.1371\n",
            "Epoch [3/400], Step [273/716], Loss: 0.1192\n",
            "Epoch [3/400], Step [274/716], Loss: 0.1193\n",
            "Epoch [3/400], Step [275/716], Loss: 0.1348\n",
            "Epoch [3/400], Step [276/716], Loss: 0.1214\n",
            "Epoch [3/400], Step [277/716], Loss: 0.1458\n",
            "Epoch [3/400], Step [278/716], Loss: 0.1430\n",
            "Epoch [3/400], Step [279/716], Loss: 0.1708\n",
            "Epoch [3/400], Step [280/716], Loss: 0.1161\n",
            "Epoch [3/400], Step [281/716], Loss: 0.1681\n",
            "Epoch [3/400], Step [282/716], Loss: 0.1302\n",
            "Epoch [3/400], Step [283/716], Loss: 0.1216\n",
            "Epoch [3/400], Step [284/716], Loss: 0.1381\n",
            "Epoch [3/400], Step [285/716], Loss: 0.2286\n",
            "Epoch [3/400], Step [286/716], Loss: 0.1613\n",
            "Epoch [3/400], Step [287/716], Loss: 0.1452\n",
            "Epoch [3/400], Step [288/716], Loss: 0.1356\n",
            "Epoch [3/400], Step [289/716], Loss: 0.2250\n",
            "Epoch [3/400], Step [290/716], Loss: 0.1479\n",
            "Epoch [3/400], Step [291/716], Loss: 0.1469\n",
            "Epoch [3/400], Step [292/716], Loss: 0.1130\n",
            "Epoch [3/400], Step [293/716], Loss: 0.1136\n",
            "Epoch [3/400], Step [294/716], Loss: 0.1329\n",
            "Epoch [3/400], Step [295/716], Loss: 0.0905\n",
            "Epoch [3/400], Step [296/716], Loss: 0.2385\n",
            "Epoch [3/400], Step [297/716], Loss: 0.1350\n",
            "Epoch [3/400], Step [298/716], Loss: 0.1252\n",
            "Epoch [3/400], Step [299/716], Loss: 0.1256\n",
            "Epoch [3/400], Step [300/716], Loss: 0.1397\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_step_300.pth\n",
            "Epoch [3/400], Step [301/716], Loss: 0.1124\n",
            "Epoch [3/400], Step [302/716], Loss: 0.1203\n",
            "Epoch [3/400], Step [303/716], Loss: 0.1340\n",
            "Epoch [3/400], Step [304/716], Loss: 0.1588\n",
            "Epoch [3/400], Step [305/716], Loss: 0.0933\n",
            "Epoch [3/400], Step [306/716], Loss: 0.1295\n",
            "Epoch [3/400], Step [307/716], Loss: 0.1759\n",
            "Epoch [3/400], Step [308/716], Loss: 0.1439\n",
            "Epoch [3/400], Step [309/716], Loss: 0.1081\n",
            "Epoch [3/400], Step [310/716], Loss: 0.0995\n",
            "Epoch [3/400], Step [311/716], Loss: 0.1688\n",
            "Epoch [3/400], Step [312/716], Loss: 0.1585\n",
            "Epoch [3/400], Step [313/716], Loss: 0.1720\n",
            "Epoch [3/400], Step [314/716], Loss: 0.1262\n",
            "Epoch [3/400], Step [315/716], Loss: 0.1622\n",
            "Epoch [3/400], Step [316/716], Loss: 0.1889\n",
            "Epoch [3/400], Step [317/716], Loss: 0.1597\n",
            "Epoch [3/400], Step [318/716], Loss: 0.1326\n",
            "Epoch [3/400], Step [319/716], Loss: 0.1238\n",
            "Epoch [3/400], Step [320/716], Loss: 0.1744\n",
            "Epoch [3/400], Step [321/716], Loss: 0.1913\n",
            "Epoch [3/400], Step [322/716], Loss: 0.1832\n",
            "Epoch [3/400], Step [323/716], Loss: 0.1340\n",
            "Epoch [3/400], Step [324/716], Loss: 0.1499\n",
            "Epoch [3/400], Step [325/716], Loss: 0.1547\n",
            "Epoch [3/400], Step [326/716], Loss: 0.1387\n",
            "Epoch [3/400], Step [327/716], Loss: 0.1510\n",
            "Epoch [3/400], Step [328/716], Loss: 0.1637\n",
            "Epoch [3/400], Step [329/716], Loss: 0.2015\n",
            "Epoch [3/400], Step [330/716], Loss: 0.1027\n",
            "Epoch [3/400], Step [331/716], Loss: 0.1820\n",
            "Epoch [3/400], Step [332/716], Loss: 0.1423\n",
            "Epoch [3/400], Step [333/716], Loss: 0.1332\n",
            "Epoch [3/400], Step [334/716], Loss: 0.1789\n",
            "Epoch [3/400], Step [335/716], Loss: 0.1463\n",
            "Epoch [3/400], Step [336/716], Loss: 0.1315\n",
            "Epoch [3/400], Step [337/716], Loss: 0.1457\n",
            "Epoch [3/400], Step [338/716], Loss: 0.1866\n",
            "Epoch [3/400], Step [339/716], Loss: 0.1387\n",
            "Epoch [3/400], Step [340/716], Loss: 0.1620\n",
            "Epoch [3/400], Step [341/716], Loss: 0.1093\n",
            "Epoch [3/400], Step [342/716], Loss: 0.1151\n",
            "Epoch [3/400], Step [343/716], Loss: 0.1766\n",
            "Epoch [3/400], Step [344/716], Loss: 0.1373\n",
            "Epoch [3/400], Step [345/716], Loss: 0.1558\n",
            "Epoch [3/400], Step [346/716], Loss: 0.1112\n",
            "Epoch [3/400], Step [347/716], Loss: 0.1514\n",
            "Epoch [3/400], Step [348/716], Loss: 0.1471\n",
            "Epoch [3/400], Step [349/716], Loss: 0.1068\n",
            "Epoch [3/400], Step [350/716], Loss: 0.1063\n",
            "Epoch [3/400], Step [351/716], Loss: 0.1443\n",
            "Epoch [3/400], Step [352/716], Loss: 0.1198\n",
            "Epoch [3/400], Step [353/716], Loss: 0.1105\n",
            "Epoch [3/400], Step [354/716], Loss: 0.1321\n",
            "Epoch [3/400], Step [355/716], Loss: 0.1233\n",
            "Epoch [3/400], Step [356/716], Loss: 0.1502\n",
            "Epoch [3/400], Step [357/716], Loss: 0.1101\n",
            "Epoch [3/400], Step [358/716], Loss: 0.1230\n",
            "Epoch [3/400], Step [359/716], Loss: 0.1479\n",
            "Epoch [3/400], Step [360/716], Loss: 0.1107\n",
            "Epoch [3/400], Step [361/716], Loss: 0.1586\n",
            "Epoch [3/400], Step [362/716], Loss: 0.1843\n",
            "Epoch [3/400], Step [363/716], Loss: 0.1229\n",
            "Epoch [3/400], Step [364/716], Loss: 0.1429\n",
            "Epoch [3/400], Step [365/716], Loss: 0.1110\n",
            "Epoch [3/400], Step [366/716], Loss: 0.1206\n",
            "Epoch [3/400], Step [367/716], Loss: 0.1817\n",
            "Epoch [3/400], Step [368/716], Loss: 0.1345\n",
            "Epoch [3/400], Step [369/716], Loss: 0.1812\n",
            "Epoch [3/400], Step [370/716], Loss: 0.0997\n",
            "Epoch [3/400], Step [371/716], Loss: 0.1861\n",
            "Epoch [3/400], Step [372/716], Loss: 0.1507\n",
            "Epoch [3/400], Step [373/716], Loss: 0.1412\n",
            "Epoch [3/400], Step [374/716], Loss: 0.1388\n",
            "Epoch [3/400], Step [375/716], Loss: 0.1489\n",
            "Epoch [3/400], Step [376/716], Loss: 0.1357\n",
            "Epoch [3/400], Step [377/716], Loss: 0.1331\n",
            "Epoch [3/400], Step [378/716], Loss: 0.1472\n",
            "Epoch [3/400], Step [379/716], Loss: 0.1098\n",
            "Epoch [3/400], Step [380/716], Loss: 0.1239\n",
            "Epoch [3/400], Step [381/716], Loss: 0.2041\n",
            "Epoch [3/400], Step [382/716], Loss: 0.1088\n",
            "Epoch [3/400], Step [383/716], Loss: 0.2131\n",
            "Epoch [3/400], Step [384/716], Loss: 0.1424\n",
            "Epoch [3/400], Step [385/716], Loss: 0.1052\n",
            "Epoch [3/400], Step [386/716], Loss: 0.2157\n",
            "Epoch [3/400], Step [387/716], Loss: 0.1322\n",
            "Epoch [3/400], Step [388/716], Loss: 0.1269\n",
            "Epoch [3/400], Step [389/716], Loss: 0.1627\n",
            "Epoch [3/400], Step [390/716], Loss: 0.1302\n",
            "Epoch [3/400], Step [391/716], Loss: 0.1279\n",
            "Epoch [3/400], Step [392/716], Loss: 0.1046\n",
            "Epoch [3/400], Step [393/716], Loss: 0.1000\n",
            "Epoch [3/400], Step [394/716], Loss: 0.1176\n",
            "Epoch [3/400], Step [395/716], Loss: 0.1377\n",
            "Epoch [3/400], Step [396/716], Loss: 0.1609\n",
            "Epoch [3/400], Step [397/716], Loss: 0.1408\n",
            "Epoch [3/400], Step [398/716], Loss: 0.2379\n",
            "Epoch [3/400], Step [399/716], Loss: 0.1690\n",
            "Epoch [3/400], Step [400/716], Loss: 0.2052\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_step_400.pth\n",
            "Epoch [3/400], Step [401/716], Loss: 0.0939\n",
            "Epoch [3/400], Step [402/716], Loss: 0.1409\n",
            "Epoch [3/400], Step [403/716], Loss: 0.1820\n",
            "Epoch [3/400], Step [404/716], Loss: 0.1822\n",
            "Epoch [3/400], Step [405/716], Loss: 0.1953\n",
            "Epoch [3/400], Step [406/716], Loss: 0.2068\n",
            "Epoch [3/400], Step [407/716], Loss: 0.1382\n",
            "Epoch [3/400], Step [408/716], Loss: 0.1112\n",
            "Epoch [3/400], Step [409/716], Loss: 0.1263\n",
            "Epoch [3/400], Step [410/716], Loss: 0.1619\n",
            "Epoch [3/400], Step [411/716], Loss: 0.1614\n",
            "Epoch [3/400], Step [412/716], Loss: 0.1211\n",
            "Epoch [3/400], Step [413/716], Loss: 0.1797\n",
            "Epoch [3/400], Step [414/716], Loss: 0.1225\n",
            "Epoch [3/400], Step [415/716], Loss: 0.1312\n",
            "Epoch [3/400], Step [416/716], Loss: 0.1671\n",
            "Epoch [3/400], Step [417/716], Loss: 0.1668\n",
            "Epoch [3/400], Step [418/716], Loss: 0.1215\n",
            "Epoch [3/400], Step [419/716], Loss: 0.1230\n",
            "Epoch [3/400], Step [420/716], Loss: 0.1458\n",
            "Epoch [3/400], Step [421/716], Loss: 0.1115\n",
            "Epoch [3/400], Step [422/716], Loss: 0.1275\n",
            "Epoch [3/400], Step [423/716], Loss: 0.1143\n",
            "Epoch [3/400], Step [424/716], Loss: 0.1283\n",
            "Epoch [3/400], Step [425/716], Loss: 0.1500\n",
            "Epoch [3/400], Step [426/716], Loss: 0.1152\n",
            "Epoch [3/400], Step [427/716], Loss: 0.1667\n",
            "Epoch [3/400], Step [428/716], Loss: 0.1580\n",
            "Epoch [3/400], Step [429/716], Loss: 0.1295\n",
            "Epoch [3/400], Step [430/716], Loss: 0.1694\n",
            "Epoch [3/400], Step [431/716], Loss: 0.1272\n",
            "Epoch [3/400], Step [432/716], Loss: 0.1228\n",
            "Epoch [3/400], Step [433/716], Loss: 0.1796\n",
            "Epoch [3/400], Step [434/716], Loss: 0.1357\n",
            "Epoch [3/400], Step [435/716], Loss: 0.1350\n",
            "Epoch [3/400], Step [436/716], Loss: 0.1901\n",
            "Epoch [3/400], Step [437/716], Loss: 0.3157\n",
            "Epoch [3/400], Step [438/716], Loss: 0.1847\n",
            "Epoch [3/400], Step [439/716], Loss: 0.1876\n",
            "Epoch [3/400], Step [440/716], Loss: 0.1496\n",
            "Epoch [3/400], Step [441/716], Loss: 0.1363\n",
            "Epoch [3/400], Step [442/716], Loss: 0.1594\n",
            "Epoch [3/400], Step [443/716], Loss: 0.1390\n",
            "Epoch [3/400], Step [444/716], Loss: 0.1586\n",
            "Epoch [3/400], Step [445/716], Loss: 0.1368\n",
            "Epoch [3/400], Step [446/716], Loss: 0.1309\n",
            "Epoch [3/400], Step [447/716], Loss: 0.1530\n",
            "Epoch [3/400], Step [448/716], Loss: 0.1861\n",
            "Epoch [3/400], Step [449/716], Loss: 0.1494\n",
            "Epoch [3/400], Step [450/716], Loss: 0.1425\n",
            "Epoch [3/400], Step [451/716], Loss: 0.2179\n",
            "Epoch [3/400], Step [452/716], Loss: 0.1714\n",
            "Epoch [3/400], Step [453/716], Loss: 0.1038\n",
            "Epoch [3/400], Step [454/716], Loss: 0.1544\n",
            "Epoch [3/400], Step [455/716], Loss: 0.1384\n",
            "Epoch [3/400], Step [456/716], Loss: 0.2018\n",
            "Epoch [3/400], Step [457/716], Loss: 0.0867\n",
            "Epoch [3/400], Step [458/716], Loss: 0.1191\n",
            "Epoch [3/400], Step [459/716], Loss: 0.1108\n",
            "Epoch [3/400], Step [460/716], Loss: 0.1301\n",
            "Epoch [3/400], Step [461/716], Loss: 0.1646\n",
            "Epoch [3/400], Step [462/716], Loss: 0.1049\n",
            "Epoch [3/400], Step [463/716], Loss: 0.1648\n",
            "Epoch [3/400], Step [464/716], Loss: 0.1686\n",
            "Epoch [3/400], Step [465/716], Loss: 0.1038\n",
            "Epoch [3/400], Step [466/716], Loss: 0.1294\n",
            "Epoch [3/400], Step [467/716], Loss: 0.1547\n",
            "Epoch [3/400], Step [468/716], Loss: 0.1179\n",
            "Epoch [3/400], Step [469/716], Loss: 0.1222\n",
            "Epoch [3/400], Step [470/716], Loss: 0.1440\n",
            "Epoch [3/400], Step [471/716], Loss: 0.1515\n",
            "Epoch [3/400], Step [472/716], Loss: 0.1318\n",
            "Epoch [3/400], Step [473/716], Loss: 0.1593\n",
            "Epoch [3/400], Step [474/716], Loss: 0.1293\n",
            "Epoch [3/400], Step [475/716], Loss: 0.1260\n",
            "Epoch [3/400], Step [476/716], Loss: 0.1672\n",
            "Epoch [3/400], Step [477/716], Loss: 0.1109\n",
            "Epoch [3/400], Step [478/716], Loss: 0.1881\n",
            "Epoch [3/400], Step [479/716], Loss: 0.1680\n",
            "Epoch [3/400], Step [480/716], Loss: 0.2454\n",
            "Epoch [3/400], Step [481/716], Loss: 0.1372\n",
            "Epoch [3/400], Step [482/716], Loss: 0.1541\n",
            "Epoch [3/400], Step [483/716], Loss: 0.1328\n",
            "Epoch [3/400], Step [484/716], Loss: 0.1142\n",
            "Epoch [3/400], Step [485/716], Loss: 0.1592\n",
            "Epoch [3/400], Step [486/716], Loss: 0.1642\n",
            "Epoch [3/400], Step [487/716], Loss: 0.1267\n",
            "Epoch [3/400], Step [488/716], Loss: 0.1707\n",
            "Epoch [3/400], Step [489/716], Loss: 0.1121\n",
            "Epoch [3/400], Step [490/716], Loss: 0.1097\n",
            "Epoch [3/400], Step [491/716], Loss: 0.1334\n",
            "Epoch [3/400], Step [492/716], Loss: 0.1547\n",
            "Epoch [3/400], Step [493/716], Loss: 0.2291\n",
            "Epoch [3/400], Step [494/716], Loss: 0.1191\n",
            "Epoch [3/400], Step [495/716], Loss: 0.1286\n",
            "Epoch [3/400], Step [496/716], Loss: 0.1322\n",
            "Epoch [3/400], Step [497/716], Loss: 0.1020\n",
            "Epoch [3/400], Step [498/716], Loss: 0.1330\n",
            "Epoch [3/400], Step [499/716], Loss: 0.1016\n",
            "Epoch [3/400], Step [500/716], Loss: 0.1325\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_step_500.pth\n",
            "Epoch [3/400], Step [501/716], Loss: 0.1385\n",
            "Epoch [3/400], Step [502/716], Loss: 0.1187\n",
            "Epoch [3/400], Step [503/716], Loss: 0.0895\n",
            "Epoch [3/400], Step [504/716], Loss: 0.2059\n",
            "Epoch [3/400], Step [505/716], Loss: 0.2122\n",
            "Epoch [3/400], Step [506/716], Loss: 0.2102\n",
            "Epoch [3/400], Step [507/716], Loss: 0.1535\n",
            "Epoch [3/400], Step [508/716], Loss: 0.1035\n",
            "Epoch [3/400], Step [509/716], Loss: 0.1440\n",
            "Epoch [3/400], Step [510/716], Loss: 0.1048\n",
            "Epoch [3/400], Step [511/716], Loss: 0.0856\n",
            "Epoch [3/400], Step [512/716], Loss: 0.1607\n",
            "Epoch [3/400], Step [513/716], Loss: 0.1027\n",
            "Epoch [3/400], Step [514/716], Loss: 0.1516\n",
            "Epoch [3/400], Step [515/716], Loss: 0.1217\n",
            "Epoch [3/400], Step [516/716], Loss: 0.1395\n",
            "Epoch [3/400], Step [517/716], Loss: 0.1363\n",
            "Epoch [3/400], Step [518/716], Loss: 0.1894\n",
            "Epoch [3/400], Step [519/716], Loss: 0.1415\n",
            "Epoch [3/400], Step [520/716], Loss: 0.1125\n",
            "Epoch [3/400], Step [521/716], Loss: 0.1178\n",
            "Epoch [3/400], Step [522/716], Loss: 0.1333\n",
            "Epoch [3/400], Step [523/716], Loss: 0.1301\n",
            "Epoch [3/400], Step [524/716], Loss: 0.1657\n",
            "Epoch [3/400], Step [525/716], Loss: 0.1821\n",
            "Epoch [3/400], Step [526/716], Loss: 0.1730\n",
            "Epoch [3/400], Step [527/716], Loss: 0.1201\n",
            "Epoch [3/400], Step [528/716], Loss: 0.1139\n",
            "Epoch [3/400], Step [529/716], Loss: 0.1409\n",
            "Epoch [3/400], Step [530/716], Loss: 0.2189\n",
            "Epoch [3/400], Step [531/716], Loss: 0.1972\n",
            "Epoch [3/400], Step [532/716], Loss: 0.1251\n",
            "Epoch [3/400], Step [533/716], Loss: 0.1578\n",
            "Epoch [3/400], Step [534/716], Loss: 0.1945\n",
            "Epoch [3/400], Step [535/716], Loss: 0.1525\n",
            "Epoch [3/400], Step [536/716], Loss: 0.1682\n",
            "Epoch [3/400], Step [537/716], Loss: 0.2051\n",
            "Epoch [3/400], Step [538/716], Loss: 0.1410\n",
            "Epoch [3/400], Step [539/716], Loss: 0.1280\n",
            "Epoch [3/400], Step [540/716], Loss: 0.1756\n",
            "Epoch [3/400], Step [541/716], Loss: 0.1076\n",
            "Epoch [3/400], Step [542/716], Loss: 0.1574\n",
            "Epoch [3/400], Step [543/716], Loss: 0.1945\n",
            "Epoch [3/400], Step [544/716], Loss: 0.1493\n",
            "Epoch [3/400], Step [545/716], Loss: 0.1158\n",
            "Epoch [3/400], Step [546/716], Loss: 0.3072\n",
            "Epoch [3/400], Step [547/716], Loss: 0.1351\n",
            "Epoch [3/400], Step [548/716], Loss: 0.1452\n",
            "Epoch [3/400], Step [549/716], Loss: 0.1550\n",
            "Epoch [3/400], Step [550/716], Loss: 0.1368\n",
            "Epoch [3/400], Step [551/716], Loss: 0.1389\n",
            "Epoch [3/400], Step [552/716], Loss: 0.1513\n",
            "Epoch [3/400], Step [553/716], Loss: 0.1456\n",
            "Epoch [3/400], Step [554/716], Loss: 0.1251\n",
            "Epoch [3/400], Step [555/716], Loss: 0.1289\n",
            "Epoch [3/400], Step [556/716], Loss: 0.1421\n",
            "Epoch [3/400], Step [557/716], Loss: 0.1189\n",
            "Epoch [3/400], Step [558/716], Loss: 0.1463\n",
            "Epoch [3/400], Step [559/716], Loss: 0.1695\n",
            "Epoch [3/400], Step [560/716], Loss: 0.1737\n",
            "Epoch [3/400], Step [561/716], Loss: 0.1168\n",
            "Epoch [3/400], Step [562/716], Loss: 0.1034\n",
            "Epoch [3/400], Step [563/716], Loss: 0.1132\n",
            "Epoch [3/400], Step [564/716], Loss: 0.1300\n",
            "Epoch [3/400], Step [565/716], Loss: 0.2029\n",
            "Epoch [3/400], Step [566/716], Loss: 0.1240\n",
            "Epoch [3/400], Step [567/716], Loss: 0.1454\n",
            "Epoch [3/400], Step [568/716], Loss: 0.1378\n",
            "Epoch [3/400], Step [569/716], Loss: 0.1478\n",
            "Epoch [3/400], Step [570/716], Loss: 0.1265\n",
            "Epoch [3/400], Step [571/716], Loss: 0.1205\n",
            "Epoch [3/400], Step [572/716], Loss: 0.1148\n",
            "Epoch [3/400], Step [573/716], Loss: 0.1399\n",
            "Epoch [3/400], Step [574/716], Loss: 0.1188\n",
            "Epoch [3/400], Step [575/716], Loss: 0.1061\n",
            "Epoch [3/400], Step [576/716], Loss: 0.1856\n",
            "Epoch [3/400], Step [577/716], Loss: 0.0959\n",
            "Epoch [3/400], Step [578/716], Loss: 0.0988\n",
            "Epoch [3/400], Step [579/716], Loss: 0.1268\n",
            "Epoch [3/400], Step [580/716], Loss: 0.0995\n",
            "Epoch [3/400], Step [581/716], Loss: 0.1194\n",
            "Epoch [3/400], Step [582/716], Loss: 0.1306\n",
            "Epoch [3/400], Step [583/716], Loss: 0.1361\n",
            "Epoch [3/400], Step [584/716], Loss: 0.0879\n",
            "Epoch [3/400], Step [585/716], Loss: 0.1294\n",
            "Epoch [3/400], Step [586/716], Loss: 0.1538\n",
            "Epoch [3/400], Step [587/716], Loss: 0.2140\n",
            "Epoch [3/400], Step [588/716], Loss: 0.1274\n",
            "Epoch [3/400], Step [589/716], Loss: 0.0876\n",
            "Epoch [3/400], Step [590/716], Loss: 0.1045\n",
            "Epoch [3/400], Step [591/716], Loss: 0.1124\n",
            "Epoch [3/400], Step [592/716], Loss: 0.1283\n",
            "Epoch [3/400], Step [593/716], Loss: 0.1866\n",
            "Epoch [3/400], Step [594/716], Loss: 0.1189\n",
            "Epoch [3/400], Step [595/716], Loss: 0.1652\n",
            "Epoch [3/400], Step [596/716], Loss: 0.1361\n",
            "Epoch [3/400], Step [597/716], Loss: 0.0914\n",
            "Epoch [3/400], Step [598/716], Loss: 0.0996\n",
            "Epoch [3/400], Step [599/716], Loss: 0.1479\n",
            "Epoch [3/400], Step [600/716], Loss: 0.0943\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_step_600.pth\n",
            "Epoch [3/400], Step [601/716], Loss: 0.1053\n",
            "Epoch [3/400], Step [602/716], Loss: 0.1119\n",
            "Epoch [3/400], Step [603/716], Loss: 0.1375\n",
            "Epoch [3/400], Step [604/716], Loss: 0.1279\n",
            "Epoch [3/400], Step [605/716], Loss: 0.1355\n",
            "Epoch [3/400], Step [606/716], Loss: 0.1323\n",
            "Epoch [3/400], Step [607/716], Loss: 0.1719\n",
            "Epoch [3/400], Step [608/716], Loss: 0.1333\n",
            "Epoch [3/400], Step [609/716], Loss: 0.1131\n",
            "Epoch [3/400], Step [610/716], Loss: 0.1201\n",
            "Epoch [3/400], Step [611/716], Loss: 0.1106\n",
            "Epoch [3/400], Step [612/716], Loss: 0.1149\n",
            "Epoch [3/400], Step [613/716], Loss: 0.1092\n",
            "Epoch [3/400], Step [614/716], Loss: 0.1029\n",
            "Epoch [3/400], Step [615/716], Loss: 0.1186\n",
            "Epoch [3/400], Step [616/716], Loss: 0.1437\n",
            "Epoch [3/400], Step [617/716], Loss: 0.1682\n",
            "Epoch [3/400], Step [618/716], Loss: 0.1415\n",
            "Epoch [3/400], Step [619/716], Loss: 0.1166\n",
            "Epoch [3/400], Step [620/716], Loss: 0.1688\n",
            "Epoch [3/400], Step [621/716], Loss: 0.1127\n",
            "Epoch [3/400], Step [622/716], Loss: 0.2013\n",
            "Epoch [3/400], Step [623/716], Loss: 0.1168\n",
            "Epoch [3/400], Step [624/716], Loss: 0.1451\n",
            "Epoch [3/400], Step [625/716], Loss: 0.0973\n",
            "Epoch [3/400], Step [626/716], Loss: 0.1288\n",
            "Epoch [3/400], Step [627/716], Loss: 0.1466\n",
            "Epoch [3/400], Step [628/716], Loss: 0.1111\n",
            "Epoch [3/400], Step [629/716], Loss: 0.1263\n",
            "Epoch [3/400], Step [630/716], Loss: 0.1922\n",
            "Epoch [3/400], Step [631/716], Loss: 0.1035\n",
            "Epoch [3/400], Step [632/716], Loss: 0.1230\n",
            "Epoch [3/400], Step [633/716], Loss: 0.1451\n",
            "Epoch [3/400], Step [634/716], Loss: 0.1213\n",
            "Epoch [3/400], Step [635/716], Loss: 0.1725\n",
            "Epoch [3/400], Step [636/716], Loss: 0.1026\n",
            "Epoch [3/400], Step [637/716], Loss: 0.1345\n",
            "Epoch [3/400], Step [638/716], Loss: 0.1135\n",
            "Epoch [3/400], Step [639/716], Loss: 0.1202\n",
            "Epoch [3/400], Step [640/716], Loss: 0.1008\n",
            "Epoch [3/400], Step [641/716], Loss: 0.1344\n",
            "Epoch [3/400], Step [642/716], Loss: 0.1348\n",
            "Epoch [3/400], Step [643/716], Loss: 0.1143\n",
            "Epoch [3/400], Step [644/716], Loss: 0.1118\n",
            "Epoch [3/400], Step [645/716], Loss: 0.2216\n",
            "Epoch [3/400], Step [646/716], Loss: 0.1805\n",
            "Epoch [3/400], Step [647/716], Loss: 0.1671\n",
            "Epoch [3/400], Step [648/716], Loss: 0.1205\n",
            "Epoch [3/400], Step [649/716], Loss: 0.1343\n",
            "Epoch [3/400], Step [650/716], Loss: 0.1451\n",
            "Epoch [3/400], Step [651/716], Loss: 0.1231\n",
            "Epoch [3/400], Step [652/716], Loss: 0.1127\n",
            "Epoch [3/400], Step [653/716], Loss: 0.1613\n",
            "Epoch [3/400], Step [654/716], Loss: 0.1321\n",
            "Epoch [3/400], Step [655/716], Loss: 0.1279\n",
            "Epoch [3/400], Step [656/716], Loss: 0.1216\n",
            "Epoch [3/400], Step [657/716], Loss: 0.1457\n",
            "Epoch [3/400], Step [658/716], Loss: 0.1862\n",
            "Epoch [3/400], Step [659/716], Loss: 0.1334\n",
            "Epoch [3/400], Step [660/716], Loss: 0.0980\n",
            "Epoch [3/400], Step [661/716], Loss: 0.0993\n",
            "Epoch [3/400], Step [662/716], Loss: 0.1278\n",
            "Epoch [3/400], Step [663/716], Loss: 0.1099\n",
            "Epoch [3/400], Step [664/716], Loss: 0.1102\n",
            "Epoch [3/400], Step [665/716], Loss: 0.1378\n",
            "Epoch [3/400], Step [666/716], Loss: 0.1353\n",
            "Epoch [3/400], Step [667/716], Loss: 0.1357\n",
            "Epoch [3/400], Step [668/716], Loss: 0.1634\n",
            "Epoch [3/400], Step [669/716], Loss: 0.1314\n",
            "Epoch [3/400], Step [670/716], Loss: 0.1314\n",
            "Epoch [3/400], Step [671/716], Loss: 0.0980\n",
            "Epoch [3/400], Step [672/716], Loss: 0.1588\n",
            "Epoch [3/400], Step [673/716], Loss: 0.1205\n",
            "Epoch [3/400], Step [674/716], Loss: 0.1180\n",
            "Epoch [3/400], Step [675/716], Loss: 0.1428\n",
            "Epoch [3/400], Step [676/716], Loss: 0.1779\n",
            "Epoch [3/400], Step [677/716], Loss: 0.1723\n",
            "Epoch [3/400], Step [678/716], Loss: 0.1538\n",
            "Epoch [3/400], Step [679/716], Loss: 0.1276\n",
            "Epoch [3/400], Step [680/716], Loss: 0.1763\n",
            "Epoch [3/400], Step [681/716], Loss: 0.1444\n",
            "Epoch [3/400], Step [682/716], Loss: 0.1723\n",
            "Epoch [3/400], Step [683/716], Loss: 0.1593\n",
            "Epoch [3/400], Step [684/716], Loss: 0.1055\n",
            "Epoch [3/400], Step [685/716], Loss: 0.1459\n",
            "Epoch [3/400], Step [686/716], Loss: 0.1177\n",
            "Epoch [3/400], Step [687/716], Loss: 0.1178\n",
            "Epoch [3/400], Step [688/716], Loss: 0.1147\n",
            "Epoch [3/400], Step [689/716], Loss: 0.1860\n",
            "Epoch [3/400], Step [690/716], Loss: 0.1075\n",
            "Epoch [3/400], Step [691/716], Loss: 0.1876\n",
            "Epoch [3/400], Step [692/716], Loss: 0.1705\n",
            "Epoch [3/400], Step [693/716], Loss: 0.1247\n",
            "Epoch [3/400], Step [694/716], Loss: 0.1512\n",
            "Epoch [3/400], Step [695/716], Loss: 0.1382\n",
            "Epoch [3/400], Step [696/716], Loss: 0.1510\n",
            "Epoch [3/400], Step [697/716], Loss: 0.1498\n",
            "Epoch [3/400], Step [698/716], Loss: 0.1192\n",
            "Epoch [3/400], Step [699/716], Loss: 0.1552\n",
            "Epoch [3/400], Step [700/716], Loss: 0.1394\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_step_700.pth\n",
            "Epoch [3/400], Step [701/716], Loss: 0.1169\n",
            "Epoch [3/400], Step [702/716], Loss: 0.1285\n",
            "Epoch [3/400], Step [703/716], Loss: 0.1414\n",
            "Epoch [3/400], Step [704/716], Loss: 0.1233\n",
            "Epoch [3/400], Step [705/716], Loss: 0.2019\n",
            "Epoch [3/400], Step [706/716], Loss: 0.1212\n",
            "Epoch [3/400], Step [707/716], Loss: 0.1426\n",
            "Epoch [3/400], Step [708/716], Loss: 0.1167\n",
            "Epoch [3/400], Step [709/716], Loss: 0.1085\n",
            "Epoch [3/400], Step [710/716], Loss: 0.1559\n",
            "Epoch [3/400], Step [711/716], Loss: 0.1456\n",
            "Epoch [3/400], Step [712/716], Loss: 0.1578\n",
            "Epoch [3/400], Step [713/716], Loss: 0.1280\n",
            "Epoch [3/400], Step [714/716], Loss: 0.1127\n",
            "Epoch [3/400], Step [715/716], Loss: 0.1433\n",
            "Epoch [3/400], Step [716/716], Loss: 0.1274\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_3_last.pth\n",
            "Epoch [4/400], Step [1/716], Loss: 0.1879\n",
            "Epoch [4/400], Step [2/716], Loss: 0.1204\n",
            "Epoch [4/400], Step [3/716], Loss: 0.1041\n",
            "Epoch [4/400], Step [4/716], Loss: 0.1137\n",
            "Epoch [4/400], Step [5/716], Loss: 0.1171\n",
            "Epoch [4/400], Step [6/716], Loss: 0.1760\n",
            "Epoch [4/400], Step [7/716], Loss: 0.1475\n",
            "Epoch [4/400], Step [8/716], Loss: 0.1440\n",
            "Epoch [4/400], Step [9/716], Loss: 0.1457\n",
            "Epoch [4/400], Step [10/716], Loss: 0.1059\n",
            "Epoch [4/400], Step [11/716], Loss: 0.1516\n",
            "Epoch [4/400], Step [12/716], Loss: 0.1456\n",
            "Epoch [4/400], Step [13/716], Loss: 0.1659\n",
            "Epoch [4/400], Step [14/716], Loss: 0.1353\n",
            "Epoch [4/400], Step [15/716], Loss: 0.1205\n",
            "Epoch [4/400], Step [16/716], Loss: 0.1385\n",
            "Epoch [4/400], Step [17/716], Loss: 0.1236\n",
            "Epoch [4/400], Step [18/716], Loss: 0.1402\n",
            "Epoch [4/400], Step [19/716], Loss: 0.1805\n",
            "Epoch [4/400], Step [20/716], Loss: 0.0731\n",
            "Epoch [4/400], Step [21/716], Loss: 0.1246\n",
            "Epoch [4/400], Step [22/716], Loss: 0.1331\n",
            "Epoch [4/400], Step [23/716], Loss: 0.1166\n",
            "Epoch [4/400], Step [24/716], Loss: 0.1088\n",
            "Epoch [4/400], Step [25/716], Loss: 0.0932\n",
            "Epoch [4/400], Step [26/716], Loss: 0.1460\n",
            "Epoch [4/400], Step [27/716], Loss: 0.1742\n",
            "Epoch [4/400], Step [28/716], Loss: 0.1351\n",
            "Epoch [4/400], Step [29/716], Loss: 0.1187\n",
            "Epoch [4/400], Step [30/716], Loss: 0.1380\n",
            "Epoch [4/400], Step [31/716], Loss: 0.1202\n",
            "Epoch [4/400], Step [32/716], Loss: 0.1028\n",
            "Epoch [4/400], Step [33/716], Loss: 0.1052\n",
            "Epoch [4/400], Step [34/716], Loss: 0.1864\n",
            "Epoch [4/400], Step [35/716], Loss: 0.1227\n",
            "Epoch [4/400], Step [36/716], Loss: 0.1690\n",
            "Epoch [4/400], Step [37/716], Loss: 0.1017\n",
            "Epoch [4/400], Step [38/716], Loss: 0.2359\n",
            "Epoch [4/400], Step [39/716], Loss: 0.0926\n",
            "Epoch [4/400], Step [40/716], Loss: 0.1262\n",
            "Epoch [4/400], Step [41/716], Loss: 0.1287\n",
            "Epoch [4/400], Step [42/716], Loss: 0.1395\n",
            "Epoch [4/400], Step [43/716], Loss: 0.1231\n",
            "Epoch [4/400], Step [44/716], Loss: 0.1373\n",
            "Epoch [4/400], Step [45/716], Loss: 0.1095\n",
            "Epoch [4/400], Step [46/716], Loss: 0.1189\n",
            "Epoch [4/400], Step [47/716], Loss: 0.0936\n",
            "Epoch [4/400], Step [48/716], Loss: 0.1019\n",
            "Epoch [4/400], Step [49/716], Loss: 0.1303\n",
            "Epoch [4/400], Step [50/716], Loss: 0.1251\n",
            "Epoch [4/400], Step [51/716], Loss: 0.1180\n",
            "Epoch [4/400], Step [52/716], Loss: 0.1381\n",
            "Epoch [4/400], Step [53/716], Loss: 0.1830\n",
            "Epoch [4/400], Step [54/716], Loss: 0.1220\n",
            "Epoch [4/400], Step [55/716], Loss: 0.1468\n",
            "Epoch [4/400], Step [56/716], Loss: 0.1228\n",
            "Epoch [4/400], Step [57/716], Loss: 0.1259\n",
            "Epoch [4/400], Step [58/716], Loss: 0.1264\n",
            "Epoch [4/400], Step [59/716], Loss: 0.1235\n",
            "Epoch [4/400], Step [60/716], Loss: 0.1736\n",
            "Epoch [4/400], Step [61/716], Loss: 0.1703\n",
            "Epoch [4/400], Step [62/716], Loss: 0.1152\n",
            "Epoch [4/400], Step [63/716], Loss: 0.1435\n",
            "Epoch [4/400], Step [64/716], Loss: 0.1678\n",
            "Epoch [4/400], Step [65/716], Loss: 0.1661\n",
            "Epoch [4/400], Step [66/716], Loss: 0.2409\n",
            "Epoch [4/400], Step [67/716], Loss: 0.0994\n",
            "Epoch [4/400], Step [68/716], Loss: 0.1781\n",
            "Epoch [4/400], Step [69/716], Loss: 0.2363\n",
            "Epoch [4/400], Step [70/716], Loss: 0.1757\n",
            "Epoch [4/400], Step [71/716], Loss: 0.2112\n",
            "Epoch [4/400], Step [72/716], Loss: 0.1676\n",
            "Epoch [4/400], Step [73/716], Loss: 0.1367\n",
            "Epoch [4/400], Step [74/716], Loss: 0.1851\n",
            "Epoch [4/400], Step [75/716], Loss: 0.1515\n",
            "Epoch [4/400], Step [76/716], Loss: 0.1605\n",
            "Epoch [4/400], Step [77/716], Loss: 0.1201\n",
            "Epoch [4/400], Step [78/716], Loss: 0.1233\n",
            "Epoch [4/400], Step [79/716], Loss: 0.1502\n",
            "Epoch [4/400], Step [80/716], Loss: 0.1354\n",
            "Epoch [4/400], Step [81/716], Loss: 0.1645\n",
            "Epoch [4/400], Step [82/716], Loss: 0.1486\n",
            "Epoch [4/400], Step [83/716], Loss: 0.1552\n",
            "Epoch [4/400], Step [84/716], Loss: 0.1224\n",
            "Epoch [4/400], Step [85/716], Loss: 0.1117\n",
            "Epoch [4/400], Step [86/716], Loss: 0.1445\n",
            "Epoch [4/400], Step [87/716], Loss: 0.0983\n",
            "Epoch [4/400], Step [88/716], Loss: 0.1193\n",
            "Epoch [4/400], Step [89/716], Loss: 0.1748\n",
            "Epoch [4/400], Step [90/716], Loss: 0.1114\n",
            "Epoch [4/400], Step [91/716], Loss: 0.1269\n",
            "Epoch [4/400], Step [92/716], Loss: 0.1461\n",
            "Epoch [4/400], Step [93/716], Loss: 0.1094\n",
            "Epoch [4/400], Step [94/716], Loss: 0.0986\n",
            "Epoch [4/400], Step [95/716], Loss: 0.1453\n",
            "Epoch [4/400], Step [96/716], Loss: 0.1538\n",
            "Epoch [4/400], Step [97/716], Loss: 0.1914\n",
            "Epoch [4/400], Step [98/716], Loss: 0.1427\n",
            "Epoch [4/400], Step [99/716], Loss: 0.1197\n",
            "Epoch [4/400], Step [100/716], Loss: 0.0986\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_4_step_100.pth\n",
            "Epoch [4/400], Step [101/716], Loss: 0.1230\n",
            "Epoch [4/400], Step [102/716], Loss: 0.1836\n",
            "Epoch [4/400], Step [103/716], Loss: 0.1047\n",
            "Epoch [4/400], Step [104/716], Loss: 0.1332\n",
            "Epoch [4/400], Step [105/716], Loss: 0.1458\n",
            "Epoch [4/400], Step [106/716], Loss: 0.1037\n",
            "Epoch [4/400], Step [107/716], Loss: 0.1298\n",
            "Epoch [4/400], Step [108/716], Loss: 0.1009\n",
            "Epoch [4/400], Step [109/716], Loss: 0.1373\n",
            "Epoch [4/400], Step [110/716], Loss: 0.1669\n",
            "Epoch [4/400], Step [111/716], Loss: 0.2015\n",
            "Epoch [4/400], Step [112/716], Loss: 0.1220\n",
            "Epoch [4/400], Step [113/716], Loss: 0.1454\n",
            "Epoch [4/400], Step [114/716], Loss: 0.1293\n",
            "Epoch [4/400], Step [115/716], Loss: 0.1397\n",
            "Epoch [4/400], Step [116/716], Loss: 0.0904\n",
            "Epoch [4/400], Step [117/716], Loss: 0.1126\n",
            "Epoch [4/400], Step [118/716], Loss: 0.1094\n",
            "Epoch [4/400], Step [119/716], Loss: 0.1005\n",
            "Epoch [4/400], Step [120/716], Loss: 0.1262\n",
            "Epoch [4/400], Step [121/716], Loss: 0.1323\n",
            "Epoch [4/400], Step [122/716], Loss: 0.1272\n",
            "Epoch [4/400], Step [123/716], Loss: 0.1962\n",
            "Epoch [4/400], Step [124/716], Loss: 0.1433\n",
            "Epoch [4/400], Step [125/716], Loss: 0.1947\n",
            "Epoch [4/400], Step [126/716], Loss: 0.1573\n",
            "Epoch [4/400], Step [127/716], Loss: 0.1469\n",
            "Epoch [4/400], Step [128/716], Loss: 0.1348\n",
            "Epoch [4/400], Step [129/716], Loss: 0.1319\n",
            "Epoch [4/400], Step [130/716], Loss: 0.1733\n",
            "Epoch [4/400], Step [131/716], Loss: 0.1626\n",
            "Epoch [4/400], Step [132/716], Loss: 0.1567\n",
            "Epoch [4/400], Step [133/716], Loss: 0.1775\n",
            "Epoch [4/400], Step [134/716], Loss: 0.1960\n",
            "Epoch [4/400], Step [135/716], Loss: 0.1638\n",
            "Epoch [4/400], Step [136/716], Loss: 0.1972\n",
            "Epoch [4/400], Step [137/716], Loss: 0.1255\n",
            "Epoch [4/400], Step [138/716], Loss: 0.1403\n",
            "Epoch [4/400], Step [139/716], Loss: 0.1856\n",
            "Epoch [4/400], Step [140/716], Loss: 0.2205\n",
            "Epoch [4/400], Step [141/716], Loss: 0.1109\n",
            "Epoch [4/400], Step [142/716], Loss: 0.1503\n",
            "Epoch [4/400], Step [143/716], Loss: 0.0978\n",
            "Epoch [4/400], Step [144/716], Loss: 0.0959\n",
            "Epoch [4/400], Step [145/716], Loss: 0.0850\n",
            "Epoch [4/400], Step [146/716], Loss: 0.1049\n",
            "Epoch [4/400], Step [147/716], Loss: 0.1268\n",
            "Epoch [4/400], Step [148/716], Loss: 0.1531\n",
            "Epoch [4/400], Step [149/716], Loss: 0.1414\n",
            "Epoch [4/400], Step [150/716], Loss: 0.1691\n",
            "Epoch [4/400], Step [151/716], Loss: 0.1205\n",
            "Epoch [4/400], Step [152/716], Loss: 0.1278\n",
            "Epoch [4/400], Step [153/716], Loss: 0.1188\n",
            "Epoch [4/400], Step [154/716], Loss: 0.1080\n",
            "Epoch [4/400], Step [155/716], Loss: 0.1171\n",
            "Epoch [4/400], Step [156/716], Loss: 0.1083\n",
            "Epoch [4/400], Step [157/716], Loss: 0.1187\n",
            "Epoch [4/400], Step [158/716], Loss: 0.0942\n",
            "Epoch [4/400], Step [159/716], Loss: 0.1077\n",
            "Epoch [4/400], Step [160/716], Loss: 0.0899\n",
            "Epoch [4/400], Step [161/716], Loss: 0.1030\n",
            "Epoch [4/400], Step [162/716], Loss: 0.1483\n",
            "Epoch [4/400], Step [163/716], Loss: 0.0857\n",
            "Epoch [4/400], Step [164/716], Loss: 0.1157\n",
            "Epoch [4/400], Step [165/716], Loss: 0.1301\n",
            "Epoch [4/400], Step [166/716], Loss: 0.1259\n",
            "Epoch [4/400], Step [167/716], Loss: 0.1166\n",
            "Epoch [4/400], Step [168/716], Loss: 0.1571\n",
            "Epoch [4/400], Step [169/716], Loss: 0.1332\n",
            "Epoch [4/400], Step [170/716], Loss: 0.1157\n",
            "Epoch [4/400], Step [171/716], Loss: 0.1087\n",
            "Epoch [4/400], Step [172/716], Loss: 0.1146\n",
            "Epoch [4/400], Step [173/716], Loss: 0.0961\n",
            "Epoch [4/400], Step [174/716], Loss: 0.1414\n",
            "Epoch [4/400], Step [175/716], Loss: 0.1181\n",
            "Epoch [4/400], Step [176/716], Loss: 0.1541\n",
            "Epoch [4/400], Step [177/716], Loss: 0.1520\n",
            "Epoch [4/400], Step [178/716], Loss: 0.0957\n",
            "Epoch [4/400], Step [179/716], Loss: 0.1087\n",
            "Epoch [4/400], Step [180/716], Loss: 0.1497\n",
            "Epoch [4/400], Step [181/716], Loss: 0.1377\n",
            "Epoch [4/400], Step [182/716], Loss: 0.1081\n",
            "Epoch [4/400], Step [183/716], Loss: 0.1264\n",
            "Epoch [4/400], Step [184/716], Loss: 0.0999\n",
            "Epoch [4/400], Step [185/716], Loss: 0.0870\n",
            "Epoch [4/400], Step [186/716], Loss: 0.1235\n",
            "Epoch [4/400], Step [187/716], Loss: 0.1078\n",
            "Epoch [4/400], Step [188/716], Loss: 0.1081\n",
            "Epoch [4/400], Step [189/716], Loss: 0.1297\n",
            "Epoch [4/400], Step [190/716], Loss: 0.1369\n",
            "Epoch [4/400], Step [191/716], Loss: 0.0989\n",
            "Epoch [4/400], Step [192/716], Loss: 0.0982\n",
            "Epoch [4/400], Step [193/716], Loss: 0.1108\n",
            "Epoch [4/400], Step [194/716], Loss: 0.1196\n",
            "Epoch [4/400], Step [195/716], Loss: 0.1000\n",
            "Epoch [4/400], Step [196/716], Loss: 0.1075\n",
            "Epoch [4/400], Step [197/716], Loss: 0.1452\n",
            "Epoch [4/400], Step [198/716], Loss: 0.0975\n",
            "Epoch [4/400], Step [199/716], Loss: 0.2003\n",
            "Epoch [4/400], Step [200/716], Loss: 0.1116\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_4_step_200.pth\n",
            "Epoch [4/400], Step [201/716], Loss: 0.1410\n",
            "Epoch [4/400], Step [202/716], Loss: 0.1283\n",
            "Epoch [4/400], Step [203/716], Loss: 0.1557\n",
            "Epoch [4/400], Step [204/716], Loss: 0.1107\n",
            "Epoch [4/400], Step [205/716], Loss: 0.1128\n",
            "Epoch [4/400], Step [206/716], Loss: 0.1238\n",
            "Epoch [4/400], Step [207/716], Loss: 0.0940\n",
            "Epoch [4/400], Step [208/716], Loss: 0.1112\n",
            "Epoch [4/400], Step [209/716], Loss: 0.1961\n",
            "Epoch [4/400], Step [210/716], Loss: 0.1172\n",
            "Epoch [4/400], Step [211/716], Loss: 0.1631\n",
            "Epoch [4/400], Step [212/716], Loss: 0.1230\n",
            "Epoch [4/400], Step [213/716], Loss: 0.0875\n",
            "Epoch [4/400], Step [214/716], Loss: 0.2309\n",
            "Epoch [4/400], Step [215/716], Loss: 0.0989\n",
            "Epoch [4/400], Step [216/716], Loss: 0.1118\n",
            "Epoch [4/400], Step [217/716], Loss: 0.1545\n",
            "Epoch [4/400], Step [218/716], Loss: 0.1790\n",
            "Epoch [4/400], Step [219/716], Loss: 0.1396\n",
            "Epoch [4/400], Step [220/716], Loss: 0.1389\n",
            "Epoch [4/400], Step [221/716], Loss: 0.1039\n",
            "Epoch [4/400], Step [222/716], Loss: 0.1842\n",
            "Epoch [4/400], Step [223/716], Loss: 0.1182\n",
            "Epoch [4/400], Step [224/716], Loss: 0.1433\n",
            "Epoch [4/400], Step [225/716], Loss: 0.1301\n",
            "Epoch [4/400], Step [226/716], Loss: 0.1187\n",
            "Epoch [4/400], Step [227/716], Loss: 0.1792\n",
            "Epoch [4/400], Step [228/716], Loss: 0.1105\n",
            "Epoch [4/400], Step [229/716], Loss: 0.1028\n",
            "Epoch [4/400], Step [230/716], Loss: 0.1122\n",
            "Epoch [4/400], Step [231/716], Loss: 0.0849\n",
            "Epoch [4/400], Step [232/716], Loss: 0.1126\n",
            "Epoch [4/400], Step [233/716], Loss: 0.1296\n",
            "Epoch [4/400], Step [234/716], Loss: 0.0950\n",
            "Epoch [4/400], Step [235/716], Loss: 0.1197\n",
            "Epoch [4/400], Step [236/716], Loss: 0.1289\n",
            "Epoch [4/400], Step [237/716], Loss: 0.1419\n",
            "Epoch [4/400], Step [238/716], Loss: 0.1306\n",
            "Epoch [4/400], Step [239/716], Loss: 0.1407\n",
            "Epoch [4/400], Step [240/716], Loss: 0.1416\n",
            "Epoch [4/400], Step [241/716], Loss: 0.1528\n",
            "Epoch [4/400], Step [242/716], Loss: 0.1056\n",
            "Epoch [4/400], Step [243/716], Loss: 0.1454\n",
            "Epoch [4/400], Step [244/716], Loss: 0.0890\n",
            "Epoch [4/400], Step [245/716], Loss: 0.1001\n",
            "Epoch [4/400], Step [246/716], Loss: 0.1391\n",
            "Epoch [4/400], Step [247/716], Loss: 0.1403\n",
            "Epoch [4/400], Step [248/716], Loss: 0.1392\n",
            "Epoch [4/400], Step [249/716], Loss: 0.1142\n",
            "Epoch [4/400], Step [250/716], Loss: 0.0895\n",
            "Epoch [4/400], Step [251/716], Loss: 0.2003\n",
            "Epoch [4/400], Step [252/716], Loss: 0.2131\n",
            "Epoch [4/400], Step [253/716], Loss: 0.1072\n",
            "Epoch [4/400], Step [254/716], Loss: 0.1674\n",
            "Epoch [4/400], Step [255/716], Loss: 0.1083\n",
            "Epoch [4/400], Step [256/716], Loss: 0.1413\n",
            "Epoch [4/400], Step [257/716], Loss: 0.1243\n",
            "Epoch [4/400], Step [258/716], Loss: 0.1773\n",
            "Epoch [4/400], Step [259/716], Loss: 0.1460\n",
            "Epoch [4/400], Step [260/716], Loss: 0.1131\n",
            "Epoch [4/400], Step [261/716], Loss: 0.1258\n",
            "Epoch [4/400], Step [262/716], Loss: 0.1353\n",
            "Epoch [4/400], Step [263/716], Loss: 0.1244\n",
            "Epoch [4/400], Step [264/716], Loss: 0.1251\n",
            "Epoch [4/400], Step [265/716], Loss: 0.0933\n",
            "Epoch [4/400], Step [266/716], Loss: 0.1156\n",
            "Epoch [4/400], Step [267/716], Loss: 0.1117\n",
            "Epoch [4/400], Step [268/716], Loss: 0.1438\n",
            "Epoch [4/400], Step [269/716], Loss: 0.1304\n",
            "Epoch [4/400], Step [270/716], Loss: 0.1063\n",
            "Epoch [4/400], Step [271/716], Loss: 0.0831\n",
            "Epoch [4/400], Step [272/716], Loss: 0.1262\n",
            "Epoch [4/400], Step [273/716], Loss: 0.1391\n",
            "Epoch [4/400], Step [274/716], Loss: 0.1062\n",
            "Epoch [4/400], Step [275/716], Loss: 0.0836\n",
            "Epoch [4/400], Step [276/716], Loss: 0.1190\n",
            "Epoch [4/400], Step [277/716], Loss: 0.1126\n",
            "Epoch [4/400], Step [278/716], Loss: 0.1529\n",
            "Epoch [4/400], Step [279/716], Loss: 0.1083\n",
            "Epoch [4/400], Step [280/716], Loss: 0.1166\n",
            "Epoch [4/400], Step [281/716], Loss: 0.1527\n",
            "Epoch [4/400], Step [282/716], Loss: 0.1734\n",
            "Epoch [4/400], Step [283/716], Loss: 0.1475\n",
            "Epoch [4/400], Step [284/716], Loss: 0.1175\n",
            "Epoch [4/400], Step [285/716], Loss: 0.1685\n",
            "Epoch [4/400], Step [286/716], Loss: 0.1317\n",
            "Epoch [4/400], Step [287/716], Loss: 0.0872\n",
            "Epoch [4/400], Step [288/716], Loss: 0.1286\n",
            "Epoch [4/400], Step [289/716], Loss: 0.2009\n",
            "Epoch [4/400], Step [290/716], Loss: 0.1126\n",
            "Epoch [4/400], Step [291/716], Loss: 0.1615\n",
            "Epoch [4/400], Step [292/716], Loss: 0.1366\n",
            "Epoch [4/400], Step [293/716], Loss: 0.1057\n",
            "Epoch [4/400], Step [294/716], Loss: 0.1089\n",
            "Epoch [4/400], Step [295/716], Loss: 0.1518\n",
            "Epoch [4/400], Step [296/716], Loss: 0.1448\n",
            "Epoch [4/400], Step [297/716], Loss: 0.0908\n",
            "Epoch [4/400], Step [298/716], Loss: 0.1056\n",
            "Epoch [4/400], Step [299/716], Loss: 0.1753\n",
            "Epoch [4/400], Step [300/716], Loss: 0.1873\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_4_step_300.pth\n",
            "Epoch [4/400], Step [301/716], Loss: 0.1196\n",
            "Epoch [4/400], Step [302/716], Loss: 0.1588\n",
            "Epoch [4/400], Step [303/716], Loss: 0.1501\n",
            "Epoch [4/400], Step [304/716], Loss: 0.1522\n",
            "Epoch [4/400], Step [305/716], Loss: 0.1229\n",
            "Epoch [4/400], Step [306/716], Loss: 0.1262\n",
            "Epoch [4/400], Step [307/716], Loss: 0.1451\n",
            "Epoch [4/400], Step [308/716], Loss: 0.1079\n",
            "Epoch [4/400], Step [309/716], Loss: 0.1336\n",
            "Epoch [4/400], Step [310/716], Loss: 0.1167\n",
            "Epoch [4/400], Step [311/716], Loss: 0.1491\n",
            "Epoch [4/400], Step [312/716], Loss: 0.1414\n",
            "Epoch [4/400], Step [313/716], Loss: 0.1424\n",
            "Epoch [4/400], Step [314/716], Loss: 0.1193\n",
            "Epoch [4/400], Step [315/716], Loss: 0.1174\n",
            "Epoch [4/400], Step [316/716], Loss: 0.1796\n",
            "Epoch [4/400], Step [317/716], Loss: 0.0954\n",
            "Epoch [4/400], Step [318/716], Loss: 0.0980\n",
            "Epoch [4/400], Step [319/716], Loss: 0.1316\n",
            "Epoch [4/400], Step [320/716], Loss: 0.1242\n",
            "Epoch [4/400], Step [321/716], Loss: 0.1264\n",
            "Epoch [4/400], Step [322/716], Loss: 0.0850\n",
            "Epoch [4/400], Step [323/716], Loss: 0.1181\n",
            "Epoch [4/400], Step [324/716], Loss: 0.1199\n",
            "Epoch [4/400], Step [325/716], Loss: 0.1516\n",
            "Epoch [4/400], Step [326/716], Loss: 0.1098\n",
            "Epoch [4/400], Step [327/716], Loss: 0.1662\n",
            "Epoch [4/400], Step [328/716], Loss: 0.1339\n",
            "Epoch [4/400], Step [329/716], Loss: 0.0988\n",
            "Epoch [4/400], Step [330/716], Loss: 0.1226\n",
            "Epoch [4/400], Step [331/716], Loss: 0.1502\n",
            "Epoch [4/400], Step [332/716], Loss: 0.2013\n",
            "Epoch [4/400], Step [333/716], Loss: 0.1190\n",
            "Epoch [4/400], Step [334/716], Loss: 0.1424\n",
            "Epoch [4/400], Step [335/716], Loss: 0.1827\n",
            "Epoch [4/400], Step [336/716], Loss: 0.1669\n",
            "Epoch [4/400], Step [337/716], Loss: 0.1098\n",
            "Epoch [4/400], Step [338/716], Loss: 0.1355\n",
            "Epoch [4/400], Step [339/716], Loss: 0.1595\n",
            "Epoch [4/400], Step [340/716], Loss: 0.1357\n",
            "Epoch [4/400], Step [341/716], Loss: 0.1048\n",
            "Epoch [4/400], Step [342/716], Loss: 0.1572\n",
            "Epoch [4/400], Step [343/716], Loss: 0.1450\n",
            "Epoch [4/400], Step [344/716], Loss: 0.1205\n",
            "Epoch [4/400], Step [345/716], Loss: 0.1437\n",
            "Epoch [4/400], Step [346/716], Loss: 0.1196\n",
            "Epoch [4/400], Step [347/716], Loss: 0.1712\n",
            "Epoch [4/400], Step [348/716], Loss: 0.1157\n",
            "Epoch [4/400], Step [349/716], Loss: 0.1109\n",
            "Epoch [4/400], Step [350/716], Loss: 0.1050\n",
            "Epoch [4/400], Step [351/716], Loss: 0.0983\n",
            "Epoch [4/400], Step [352/716], Loss: 0.1278\n",
            "Epoch [4/400], Step [353/716], Loss: 0.1042\n",
            "Epoch [4/400], Step [354/716], Loss: 0.0973\n",
            "Epoch [4/400], Step [355/716], Loss: 0.0977\n",
            "Epoch [4/400], Step [356/716], Loss: 0.1510\n",
            "Epoch [4/400], Step [357/716], Loss: 0.0958\n",
            "Epoch [4/400], Step [358/716], Loss: 0.2049\n",
            "Epoch [4/400], Step [359/716], Loss: 0.1325\n",
            "Epoch [4/400], Step [360/716], Loss: 0.1413\n",
            "Epoch [4/400], Step [361/716], Loss: 0.1107\n",
            "Epoch [4/400], Step [362/716], Loss: 0.1261\n",
            "Epoch [4/400], Step [363/716], Loss: 0.1956\n",
            "Epoch [4/400], Step [364/716], Loss: 0.1428\n",
            "Epoch [4/400], Step [365/716], Loss: 0.1072\n",
            "Epoch [4/400], Step [366/716], Loss: 0.1589\n",
            "Epoch [4/400], Step [367/716], Loss: 0.1161\n",
            "Epoch [4/400], Step [368/716], Loss: 0.1952\n",
            "Epoch [4/400], Step [369/716], Loss: 0.1129\n",
            "Epoch [4/400], Step [370/716], Loss: 0.1093\n",
            "Epoch [4/400], Step [371/716], Loss: 0.1224\n",
            "Epoch [4/400], Step [372/716], Loss: 0.1364\n",
            "Epoch [4/400], Step [373/716], Loss: 0.1451\n",
            "Epoch [4/400], Step [374/716], Loss: 0.1618\n",
            "Epoch [4/400], Step [375/716], Loss: 0.2098\n",
            "Epoch [4/400], Step [376/716], Loss: 0.1641\n",
            "Epoch [4/400], Step [377/716], Loss: 0.1700\n",
            "Epoch [4/400], Step [378/716], Loss: 0.1024\n",
            "Epoch [4/400], Step [379/716], Loss: 0.1311\n",
            "Epoch [4/400], Step [380/716], Loss: 0.1174\n",
            "Epoch [4/400], Step [381/716], Loss: 0.0904\n",
            "Epoch [4/400], Step [382/716], Loss: 0.1554\n",
            "Epoch [4/400], Step [383/716], Loss: 0.1293\n",
            "Epoch [4/400], Step [384/716], Loss: 0.1193\n",
            "Epoch [4/400], Step [385/716], Loss: 0.0966\n",
            "Epoch [4/400], Step [386/716], Loss: 0.1367\n",
            "Epoch [4/400], Step [387/716], Loss: 0.1454\n",
            "Epoch [4/400], Step [388/716], Loss: 0.1125\n",
            "Epoch [4/400], Step [389/716], Loss: 0.1285\n",
            "Epoch [4/400], Step [390/716], Loss: 0.1159\n",
            "Epoch [4/400], Step [391/716], Loss: 0.1103\n",
            "Epoch [4/400], Step [392/716], Loss: 0.0852\n",
            "Epoch [4/400], Step [393/716], Loss: 0.1402\n",
            "Epoch [4/400], Step [394/716], Loss: 0.0942\n",
            "Epoch [4/400], Step [395/716], Loss: 0.1033\n",
            "Epoch [4/400], Step [396/716], Loss: 0.0995\n",
            "Epoch [4/400], Step [397/716], Loss: 0.1632\n",
            "Epoch [4/400], Step [398/716], Loss: 0.1491\n",
            "Epoch [4/400], Step [399/716], Loss: 0.1146\n",
            "Epoch [4/400], Step [400/716], Loss: 0.1506\n",
            "Checkpoint saved at /content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints/checkpoint_epoch_4_step_400.pth\n",
            "Epoch [4/400], Step [401/716], Loss: 0.1578\n",
            "Epoch [4/400], Step [402/716], Loss: 0.1604\n",
            "Epoch [4/400], Step [403/716], Loss: 0.1403\n",
            "Epoch [4/400], Step [404/716], Loss: 0.1423\n",
            "Epoch [4/400], Step [405/716], Loss: 0.1180\n",
            "Epoch [4/400], Step [406/716], Loss: 0.1184\n",
            "Epoch [4/400], Step [407/716], Loss: 0.1089\n",
            "Epoch [4/400], Step [408/716], Loss: 0.1308\n",
            "Epoch [4/400], Step [409/716], Loss: 0.1731\n",
            "Epoch [4/400], Step [410/716], Loss: 0.1095\n",
            "Epoch [4/400], Step [411/716], Loss: 0.1199\n",
            "Epoch [4/400], Step [412/716], Loss: 0.1115\n",
            "Epoch [4/400], Step [413/716], Loss: 0.1235\n",
            "Epoch [4/400], Step [414/716], Loss: 0.1298\n",
            "Epoch [4/400], Step [415/716], Loss: 0.1262\n",
            "Epoch [4/400], Step [416/716], Loss: 0.1268\n",
            "Epoch [4/400], Step [417/716], Loss: 0.1456\n",
            "Epoch [4/400], Step [418/716], Loss: 0.1754\n",
            "Epoch [4/400], Step [419/716], Loss: 0.1546\n",
            "Epoch [4/400], Step [420/716], Loss: 0.1197\n",
            "Epoch [4/400], Step [421/716], Loss: 0.1479\n",
            "Epoch [4/400], Step [422/716], Loss: 0.1163\n",
            "Epoch [4/400], Step [423/716], Loss: 0.1117\n",
            "Epoch [4/400], Step [424/716], Loss: 0.1274\n",
            "Epoch [4/400], Step [425/716], Loss: 0.1046\n",
            "Epoch [4/400], Step [426/716], Loss: 0.1170\n",
            "Epoch [4/400], Step [427/716], Loss: 0.1312\n",
            "Epoch [4/400], Step [428/716], Loss: 0.1223\n",
            "Epoch [4/400], Step [429/716], Loss: 0.2253\n",
            "Epoch [4/400], Step [430/716], Loss: 0.0853\n",
            "Epoch [4/400], Step [431/716], Loss: 0.1231\n",
            "Epoch [4/400], Step [432/716], Loss: 0.0848\n",
            "Epoch [4/400], Step [433/716], Loss: 0.1123\n",
            "Epoch [4/400], Step [434/716], Loss: 0.1073\n",
            "Epoch [4/400], Step [435/716], Loss: 0.1742\n",
            "Epoch [4/400], Step [436/716], Loss: 0.1025\n",
            "Epoch [4/400], Step [437/716], Loss: 0.1234\n",
            "Epoch [4/400], Step [438/716], Loss: 0.1159\n",
            "Epoch [4/400], Step [439/716], Loss: 0.1023\n",
            "Epoch [4/400], Step [440/716], Loss: 0.1034\n",
            "Epoch [4/400], Step [441/716], Loss: 0.0983\n",
            "Epoch [4/400], Step [442/716], Loss: 0.1053\n",
            "Epoch [4/400], Step [443/716], Loss: 0.1357\n",
            "Epoch [4/400], Step [444/716], Loss: 0.1154\n",
            "Epoch [4/400], Step [445/716], Loss: 0.1139\n",
            "Epoch [4/400], Step [446/716], Loss: 0.1210\n",
            "Epoch [4/400], Step [447/716], Loss: 0.1412\n",
            "Epoch [4/400], Step [448/716], Loss: 0.0896\n",
            "Epoch [4/400], Step [449/716], Loss: 0.0963\n",
            "Epoch [4/400], Step [450/716], Loss: 0.1248\n",
            "Epoch [4/400], Step [451/716], Loss: 0.1029\n",
            "Epoch [4/400], Step [452/716], Loss: 0.1193\n",
            "Epoch [4/400], Step [453/716], Loss: 0.1416\n",
            "Epoch [4/400], Step [454/716], Loss: 0.1084\n",
            "Epoch [4/400], Step [455/716], Loss: 0.1539\n",
            "Epoch [4/400], Step [456/716], Loss: 0.1191\n",
            "Epoch [4/400], Step [457/716], Loss: 0.1337\n",
            "Epoch [4/400], Step [458/716], Loss: 0.1590\n",
            "Epoch [4/400], Step [459/716], Loss: 0.1453\n",
            "Epoch [4/400], Step [460/716], Loss: 0.1269\n",
            "Epoch [4/400], Step [461/716], Loss: 0.1317\n",
            "Epoch [4/400], Step [462/716], Loss: 0.1484\n",
            "Epoch [4/400], Step [463/716], Loss: 0.2010\n",
            "Epoch [4/400], Step [464/716], Loss: 0.1003\n",
            "Epoch [4/400], Step [465/716], Loss: 0.2158\n",
            "Epoch [4/400], Step [466/716], Loss: 0.1352\n",
            "Epoch [4/400], Step [467/716], Loss: 0.1847\n",
            "Epoch [4/400], Step [468/716], Loss: 0.1258\n",
            "Epoch [4/400], Step [469/716], Loss: 0.1432\n",
            "Epoch [4/400], Step [470/716], Loss: 0.1205\n",
            "Epoch [4/400], Step [471/716], Loss: 0.1145\n",
            "Epoch [4/400], Step [472/716], Loss: 0.1605\n",
            "Epoch [4/400], Step [473/716], Loss: 0.1288\n",
            "Epoch [4/400], Step [474/716], Loss: 0.1413\n",
            "Epoch [4/400], Step [475/716], Loss: 0.1542\n",
            "Epoch [4/400], Step [476/716], Loss: 0.1295\n",
            "Epoch [4/400], Step [477/716], Loss: 0.1398\n",
            "Epoch [4/400], Step [478/716], Loss: 0.1015\n",
            "Epoch [4/400], Step [479/716], Loss: 0.1058\n",
            "Epoch [4/400], Step [480/716], Loss: 0.0784\n",
            "Epoch [4/400], Step [481/716], Loss: 0.1211\n",
            "Epoch [4/400], Step [482/716], Loss: 0.1241\n",
            "Epoch [4/400], Step [483/716], Loss: 0.1474\n",
            "Epoch [4/400], Step [484/716], Loss: 0.1067\n",
            "Epoch [4/400], Step [485/716], Loss: 0.1240\n",
            "Epoch [4/400], Step [486/716], Loss: 0.1044\n",
            "Epoch [4/400], Step [487/716], Loss: 0.1123\n",
            "Epoch [4/400], Step [488/716], Loss: 0.1323\n",
            "Epoch [4/400], Step [489/716], Loss: 0.1313\n",
            "Epoch [4/400], Step [490/716], Loss: 0.1637\n",
            "Epoch [4/400], Step [491/716], Loss: 0.1239\n",
            "Epoch [4/400], Step [492/716], Loss: 0.1170\n",
            "Epoch [4/400], Step [493/716], Loss: 0.1165\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-27ed4653922c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define a function to save a checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "\n",
        "    # Make sure the checkpoint directory exists\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Save the checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "# Define a function to load a checkpoint\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Resuming from epoch {epoch + 1} with loss {loss:.4f}\")\n",
        "    return model, optimizer, epoch, loss\n",
        "\n",
        "# Set your checkpoint directory\n",
        "checkpoint_dir = '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/checkpoints'\n",
        "checkpoint_filename = 'checkpoint_epoch_1_step_100.pth'  # The latest checkpoint file\n",
        "\n",
        "# Check if a checkpoint exists to resume training\n",
        "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "start_epoch = 0  # Default starting epoch\n",
        "loss = 0.0\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    # Load checkpoint if it exists\n",
        "    model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "else:\n",
        "    print(\"No checkpoint found, starting from scratch.\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, cfig['num_epochs']):\n",
        "    model.train()\n",
        "    for i, data_dict in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(data_dict['data'].to(device))\n",
        "        loss = criterion(outputs, data_dict['label'].to(device))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the current loss\n",
        "        print(f\"Epoch [{epoch+1}/{cfig['num_epochs']}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Save checkpoint every 'N' steps (for example every 100 steps)\n",
        "        if (i + 1) % 100 == 0:\n",
        "            save_checkpoint(model, optimizer, epoch, loss.item(), checkpoint_dir, filename=f\"checkpoint_epoch_{epoch+1}_step_{i+1}.pth\")\n",
        "\n",
        "    # Optionally, save a checkpoint at the end of each epoch\n",
        "    save_checkpoint(model, optimizer, epoch, loss.item(), checkpoint_dir, filename=f\"checkpoint_epoch_{epoch+1}_last.pth\")\n",
        "\n",
        "# After training, save the final model weights\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Contents/gdp-hmm_aapmchallenge/model_weights.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f83985",
      "metadata": {
        "id": "55f83985",
        "tags": []
      },
      "source": [
        "## 7. Timing for inference of the deep learning module\n",
        "\n",
        "In this challenge, we impose a time regularization specifically for the deep learning module in dose prediction, reflecting its clinical application nature. Data preprocessing, however, is outside the scope of this challenge and can be optimized using C++/CUDA for significantly faster performance.  \n",
        "\n",
        "The MedNeXt baseline we provided comprises approximately 10 million parameters and achieves an inference time of just 0.13 seconds. While we allow a relatively lenient inference time constraint of **3 seconds**, caution is advised when employing diffusion models, particularly if acceleration techniques are not utilized. For example, the default DDPM requires 1000 steps to generate results, which can easily exceed the time constraint.\n",
        "\n",
        "Please check below code to get sense of how inference time is calculated. Also, the peak inference GPU memory cannot exceed **24 GB** (the baseline is ~5.7 GB).\n",
        "\n",
        "***The solution exceeds either time constraint or GPU memory constraint will be rejected!***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0dea8dd9",
      "metadata": {
        "id": "0dea8dd9",
        "outputId": "e011ceca-6f5e-4177-bbe0-5edc254d4a76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the total parameters of the model is 10526498\n",
            "----- skip first 20 times, to avoid delay because of running start ----\n",
            "Time taken for average forward pass: 0.1520 seconds\n"
          ]
        }
      ],
      "source": [
        "import time, os\n",
        "model.eval()\n",
        "data_dict = next(iter(train_loader)) # since this is a dummy test, it does not matter using train or test loaders.\n",
        "\n",
        "print (f\"the total parameters of the model is {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "    print ('----- skip first 20 times, to avoid delay because of running start ----')\n",
        "    for i in range(20):\n",
        "        outputs = model(data_dict['data'].to(device))\n",
        "    os.system('nvidia-smi')\n",
        "    start = time.time()\n",
        "    for i in range(20):\n",
        "        outputs = model(data_dict['data'].to(device))\n",
        "    end = time.time()\n",
        "    print(f\"Time taken for average forward pass: {(end-start) / 20:.4f} seconds\")\n",
        "    assert (end-start) / 20 < 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05b178d-d9da-4513-bf10-399fb74dd147",
      "metadata": {
        "id": "b05b178d-d9da-4513-bf10-399fb74dd147"
      },
      "source": [
        "## 8. Training Regularization\n",
        "\n",
        "To strengthen the challenge's objectives, participants are required to develop a generalizable model rather than separate models tailored to individual contexts. To ensure compliance, top-performing participants must submit their training and inference code for review by the organizers.\n",
        "\n",
        "**Prohibited approaches include (but are not limited to):**\n",
        "\n",
        "Training separate models for different treatment techniques, such as one for IMRT and another for VMAT.\n",
        "Training separate models for different treatment sites, such as one for head-and-neck cancers and another for lung cancers.\n",
        "\n",
        "**Rationale for this regularization:**\n",
        "\n",
        "In real-world applications, many other contexts exist, including diverse treatment sites (e.g., prostate, breast, cervical, esophageal, and bladder cancers) and varying treatment geometries (e.g., combinations of IMRT and VMAT, such as RapidArc Dynamic). The goal is to develop a generalizable model capable of adapting to new contexts as more training data become available, rather than creating multiple context-specific models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57aaaa7-cb72-449c-9330-7e34de410498",
      "metadata": {
        "id": "a57aaaa7-cb72-449c-9330-7e34de410498"
      },
      "source": [
        "## 9. Start your development"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec697f81-fd7a-4fb2-a13b-dae7471a1368",
      "metadata": {
        "id": "ec697f81-fd7a-4fb2-a13b-dae7471a1368"
      },
      "source": [
        "Congradulations! You have reached the end of the tutorial and should get the sense how the task is.\n",
        "\n",
        "Here we just provide a example to help you get started. Some of the parameters are not optimal, only few examples included in the csv file.\n",
        "\n",
        "Now, it is time for you to include more data from the challenge and use your AI expertise to get better results.\n",
        "\n",
        "Wish you a great experience with this challenge and research beyond!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}