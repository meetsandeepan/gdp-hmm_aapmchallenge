{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a4bb345-7c95-4c74-a059-c079e5cd0d5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get Started for GDP-HMM Challenge\n",
    "\n",
    "This tutorial offers a quick start for training a 3D dose prediction. The participants are encouraged to bring more advanced techniques to improvement the baseline. \n",
    "\n",
    "If you do not like this Jupyter Notebook style, you can directly run the [train.py](train.py) with command line as below (after you have installed necessary packages): \n",
    "\n",
    "```\n",
    "python train.py config_files/config.yaml\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "python train_lightning.py config_files/config.yaml\n",
    "```\n",
    "\n",
    "where [config.yaml](config_files/config.yaml) summarizes all the important hyperparameters. The lightning version can directly use multi-process and multi-gpu. \n",
    "\n",
    "After the training finished, run the below command after set the pre-trained model path in the `config_infer.yaml` file.\n",
    "\n",
    "```\n",
    "python inference.py config_files/config_infer.yaml\n",
    "```\n",
    "\n",
    "Want more details? please continue the following. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9132d",
   "metadata": {},
   "source": [
    "# 0. Before Start\n",
    "\n",
    "**Step 1. Register the challenge**. \n",
    "\n",
    "Go to the <a href=\"https://qtim-challenges.southcentralus.cloudapp.azure.com/competitions/38/\" _target='blank'>challenge platform</a>: \n",
    "\n",
    "1.1 create an account of the platform; \n",
    "\n",
    "1.2 go to \"My Submissions\" and read the terms carefully and register the challenge.\n",
    "\n",
    "**Step 2: Download data/model resources**. \n",
    "\n",
    "2.1 download the data (and pre-train models) in huggingface (you will need to submit registration to challenge platform first). \n",
    "\n",
    "[Data](https://huggingface.co/datasets/Jungle15/GDP-HMM_Challenge)\n",
    "\n",
    "[Model](https://huggingface.co/Jungle15/GDP-HMM_baseline)\n",
    "\n",
    "2.2 [optional] for data/prediction samples, you can download from [OneDrive](https://1drv.ms/f/c/347c1b40c8c6e5ec/Ej5OQVE_APpOnNuP-ZXpnZcBnr_-ix5W-twQcYIJ-dvW2A?e=YcBSPF), and put them into `data` and `results` folders, respectively. This is not the whole dataset for the challenge. \n",
    "\n",
    "2.3 change the `npz_path` in the `meta_files/meta_data.csv` depending on the data path on your local machine.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c857e-340f-4c27-ba3a-6bd7bea5f56b",
   "metadata": {},
   "source": [
    "## 1. Python Environment\n",
    "\n",
    "The baseline has been tested with Python 3.10, PyTorch 2.1.2, and MONAI 1.4.0. Similar versions should work but have not been tested by organizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c386e22-0f31-41d7-9674-1422aa754d31",
   "metadata": {},
   "source": [
    "## 2. Install the MedNeXt as the network backbone\n",
    "\n",
    "In the baseline, we choose the [MedNeXt](https://github.com/MIC-DKFZ/MedNeXt) as backbone. One major reason is that MedNeXt has achieved the top performance in recently release **external** testing benckmarks including the [TouchStone (NeurIPS 2024)](https://github.com/MrGiovanni/Touchstone) and [nnUnet revisited (MICCAI 2024)](https://arxiv.org/abs/2404.09556). MedNeXt is still a CNN-based structure, while in the external testing benckmarks, it has consistently beated all the other Transformers and Mamaba structures, sometimes with a large margin. \n",
    "\n",
    "Please follow the [MedNeXt official instructions](https://github.com/MIC-DKFZ/MedNeXt) to install and use. It is quite detailed and easy to follow. For example, you can use below command lines to install: \n",
    "\n",
    "```\n",
    "git clone https://github.com/MIC-DKFZ/MedNeXt.git mednext\n",
    "cd mednext\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fa33d-e04d-4004-b44f-77a1f0b6b5a4",
   "metadata": {},
   "source": [
    "## 3. Import neccessary packages and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cb291c-5e87-4a61-94c4-ccdf8d6bb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "\n",
    "\n",
    "from nnunet_mednext import create_mednext_v1\n",
    "\n",
    "# Import data_loader from the current directory\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "import data_loader\n",
    "\n",
    "cfig = yaml.load(open('config_files/config_dummy.yaml'), Loader=yaml.FullLoader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4531415-8ffb-46c9-81d9-b357785a76d1",
   "metadata": {},
   "source": [
    "The config includes two major parts: loader_params and model_params. We will introduce them more in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdb9d4d-cc8d-435c-87f6-7a5418c6d0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_bs': 1,\n",
       " 'val_bs': 4,\n",
       " 'csv_root': 'meta_files/meta_data_dummy.csv',\n",
       " 'scale_dose_dict': 'meta_files/PTV_DICT.json',\n",
       " 'pat_obj_dict': 'meta_files/Pat_Obj_DICT.json',\n",
       " 'num_workers': 2,\n",
       " 'down_HU': -1000,\n",
       " 'up_HU': 1000,\n",
       " 'denom_norm_HU': 500,\n",
       " 'in_size': [96, 128, 144],\n",
       " 'out_size': [96, 128, 144],\n",
       " 'norm_oar': True,\n",
       " 'CatStructures': False,\n",
       " 'dose_div_factor': 10}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfig['loader_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055c9bff-c98d-44ab-8aed-217c1e4832d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_input_channels': 8,\n",
       " 'out_channels': 1,\n",
       " 'model_id': 'B',\n",
       " 'kernel_size': 3,\n",
       " 'deep_supervision': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfig['model_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80cde4-46fd-4cdc-bd60-3d79b3290be6",
   "metadata": {},
   "source": [
    "## 3. Data loader for this challenge\n",
    "\n",
    "For getting started, data loader might be most difficult part for the majority of participants. Do not worry, we will help you here! \n",
    "\n",
    "We include a complete data loader script in [data_loader.py](data_loader.py), with explanation of each input and parameter. You can simply test the data loader alone by running \n",
    "\n",
    "```\n",
    "python data_loader.py\n",
    "```\n",
    "\n",
    "If you want to visualize the 3D data and Dose-Volume Histograms (DVHs) with Python, we provide a jupyter notebook [here](data_visual_understand.ipynb). \n",
    "\n",
    "If you want to know more about the preprocess of data and adjust it if needed, we provide code [here](geometry_creation.ipynb). \n",
    "\n",
    "For loading the data in deep learning framework, you can use below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa66dea-fdad-490e-99b0-ee84a3953dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = data_loader.GetLoader(cfig = cfig['loader_params'])\n",
    "train_loader =loaders.train_dataloader()\n",
    "val_loader = loaders.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9cc78-89cd-4e27-8d2e-e32ed6d6375e",
   "metadata": {},
   "source": [
    "## 4. Network structure\n",
    "\n",
    "As mentioned earlier, we use MedNeXt as the backbone. Please follow the MedNeXt official instructions to adjust the structure. The example we use is as below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8de195-b584-4f51-afcb-67002a4f09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_mednext_v1( num_input_channels = cfig['model_params']['num_input_channels'],\n",
    "  num_classes = cfig['model_params']['out_channels'],\n",
    "  model_id = cfig['model_params']['model_id'],          # S, B, M and L are valid model ids\n",
    "  kernel_size = cfig['model_params']['kernel_size'],   # 3x3x3 and 5x5x5 were tested in publication\n",
    "  deep_supervision = cfig['model_params']['deep_supervision']   \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c19968b-6d82-4785-992f-8f13d1043892",
   "metadata": {},
   "source": [
    "## 5. Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe7bca4-a3f1-44b3-b1d8-7af538ad865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=cfig['lr'])\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c4753-af88-4384-84f9-58a3618a6532",
   "metadata": {},
   "source": [
    "## 6. Training \n",
    "\n",
    "Then, you are ready to with training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d5ef64-e046-43b6-ad6b-df3b767cbb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/2], Loss: 21.6373\n",
      "Epoch [1/10], Step [2/2], Loss: 4.7551\n",
      "Epoch [2/10], Step [1/2], Loss: 4.4043\n",
      "Epoch [2/10], Step [2/2], Loss: 26.0072\n",
      "Epoch [3/10], Step [1/2], Loss: 23.5479\n",
      "Epoch [3/10], Step [2/2], Loss: 4.1487\n",
      "Epoch [4/10], Step [1/2], Loss: 20.6044\n",
      "Epoch [4/10], Step [2/2], Loss: 3.8255\n",
      "Epoch [5/10], Step [1/2], Loss: 3.8469\n",
      "Epoch [5/10], Step [2/2], Loss: 19.6087\n",
      "Epoch [6/10], Step [1/2], Loss: 3.7697\n",
      "Epoch [6/10], Step [2/2], Loss: 19.5941\n",
      "Epoch [7/10], Step [1/2], Loss: 22.0368\n",
      "Epoch [7/10], Step [2/2], Loss: 3.2463\n",
      "Epoch [8/10], Step [1/2], Loss: 3.3352\n",
      "Epoch [8/10], Step [2/2], Loss: 20.9835\n",
      "Epoch [9/10], Step [1/2], Loss: 19.2807\n",
      "Epoch [9/10], Step [2/2], Loss: 3.0964\n",
      "Epoch [10/10], Step [1/2], Loss: 18.6419\n",
      "Epoch [10/10], Step [2/2], Loss: 3.0771\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(cfig['num_epochs']):\n",
    "    model.train()\n",
    "    for i, data_dict in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data_dict['data'].to(device))\n",
    "        loss = criterion(outputs, data_dict['label'].to(device))\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{cfig['num_epochs']}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f83985",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Timing for inference of the deep learning module\n",
    "\n",
    "In this challenge, we impose a time regularization specifically for the deep learning module in dose prediction, reflecting its clinical application nature. Data preprocessing, however, is outside the scope of this challenge and can be optimized using C++/CUDA for significantly faster performance.  \n",
    "\n",
    "The MedNeXt baseline we provided comprises approximately 10 million parameters and achieves an inference time of just 0.13 seconds. While we allow a relatively lenient inference time constraint of **3 seconds**, caution is advised when employing diffusion models, particularly if acceleration techniques are not utilized. For example, the default DDPM requires 1000 steps to generate results, which can easily exceed the time constraint. \n",
    "\n",
    "Please check below code to get sense of how inference time is calculated. Also, the peak inference GPU memory cannot exceed **24 GB** (the baseline is ~5.7 GB). \n",
    "\n",
    "***The solution exceeds either time constraint or GPU memory constraint will be rejected!***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dea8dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total parameters of the model is 10526498\n",
      "----- skip first 20 times, to avoid delay because of running start ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: nvidia-smi: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for average forward pass: 6.1510 seconds\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken for average forward pass: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (end\u001b[38;5;241m-\u001b[39mstart) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "model.eval()\n",
    "data_dict = next(iter(train_loader)) # since this is a dummy test, it does not matter using train or test loaders.\n",
    "\n",
    "print (f\"the total parameters of the model is {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    print ('----- skip first 20 times, to avoid delay because of running start ----')\n",
    "    for i in range(20):\n",
    "        outputs = model(data_dict['data'].to(device))\n",
    "    os.system('nvidia-smi')\n",
    "    start = time.time()\n",
    "    for i in range(20):\n",
    "        outputs = model(data_dict['data'].to(device))\n",
    "    end = time.time()\n",
    "    print(f\"Time taken for average forward pass: {(end-start) / 20:.4f} seconds\")\n",
    "    assert (end-start) / 20 < 2\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b178d-d9da-4513-bf10-399fb74dd147",
   "metadata": {},
   "source": [
    "## 8. Training Regularization\n",
    "\n",
    "To strengthen the challenge's objectives, participants are required to develop a generalizable model rather than separate models tailored to individual contexts. To ensure compliance, top-performing participants must submit their training and inference code for review by the organizers.\n",
    "\n",
    "**Prohibited approaches include (but are not limited to):**\n",
    "\n",
    "Training separate models for different treatment techniques, such as one for IMRT and another for VMAT.\n",
    "Training separate models for different treatment sites, such as one for head-and-neck cancers and another for lung cancers.\n",
    "\n",
    "**Rationale for this regularization:**\n",
    "\n",
    "In real-world applications, many other contexts exist, including diverse treatment sites (e.g., prostate, breast, cervical, esophageal, and bladder cancers) and varying treatment geometries (e.g., combinations of IMRT and VMAT, such as RapidArc Dynamic). The goal is to develop a generalizable model capable of adapting to new contexts as more training data become available, rather than creating multiple context-specific models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57aaaa7-cb72-449c-9330-7e34de410498",
   "metadata": {},
   "source": [
    "## 9. Start your development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec697f81-fd7a-4fb2-a13b-dae7471a1368",
   "metadata": {},
   "source": [
    "Congradulations! You have reached the end of the tutorial and should get the sense how the task is. \n",
    "\n",
    "Here we just provide a example to help you get started. Some of the parameters are not optimal, only few examples included in the csv file. \n",
    "\n",
    "Now, it is time for you to include more data from the challenge and use your AI expertise to get better results. \n",
    "\n",
    "Wish you a great experience with this challenge and research beyond!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42369d6c-efda-450e-a975-97e94ed8e20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
